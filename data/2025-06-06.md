<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [hdl2v: A Code Translation Dataset for Enhanced LLM Verilog Generation](https://arxiv.org/abs/2506.04544)
*Charles Hong,Brendan Roberts,Huijae An,Alex Um,Advay Ratan,Yakun Sophia Shao*

Main category: cs.AR

TL;DR: 论文介绍了hdl2v数据集，通过将VHDL、Chisel和PyMTL3翻译为Verilog，增加公开可用的Verilog代码量，并展示了其在提升LLM生成Verilog代码性能方面的价值。


<details>
  <summary>Details</summary>
Motivation: 公开可用的Verilog代码远少于其他软件语言（如Python），限制了LLM在硬件代码生成中的应用。

Method: 通过翻译VHDL、Chisel和PyMTL3生成Verilog代码，构建hdl2v数据集，并用于增强LLM的Verilog生成能力。

Result: hdl2v将32B参数模型的性能提升23%（pass@10），数据增强微调方法性能提升63%。

Conclusion: hdl2v数据集有效提升了LLM的Verilog生成能力，未来可进一步扩展其特性以优化性能。

Abstract: Large language models (LLMs) are playing an increasingly large role in
domains such as code generation, including hardware code generation, where
Verilog is the key language. However, the amount of publicly available Verilog
code pales in comparison to the amount of code available for software languages
like Python. In this work, we present hdl2v ("HDL-to-Verilog"), a dataset which
seeks to increase the amount of available human-written Verilog data by
translating or compiling three other hardware description languages - VHDL,
Chisel, and PyMTL3 - to Verilog. Furthermore, we demonstrate the value of hdl2v
in enhancing LLM Verilog generation by improving performance of a 32
billion-parameter open-weight model by up to 23% (pass@10) in VerilogEvalV2,
without utilizing any data augmentation or knowledge distillation from larger
models. We also show hdl2v's ability to boost the performance of a data
augmentation-based fine-tuning approach by 63%. Finally, we characterize and
analyze our dataset to better understand which characteristics of
HDL-to-Verilog datasets can be expanded upon in future work for even better
performance.

</details>
