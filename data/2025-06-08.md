<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [hdl2v: A Code Translation Dataset for Enhanced LLM Verilog Generation](https://arxiv.org/abs/2506.04544)
*Charles Hong,Brendan Roberts,Huijae An,Alex Um,Advay Ratan,Yakun Sophia Shao*

Main category: cs.AR

TL;DR: 论文介绍了hdl2v数据集，通过将VHDL、Chisel和PyMTL3转换为Verilog，增加公开可用的Verilog代码量，并展示了其在提升LLM生成Verilog代码性能方面的价值。


<details>
  <summary>Details</summary>
Motivation: 公开可用的Verilog代码远少于其他软件语言（如Python），限制了LLM在硬件代码生成领域的应用。

Method: 提出hdl2v数据集，通过翻译或编译VHDL、Chisel和PyMTL3生成Verilog代码。

Result: hdl2v显著提升了32B参数LLM在VerilogEvalV2中的性能（最高23%），并增强了数据增强方法的性能（63%）。

Conclusion: hdl2v为未来研究提供了扩展HDL-to-Verilog数据集特性的基础。

Abstract: Large language models (LLMs) are playing an increasingly large role in
domains such as code generation, including hardware code generation, where
Verilog is the key language. However, the amount of publicly available Verilog
code pales in comparison to the amount of code available for software languages
like Python. In this work, we present hdl2v ("HDL-to-Verilog"), a dataset which
seeks to increase the amount of available human-written Verilog data by
translating or compiling three other hardware description languages - VHDL,
Chisel, and PyMTL3 - to Verilog. Furthermore, we demonstrate the value of hdl2v
in enhancing LLM Verilog generation by improving performance of a 32
billion-parameter open-weight model by up to 23% (pass@10) in VerilogEvalV2,
without utilizing any data augmentation or knowledge distillation from larger
models. We also show hdl2v's ability to boost the performance of a data
augmentation-based fine-tuning approach by 63%. Finally, we characterize and
analyze our dataset to better understand which characteristics of
HDL-to-Verilog datasets can be expanded upon in future work for even better
performance.

</details>
