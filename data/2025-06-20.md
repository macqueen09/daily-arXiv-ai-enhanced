<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.SE](#cs.SE) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [A Novel Compiler Transformation for Fast Sparse Matrix Multiplication in GPUs](https://arxiv.org/abs/2506.15174)
*Hossein Albakri,Kazem Cheshmi*

Main category: cs.PL

TL;DR: 提出一种新的编译器转换方法，用于加速GPU上的稀疏矩阵乘法，提高数据重用和负载均衡，在A100 GPU上实现1.84×到2.27×的加速。


<details>
  <summary>Details</summary>
Motivation: 稀疏数据结构在神经网络中常用以减少内存占用，但其随机内存访问导致GPU资源利用效率低下，需要优化。

Method: 提出enumerate-and-sparse-coarsen编译器转换，通过增加寄存器和缓存中的数据重用，优化GPU计算资源负载均衡。

Result: 在A100 GPU上，对稀疏矩阵乘法（SPMM）实现1.84×到2.27×的几何平均加速。

Conclusion: 该方法显著提升了GPU上稀疏矩阵乘法的效率，适用于卷积和Transformer模型。

Abstract: Sparse data structures are commonly used in neural networks to reduce the memory footprint. These data structures are compact but cause irregularities such as random memory accesses, which prevent efficient use of the memory hierarchy. GPUs are a common platform for machine learning practitioners, but running compact data structures on these devices often leads to slow-downs due to inefficient use of computing and memory resources. This paper proposes a new compiler transformation, enumerate-and-sparse-coarsen, that accelerates sparse matrix-matrix multiplication (SPMM) on GPU devices. The transformation increases data reuse in registers and caches while creating more balanced workloads for GPU computing resources. The transformation is tested on sparse neural networks in convolutional and transformer models. On an A100 GPU and across a columns of matrix B (bCols) in $ A \times B = C$ from range of 32 to 128, the transformation yields a geometric mean speedup of 1.84$\times$ to 2.27$\times$ compared to cuBLAS and cuSPARSE baselines, respectively.

</details>


### [2] [PSM: Policy Synchronised Deterministic Memory](https://arxiv.org/abs/2506.15424)
*Michael Mendler,Marc Pouzet*

Main category: cs.PL

TL;DR: 论文提出了一种新的Haskell类型上下文PSM，用于实现并发且确定性的共享内存编程，解决了传统方法中并发与确定性难以共存的问题。


<details>
  <summary>Details</summary>
Motivation: 传统Haskell中的并行编程抽象（如Par monad）和并发抽象（如IO和STM monads）无法同时满足确定性和破坏性更新的需求，而PSM旨在填补这一空白。

Method: 论文介绍了PSM（Policy Synchronised Memory）类型上下文，支持并发线程和共享状态，并通过策略协调确保无竞争和确定性。

Result: PSM上下文实现了并发、共享和确定性的统一，支持抽象数据结构的确定性行为。

Conclusion: PSM为Haskell提供了一种新的编程范式，解决了并发与确定性的矛盾，适用于需要确定性行为的并发编程场景。

Abstract: Concurrency and determinacy do not go well with each other when resources must be shared. Haskell provides parallel programming abstractions such as IVar and LVar in the Par monad and concurrent abstractions such as MVar and TVar in the in IO and STM monads, respectively. The former are determinate but have no destructive updates and the latter have destructive updates but do not guarantee determinacy. Programming patterns that are both concurrent and determinate, such as those provided by Kahn or Berry require memory abstractions at a higher level than is currently available. In this paper we describe a new type context PSM for policy synchronised memory in Haskell. Like STM and IO, the computations in PSM can access persistent state and, as a side-effect, update the memory in imperative style. Like the Par and IO monads, PSM supports concurrent threads and shared state. However, in contrast to IO, our PSM contexts are race-free since concurrent accesses are policy coordinated which guarantees determinacy.Well-typed transactions in the PSM context can accommodate abstract data structures that are imperative, concurrently shareable and still behave deterministically, by construction.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [3] [Towards Bug-Free Distributed Go Programs](https://arxiv.org/abs/2506.15135)
*Zhengqun Koo*

Main category: cs.SE

TL;DR: 提出了一种静态验证框架，用于证明使用Go子集的分布式程序中通信竞争的缺失。


<details>
  <summary>Details</summary>
Motivation: 分布式系统中的并发问题难以推理，通信竞争会导致接收者收到错误消息或无消息，引发错误。

Method: 扩展了happens-before顺序，静态分析分布式程序的执行，适用于缓冲和非缓冲通道。

Result: 框架能够证明程序在通信竞争方面的安全性。

Conclusion: 该框架为分布式程序提供了一种静态验证通信竞争缺失的有效方法。

Abstract: Programmers of distributed systems need to reason about concurrency to avoid races. However, reasoning about concurrency is difficult, and unexpected races show up as bugs. Data race detection in shared memory systems is well-studied (dynamic data race detection [13], behavioral types [15], dynamic race detection [31]). Similar to how a data race consists of reads and writes not related by happens-before at a shared memory location, a communication race consists of receives and sends not related by happens-before on a shared channel. Communication races are problematic: a receiver expects a specific message from a specific sender, but with a communication race, the receiver can receive a message meant for another receiver, or not receive anything at all. In this work, we describe a verification framework that can prove the absence of communication races for distributed programs that use a subset of the Go programming language, where synchronization is mainly achieved via message passing. We statically reason about how a distributed program executes, using a happens-before order, extended to buffered and unbuffered channels.

</details>
