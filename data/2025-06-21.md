<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.SE](#cs.SE) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [A Novel Compiler Transformation for Fast Sparse Matrix Multiplication in GPUs](https://arxiv.org/abs/2506.15174)
*Hossein Albakri,Kazem Cheshmi*

Main category: cs.PL

TL;DR: 提出了一种新的编译器转换方法，用于加速GPU上的稀疏矩阵乘法（SPMM），提高数据在寄存器和缓存中的重用率，并在稀疏神经网络中测试，取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 稀疏数据结构在神经网络中用于减少内存占用，但会导致内存访问不规则，影响GPU的计算和内存资源利用效率。

Method: 提出了一种名为enumerate-and-sparse-coarsen的编译器转换，优化SPMM在GPU上的执行，增加数据重用并平衡工作负载。

Result: 在A100 GPU上测试，与cuBLAS和cuSPARSE基线相比，性能提升几何平均为1.84倍至2.27倍。

Conclusion: 该方法显著提升了稀疏矩阵乘法在GPU上的性能，适用于卷积和Transformer模型。

Abstract: Sparse data structures are commonly used in neural networks to reduce the
memory footprint. These data structures are compact but cause irregularities
such as random memory accesses, which prevent efficient use of the memory
hierarchy. GPUs are a common platform for machine learning practitioners, but
running compact data structures on these devices often leads to slow-downs due
to inefficient use of computing and memory resources. This paper proposes a new
compiler transformation, enumerate-and-sparse-coarsen, that accelerates sparse
matrix-matrix multiplication (SPMM) on GPU devices. The transformation
increases data reuse in registers and caches while creating more balanced
workloads for GPU computing resources. The transformation is tested on sparse
neural networks in convolutional and transformer models. On an A100 GPU and
across a columns of matrix B (bCols) in $ A \times B = C$ from range of 32 to
128, the transformation yields a geometric mean speedup of 1.84$\times$ to
2.27$\times$ compared to cuBLAS and cuSPARSE baselines, respectively.

</details>


### [2] [PSM: Policy Synchronised Deterministic Memory](https://arxiv.org/abs/2506.15424)
*Michael Mendler,Marc Pouzet*

Main category: cs.PL

TL;DR: 论文提出了一种新的Haskell类型上下文PSM，用于实现并发且确定性的内存访问，解决了现有抽象（如Par和IO）在共享资源时无法兼顾并发性和确定性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有Haskell的并行和并发抽象（如IVar、LVar、MVar和TVar）无法同时满足并发性和确定性需求，限制了编程模式的灵活性。

Method: 引入PSM（Policy Synchronised Memory）类型上下文，支持并发线程和共享状态，并通过策略协调保证确定性。

Result: PSM实现了并发且确定性的内存访问，支持抽象数据结构的共享和确定性行为。

Conclusion: PSM为Haskell提供了一种新的并发编程抽象，兼具并发性和确定性，扩展了编程模式的可能性。

Abstract: Concurrency and determinacy do not go well with each other when resources
must be shared. Haskell provides parallel programming abstractions such as IVar
and LVar in the Par monad and concurrent abstractions such as MVar and TVar in
the in IO and STM monads, respectively. The former are determinate but have no
destructive updates and the latter have destructive updates but do not
guarantee determinacy. Programming patterns that are both concurrent and
determinate, such as those provided by Kahn or Berry require memory
abstractions at a higher level than is currently available. In this paper we
describe a new type context PSM for policy synchronised memory in Haskell. Like
STM and IO, the computations in PSM can access persistent state and, as a
side-effect, update the memory in imperative style. Like the Par and IO monads,
PSM supports concurrent threads and shared state. However, in contrast to IO,
our PSM contexts are race-free since concurrent accesses are policy coordinated
which guarantees determinacy.Well-typed transactions in the PSM context can
accommodate abstract data structures that are imperative, concurrently
shareable and still behave deterministically, by construction.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [3] [Towards Bug-Free Distributed Go Programs](https://arxiv.org/abs/2506.15135)
*Zhengqun Koo*

Main category: cs.SE

TL;DR: 提出了一种静态验证框架，用于证明使用Go语言子集的分布式程序中通信竞争的缺失。


<details>
  <summary>Details</summary>
Motivation: 分布式系统中的并发问题（如通信竞争）可能导致接收错误消息或无消息接收，需要一种方法来避免此类问题。

Method: 扩展了happens-before顺序，支持缓冲和非缓冲通道，静态分析分布式程序的执行。

Result: 框架能够证明程序在通信竞争方面的安全性。

Conclusion: 该框架为分布式程序提供了一种静态验证通信竞争缺失的有效方法。

Abstract: Programmers of distributed systems need to reason about concurrency to avoid
races. However, reasoning about concurrency is difficult, and unexpected races
show up as bugs. Data race detection in shared memory systems is well-studied
(dynamic data race detection [13], behavioral types [15], dynamic race
detection [31]). Similar to how a data race consists of reads and writes not
related by happens-before at a shared memory location, a communication race
consists of receives and sends not related by happens-before on a shared
channel. Communication races are problematic: a receiver expects a specific
message from a specific sender, but with a communication race, the receiver can
receive a message meant for another receiver, or not receive anything at all.
In this work, we describe a verification framework that can prove the absence
of communication races for distributed programs that use a subset of the Go
programming language, where synchronization is mainly achieved via message
passing. We statically reason about how a distributed program executes, using a
happens-before order, extended to buffered and unbuffered channels.

</details>
