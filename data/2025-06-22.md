<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.SE](#cs.SE) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [A Novel Compiler Transformation for Fast Sparse Matrix Multiplication in GPUs](https://arxiv.org/abs/2506.15174)
*Hossein Albakri,Kazem Cheshmi*

Main category: cs.PL

TL;DR: 论文提出了一种新的编译器转换方法，用于加速GPU上的稀疏矩阵乘法（SPMM），通过提高数据在寄存器和缓存中的重用率，优化GPU资源利用。


<details>
  <summary>Details</summary>
Motivation: 稀疏数据结构在神经网络中常用于减少内存占用，但其不规则性（如随机内存访问）导致GPU资源利用效率低下。

Method: 提出了一种名为enumerate-and-sparse-coarsen的编译器转换方法，优化SPMM在GPU上的执行效率。

Result: 在A100 GPU上测试，该方法在32到128的矩阵B列范围内，相比cuBLAS和cuSPARSE基准，分别实现了1.84倍和2.27倍的几何平均加速。

Conclusion: 该方法显著提升了稀疏矩阵乘法在GPU上的性能，适用于卷积和Transformer模型。

Abstract: Sparse data structures are commonly used in neural networks to reduce the
memory footprint. These data structures are compact but cause irregularities
such as random memory accesses, which prevent efficient use of the memory
hierarchy. GPUs are a common platform for machine learning practitioners, but
running compact data structures on these devices often leads to slow-downs due
to inefficient use of computing and memory resources. This paper proposes a new
compiler transformation, enumerate-and-sparse-coarsen, that accelerates sparse
matrix-matrix multiplication (SPMM) on GPU devices. The transformation
increases data reuse in registers and caches while creating more balanced
workloads for GPU computing resources. The transformation is tested on sparse
neural networks in convolutional and transformer models. On an A100 GPU and
across a columns of matrix B (bCols) in $ A \times B = C$ from range of 32 to
128, the transformation yields a geometric mean speedup of 1.84$\times$ to
2.27$\times$ compared to cuBLAS and cuSPARSE baselines, respectively.

</details>


### [2] [PSM: Policy Synchronised Deterministic Memory](https://arxiv.org/abs/2506.15424)
*Michael Mendler,Marc Pouzet*

Main category: cs.PL

TL;DR: 论文提出了一种新的Haskell类型上下文PSM，用于实现并发且确定性的内存共享，解决了传统方法中并发与确定性难以共存的问题。


<details>
  <summary>Details</summary>
Motivation: 传统Haskell中的并行和并发抽象（如IVar、MVar）无法同时满足确定性和共享内存的需求，PSM旨在填补这一空白。

Method: 引入PSM（Policy Synchronised Memory）类型上下文，支持并发线程和共享状态，并通过策略协调保证确定性。

Result: PSM实现了并发共享内存的确定性行为，且类型安全的交易支持抽象数据结构。

Conclusion: PSM为Haskell提供了一种新的并发确定性编程模式，解决了现有方法的局限性。

Abstract: Concurrency and determinacy do not go well with each other when resources
must be shared. Haskell provides parallel programming abstractions such as IVar
and LVar in the Par monad and concurrent abstractions such as MVar and TVar in
the in IO and STM monads, respectively. The former are determinate but have no
destructive updates and the latter have destructive updates but do not
guarantee determinacy. Programming patterns that are both concurrent and
determinate, such as those provided by Kahn or Berry require memory
abstractions at a higher level than is currently available. In this paper we
describe a new type context PSM for policy synchronised memory in Haskell. Like
STM and IO, the computations in PSM can access persistent state and, as a
side-effect, update the memory in imperative style. Like the Par and IO monads,
PSM supports concurrent threads and shared state. However, in contrast to IO,
our PSM contexts are race-free since concurrent accesses are policy coordinated
which guarantees determinacy.Well-typed transactions in the PSM context can
accommodate abstract data structures that are imperative, concurrently
shareable and still behave deterministically, by construction.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [3] [Towards Bug-Free Distributed Go Programs](https://arxiv.org/abs/2506.15135)
*Zhengqun Koo*

Main category: cs.SE

TL;DR: 本文提出了一种验证框架，用于证明使用Go语言子集的分布式程序中通信竞争的缺失。


<details>
  <summary>Details</summary>
Motivation: 分布式系统中的并发问题可能导致通信竞争，引发程序错误，因此需要一种方法来静态验证其缺失。

Method: 通过扩展happens-before顺序，静态分析分布式程序的执行，覆盖缓冲和非缓冲通道。

Result: 该框架能够有效证明分布式程序中通信竞争的缺失。

Conclusion: 提出的方法为分布式程序的通信竞争问题提供了一种静态验证解决方案。

Abstract: Programmers of distributed systems need to reason about concurrency to avoid
races. However, reasoning about concurrency is difficult, and unexpected races
show up as bugs. Data race detection in shared memory systems is well-studied
(dynamic data race detection [13], behavioral types [15], dynamic race
detection [31]). Similar to how a data race consists of reads and writes not
related by happens-before at a shared memory location, a communication race
consists of receives and sends not related by happens-before on a shared
channel. Communication races are problematic: a receiver expects a specific
message from a specific sender, but with a communication race, the receiver can
receive a message meant for another receiver, or not receive anything at all.
In this work, we describe a verification framework that can prove the absence
of communication races for distributed programs that use a subset of the Go
programming language, where synchronization is mainly achieved via message
passing. We statically reason about how a distributed program executes, using a
happens-before order, extended to buffered and unbuffered channels.

</details>
