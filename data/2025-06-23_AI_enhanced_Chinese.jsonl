{"id": "2506.15875", "pdf": "https://arxiv.org/pdf/2506.15875", "abs": "https://arxiv.org/abs/2506.15875", "authors": ["Dirk Van Essendelft", "Patrick Wingo", "Terry Jordan", "Ryan Smith", "Wissam Saidi"], "title": "A System Level Compiler for Massively-Parallel, Spatial, Dataflow Architectures", "categories": ["cs.PL", "cs.AR", "cs.DC", "cs.ET", "D.3; D.1; I.6; J.2"], "comment": "26 pages, 5 figures, 14 listings", "summary": "We have developed a novel compiler called the Multiple-Architecture Compiler\nfor Advanced Computing Hardware (MACH) designed specifically for\nmassively-parallel, spatial, dataflow architectures like the Wafer Scale\nEngine. Additionally, MACH can execute code on traditional unified-memory\ndevices. MACH addresses the complexities in compiling for spatial architectures\nthrough a conceptual Virtual Machine, a flexible domain-specific language, and\na compiler that can lower high-level languages to machine-specific code in\ncompliance with the Virtual Machine concept. While MACH is designed to be\noperable on several architectures and provide the flexibility for several\nstandard and user-defined data mappings, we introduce the concept with dense\ntensor examples from NumPy and show lowering to the Wafer Scale Engine by\ntargeting Cerebras' hardware specific languages.", "AI": {"tldr": "MACH\u662f\u4e00\u79cd\u65b0\u578b\u7f16\u8bd1\u5668\uff0c\u4e13\u4e3a\u5927\u89c4\u6a21\u5e76\u884c\u3001\u7a7a\u95f4\u6570\u636e\u6d41\u67b6\u6784\u8bbe\u8ba1\uff0c\u540c\u65f6\u652f\u6301\u4f20\u7edf\u7edf\u4e00\u5185\u5b58\u8bbe\u5907\u3002\u5b83\u901a\u8fc7\u865a\u62df\u673a\u548c\u9886\u57df\u7279\u5b9a\u8bed\u8a00\u7b80\u5316\u4e86\u7a7a\u95f4\u67b6\u6784\u7684\u7f16\u8bd1\u590d\u6742\u6027\u3002", "motivation": "\u89e3\u51b3\u7a7a\u95f4\u67b6\u6784\u7f16\u8bd1\u7684\u590d\u6742\u6027\uff0c\u63d0\u4f9b\u8de8\u67b6\u6784\u7684\u7075\u6d3b\u6027\u548c\u517c\u5bb9\u6027\u3002", "method": "\u91c7\u7528\u865a\u62df\u673a\u6982\u5ff5\u3001\u9886\u57df\u7279\u5b9a\u8bed\u8a00\u548c\u7f16\u8bd1\u5668\uff0c\u5c06\u9ad8\u7ea7\u8bed\u8a00\u8f6c\u6362\u4e3a\u673a\u5668\u7279\u5b9a\u4ee3\u7801\u3002", "result": "\u6210\u529f\u5c55\u793a\u4e86\u4eceNumPy\u5bc6\u96c6\u5f20\u91cf\u5230Wafer Scale Engine\u7684\u4ee3\u7801\u8f6c\u6362\u3002", "conclusion": "MACH\u4e3a\u7a7a\u95f4\u67b6\u6784\u7f16\u8bd1\u63d0\u4f9b\u4e86\u9ad8\u6548\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5c55\u793a\u4e86\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.16048", "pdf": "https://arxiv.org/pdf/2506.16048", "abs": "https://arxiv.org/abs/2506.16048", "authors": ["Byeongjee Kang", "Harsh Desai", "Limin Jia", "Brandon Lucia"], "title": "WAMI: Compilation to WebAssembly through MLIR without Losing Abstraction", "categories": ["cs.PL"], "comment": null, "summary": "WebAssembly (Wasm) is a portable bytecode format that serves as a compilation\ntarget for high-level languages, enabling their secure and efficient execution\nacross diverse platforms, including web browsers and embedded systems. To\nimprove support for high-level languages without incurring significant code\nsize or performance overheads, Wasm continuously evolves by integrating\nhigh-level features such as Garbage Collection and Stack Switching. However,\nexisting compilation approaches either lack reusable design -- requiring\nredundant implementation efforts for each language -- or lose abstraction by\nlowering high-level constructs into low-level shared representations like LLVM\nIR, which hinder the adoption of high-level features. MLIR compiler\ninfrastructure provides the compilation pipeline with multiple levels of\nabstraction, preserving high-level abstractions throughout the compilation\npipeline, yet the current MLIR pipeline relies on the LLVM backend for Wasm\ncode generation, thereby inheriting LLVM's limitations.\n  This paper presents a novel compilation pipeline for Wasm, featuring Wasm\ndialects explicitly designed to represent high-level Wasm constructs within\nMLIR. Our approach enables direct generation of high-level Wasm code from\ncorresponding high-level MLIR dialects without losing abstraction, providing a\nmodular and extensible way to incorporate high-level Wasm features. We\nillustrate this extensibility through a case study that leverages Stack\nSwitching, a recently introduced high-level feature of Wasm. Performance\nevaluations on PolyBench benchmarks show that our pipeline, benefiting from\noptimizations within the MLIR and Wasm ecosystems, produces code with at most\n7.7\\% slower, and faster in some execution environments, compared to LLVM-based\ncompilers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMLIR\u7684\u65b0\u578bWebAssembly\uff08Wasm\uff09\u7f16\u8bd1\u6d41\u6c34\u7ebf\uff0c\u901a\u8fc7\u8bbe\u8ba1Wasm\u65b9\u8a00\u76f4\u63a5\u751f\u6210\u9ad8\u7ea7Wasm\u4ee3\u7801\uff0c\u907f\u514d\u4e86\u62bd\u8c61\u4e22\u5931\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u63a5\u8fd1\u6216\u4f18\u4e8e\u57fa\u4e8eLLVM\u7684\u7f16\u8bd1\u5668\u3002", "motivation": "\u73b0\u6709\u7684Wasm\u7f16\u8bd1\u65b9\u6cd5\u8981\u4e48\u7f3a\u4e4f\u53ef\u91cd\u7528\u8bbe\u8ba1\uff0c\u8981\u4e48\u56e0\u964d\u4f4e\u9ad8\u7ea7\u6784\u9020\u4e3a\u4f4e\u7ea7\u8868\u793a\uff08\u5982LLVM IR\uff09\u800c\u4e22\u5931\u62bd\u8c61\uff0c\u963b\u788d\u4e86\u9ad8\u7ea7\u7279\u6027\u7684\u91c7\u7528\u3002", "method": "\u5229\u7528MLIR\u7684\u591a\u5c42\u6b21\u62bd\u8c61\u80fd\u529b\uff0c\u8bbe\u8ba1Wasm\u65b9\u8a00\u4ee5\u76f4\u63a5\u751f\u6210\u9ad8\u7ea7Wasm\u4ee3\u7801\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u5176\u53ef\u6269\u5c55\u6027\u3002", "result": "\u5728PolyBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u65b0\u6d41\u6c34\u7ebf\u751f\u6210\u7684\u4ee3\u7801\u6027\u80fd\u6700\u591a\u6bd4\u57fa\u4e8eLLVM\u7684\u7f16\u8bd1\u5668\u61627.7%\uff0c\u5728\u67d0\u4e9b\u73af\u5883\u4e0b\u66f4\u5feb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u548c\u53ef\u6269\u5c55\u7684\u65b9\u5f0f\uff0c\u652f\u6301\u9ad8\u7ea7Wasm\u7279\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6027\u80fd\u7ade\u4e89\u529b\u3002"}}
{"id": "2506.16883", "pdf": "https://arxiv.org/pdf/2506.16883", "abs": "https://arxiv.org/abs/2506.16883", "authors": ["Christoph Jung", "C. F. Bolz-Tereick"], "title": "Low Overhead Allocation Sampling in a Garbage Collected Virtual Machine", "categories": ["cs.PL"], "comment": null, "summary": "Compared to the more commonly used time-based profiling, allocation profiling\nprovides an alternate view of the execution of allocation heavy dynamically\ntyped languages. However, profiling every single allocation in a program is\nvery inefficient. We present a sampling allocation profiler that is deeply\nintegrated into the garbage collector of PyPy, a Python virtual machine. This\nintegration ensures tunable low overhead for the allocation profiler, which we\nmeasure and quantify. Enabling allocation sampling profiling with a sampling\nperiod of 4 MB leads to a maximum time overhead of 25% in our benchmarks, over\nun-profiled regular execution.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u91c7\u6837\u5206\u914d\u5206\u6790\u5668\uff0c\u96c6\u6210\u5230PyPy\u7684\u5783\u573e\u56de\u6536\u5668\u4e2d\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5206\u6790\u5f00\u9500\u3002", "motivation": "\u52a8\u6001\u7c7b\u578b\u8bed\u8a00\u4e2d\u5206\u914d\u5206\u6790\u6bd4\u65f6\u95f4\u5206\u6790\u66f4\u6709\u6548\uff0c\u4f46\u5168\u90e8\u5206\u914d\u5206\u6790\u6548\u7387\u4f4e\u3002", "method": "\u5728PyPy\u7684\u5783\u573e\u56de\u6536\u5668\u4e2d\u96c6\u6210\u91c7\u6837\u5206\u914d\u5206\u6790\u5668\uff0c\u53ef\u8c03\u8282\u91c7\u6837\u5468\u671f\u3002", "result": "\u91c7\u6837\u5468\u671f\u4e3a4MB\u65f6\uff0c\u6700\u5927\u65f6\u95f4\u5f00\u9500\u4e3a25%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4f4e\u5f00\u9500\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5206\u914d\u5206\u6790\u3002"}}
{"id": "2506.16065", "pdf": "https://arxiv.org/pdf/2506.16065", "abs": "https://arxiv.org/abs/2506.16065", "authors": ["Geonho Hwang", "Wonyeol Lee", "Yeachan Park", "Sejun Park", "Feras Saad"], "title": "Floating-Point Neural Networks Are Provably Robust Universal Approximators", "categories": ["cs.LG", "cs.LO", "cs.PL"], "comment": "70 pages, 4 figures. Appearing in CAV 2025", "summary": "The classical universal approximation (UA) theorem for neural networks\nestablishes mild conditions under which a feedforward neural network can\napproximate a continuous function $f$ with arbitrary accuracy. A recent result\nshows that neural networks also enjoy a more general interval universal\napproximation (IUA) theorem, in the sense that the abstract interpretation\nsemantics of the network using the interval domain can approximate the direct\nimage map of $f$ (i.e., the result of applying $f$ to a set of inputs) with\narbitrary accuracy. These theorems, however, rest on the unrealistic assumption\nthat the neural network computes over infinitely precise real numbers, whereas\ntheir software implementations in practice compute over finite-precision\nfloating-point numbers. An open question is whether the IUA theorem still holds\nin the floating-point setting.\n  This paper introduces the first IUA theorem for floating-point neural\nnetworks that proves their remarkable ability to perfectly capture the direct\nimage map of any rounded target function $f$, showing no limits exist on their\nexpressiveness. Our IUA theorem in the floating-point setting exhibits material\ndifferences from the real-valued setting, which reflects the fundamental\ndistinctions between these two computational models. This theorem also implies\nsurprising corollaries, which include (i) the existence of provably robust\nfloating-point neural networks; and (ii) the computational completeness of the\nclass of straight-line programs that use only floating-point additions and\nmultiplications for the class of all floating-point programs that halt.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u7b2c\u4e00\u4e2a\u9002\u7528\u4e8e\u6d6e\u70b9\u795e\u7ecf\u7f51\u7edc\u7684\u533a\u95f4\u901a\u7528\u903c\u8fd1\u5b9a\u7406\uff08IUA\uff09\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u6d6e\u70b9\u8ba1\u7b97\u73af\u5883\u4e0b\u4ecd\u80fd\u5b8c\u7f8e\u903c\u8fd1\u76ee\u6807\u51fd\u6570\u7684\u76f4\u63a5\u56fe\u50cf\u6620\u5c04\u3002", "motivation": "\u7ecf\u5178\u901a\u7528\u903c\u8fd1\u5b9a\u7406\u5047\u8bbe\u795e\u7ecf\u7f51\u7edc\u5728\u65e0\u9650\u7cbe\u5ea6\u7684\u5b9e\u6570\u4e0a\u8ba1\u7b97\uff0c\u800c\u5b9e\u9645\u8f6f\u4ef6\u5b9e\u73b0\u4f7f\u7528\u6709\u9650\u7cbe\u5ea6\u7684\u6d6e\u70b9\u6570\u3002\u672c\u6587\u65e8\u5728\u63a2\u8ba8\u6d6e\u70b9\u795e\u7ecf\u7f51\u7edc\u662f\u5426\u4ecd\u6ee1\u8db3IUA\u5b9a\u7406\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u8bc1\u660e\u4e86\u6d6e\u70b9\u795e\u7ecf\u7f51\u7edc\u5728\u6d6e\u70b9\u8ba1\u7b97\u73af\u5883\u4e0b\u4ecd\u80fd\u5b8c\u7f8e\u903c\u8fd1\u76ee\u6807\u51fd\u6570\u7684\u76f4\u63a5\u56fe\u50cf\u6620\u5c04\u3002", "result": "\u6d6e\u70b9\u795e\u7ecf\u7f51\u7edc\u5728\u6d6e\u70b9\u73af\u5883\u4e0b\u4ecd\u5177\u6709\u5f3a\u5927\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u4e14\u4e0e\u5b9e\u6570\u73af\u5883\u4e0b\u7684IUA\u5b9a\u7406\u5b58\u5728\u672c\u8d28\u5dee\u5f02\u3002\u6b64\u5916\uff0c\u8fd8\u5f97\u51fa\u4e86\u4e00\u4e9b\u610f\u5916\u63a8\u8bba\u3002", "conclusion": "\u6d6e\u70b9\u795e\u7ecf\u7f51\u7edc\u5728\u6d6e\u70b9\u8ba1\u7b97\u73af\u5883\u4e0b\u4ecd\u6ee1\u8db3IUA\u5b9a\u7406\uff0c\u4e14\u5176\u8868\u8fbe\u80fd\u529b\u4e0d\u53d7\u9650\u5236\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u6d6e\u70b9\u8ba1\u7b97\u6a21\u578b\u7684\u72ec\u7279\u6027\u8d28\u3002"}}
