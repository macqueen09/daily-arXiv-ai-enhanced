{"id": "2506.22169", "pdf": "https://arxiv.org/pdf/2506.22169", "abs": "https://arxiv.org/abs/2506.22169", "authors": ["Zheng Zhang", "Donglin Yang", "Xiaobo Zhou", "Dazhao Cheng"], "title": "MCFuser: High-Performance and Rapid Fusion of Memory-Bound Compute-Intensive Operators", "categories": ["cs.DC", "cs.PL"], "comment": "12 pages, accepted at SC 2024", "summary": "Operator fusion, a key technique to improve data locality and alleviate GPU\nmemory bandwidth pressure, often fails to extend to the fusion of multiple\ncompute-intensive operators due to saturated computation throughput. However,\nthe dynamicity of tensor dimension sizes could potentially lead to these\noperators becoming memory-bound, necessitating the generation of fused kernels,\na task hindered by limited search spaces for fusion strategies, redundant\nmemory access, and prolonged tuning time, leading to sub-optimal performance\nand inefficient deployment.\n  We introduce MCFuser, a pioneering framework designed to overcome these\nobstacles by generating high-performance fused kernels for what we define as\nmemory-bound compute-intensive (MBCI) operator chains. Leveraging high-level\ntiling expressions to delineate a comprehensive search space, coupled with\nDirected Acyclic Graph (DAG) analysis to eliminate redundant memory accesses,\nMCFuser streamlines kernel optimization. By implementing guidelines to prune\nthe search space and incorporating an analytical performance model with a\nheuristic search, MCFuser not only significantly accelerates the tuning process\nbut also demonstrates superior performance. Benchmarked against leading\ncompilers like Ansor on NVIDIA A100 and RTX3080 GPUs, MCFuser achieves up to a\n5.9x speedup in kernel performance and outpaces other baselines while reducing\ntuning time by over 70-fold, showcasing its agility."}
{"id": "2506.22323", "pdf": "https://arxiv.org/pdf/2506.22323", "abs": "https://arxiv.org/abs/2506.22323", "authors": ["Alessio Di Santo"], "title": "Under the Hood of BlotchyQuasar: DLL-Based RAT Campaigns Against Latin America", "categories": ["cs.CR", "cs.CY", "cs.NI", "cs.OS", "cs.PL"], "comment": null, "summary": "A sophisticated malspam campaign was recently uncovered targeting Latin\nAmerican countries, with a particular focus on Brazil. This operation utilizes\na highly deceptive phishing email to trick users into executing a malicious MSI\nfile, initiating a multi-stage infection. The core of the attack leverages DLL\nside-loading, where a legitimate executable from Valve Corporation is used to\nload a trojanized DLL, thereby bypassing standard security defenses.\n  Once active, the malware, a variant of QuasarRAT known as BlotchyQuasar, is\ncapable of a wide range of malicious activities. It is designed to steal\nsensitive browser-stored credentials and banking information, the latter\nthrough fake login windows mimicking well-known Brazilian banks. The threat\nestablishes persistence by modifying the Windows registry , captures user\nkeystrokes through keylogging , and exfiltrates stolen data to a\nCommand-and-Control (C2) server using encrypted payloads. Despite its advanced\ncapabilities, the malware code exhibits signs of rushed development, with\ninefficiencies and poor error handling that suggest the threat actors\nprioritized rapid deployment over meticulous design. Nonetheless, the campaign\nextensive reach and sophisticated mechanisms pose a serious and immediate\nthreat to the targeted regions, underscoring the need for robust cybersecurity\ndefenses."}
{"id": "2506.22370", "pdf": "https://arxiv.org/pdf/2506.22370", "abs": "https://arxiv.org/abs/2506.22370", "authors": ["Carolina Carreira", "√Ålvaro Silva", "Alexandre Abreu", "Alexandra Mendes"], "title": "Can Large Language Models Help Students Prove Software Correctness? An Experimental Study with Dafny", "categories": ["cs.SE", "cs.PL"], "comment": null, "summary": "Students in computing education increasingly use large language models (LLMs)\nsuch as ChatGPT. Yet, the role of LLMs in supporting cognitively demanding\ntasks, like deductive program verification, remains poorly understood. This\npaper investigates how students interact with an LLM when solving formal\nverification exercises in Dafny, a language that supports functional\ncorrectness, by allowing programmers to write formal specifications and\nautomatically verifying that the implementation satisfies the specification. We\nconducted a mixed-methods study with master's students enrolled in a formal\nmethods course. Each participant completed two verification problems, one with\naccess to a custom ChatGPT interface, that logged all interactions, and the\nother without. We identified strategies used by successful students and\nassessed the level of trust students place in LLMs. %\\todo{Our findings show\nthat something here} Our findings show that students perform significantly\nbetter when using ChatGPT; however, performance gains are tied to prompt\nquality. We conclude with practical recommendations for integrating LLMs into\nformal methods courses more effectively, including designing LLM-aware\nchallenges that promote learning rather than substitution."}
