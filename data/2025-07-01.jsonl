{"id": "2506.23058", "pdf": "https://arxiv.org/pdf/2506.23058", "abs": "https://arxiv.org/abs/2506.23058", "authors": ["Nikolaj Hey Hinnerskov", "Robert Schenck", "Cosmin E. Oancea"], "title": "Verifying Properties of Index Arrays in a Purely-Functional Data-Parallel Language", "categories": ["cs.PL", "cs.DC"], "comment": null, "summary": "This paper presents a novel approach to automatically verify properties of\npure data-parallel programs with non-linear indexing -- expressed as pre- and\npost-conditions on functions. Programs consist of nests of second-order array\ncombinators (e.g., map, scan, and scatter) and loops. The key idea is to\nrepresent arrays as index functions: programs are index function\ntransformations over which properties are propagated and inferred. Our\nframework proves properties on index functions by distilling them into\nalgebraic (in)equalities and discharging them to a Fourier-Motzkin-based\nsolver. The framework is practical and accessible: properties are not\nrestricted to a decidable logic, but instead are carefully selected to express\npractically useful guarantees that can be automatically reasoned about and\ninferred. These guarantees extend beyond program correctness and can be\nexploited by the entire compiler pipeline for optimization. We implement our\nsystem in the pure data-parallel language Futhark and demonstrate its\npracticality on seven applications, reporting an average verification time of 1\nsecond. Two case studies show how eliminating dynamic verification in GPU\nprograms results in significant speedups."}
{"id": "2506.23320", "pdf": "https://arxiv.org/pdf/2506.23320", "abs": "https://arxiv.org/abs/2506.23320", "authors": ["Nicola Assolini", "Alessandra Di Pierro"], "title": "A Denotational Semantics for Quantum Loops", "categories": ["cs.PL"], "comment": "17 pages", "summary": "Programming a quantum computer, i.e., implementing quantum algorithms on a\nquantum processor-based copmputer architecture, is a task that can be addressed\n(just as for classical computers) at different levels of abstraction. This\npaper proposes a denotational semantics for high-level quantum programming\nconstructs, focusing on the conceptual meaning of quantum-controlled branching\nand iteration. We introduce a denotational domain where a mathematical meaning\nof a quantum control flow with loops can be defined, which reflects the\ncoherent evolution of the quantum system implementing the program."}
{"id": "2506.23407", "pdf": "https://arxiv.org/pdf/2506.23407", "abs": "https://arxiv.org/abs/2506.23407", "authors": ["Marcus Edwards"], "title": "Compiling a Q# Subset to QASM 3.0 in TypeScript via a JSON Based IR", "categories": ["cs.PL", "quant-ph"], "comment": null, "summary": "We implement a compile toolchain from Q# to QASM 3.0 including a\nfull-featured lexer and parser implementation, as well as a compiler that\nsupports a subset of Q# features. The lexer, parser and compiler are shown to\nwork with various input Q# programs and the implementation is compared against\nexisting Q# compile tools. Unlike the Microsoft implementation of the official\nQ# compile toolchain, our implementation is written in TypeScript in order to\nport functionality to web environments."}
{"id": "2506.22776", "pdf": "https://arxiv.org/pdf/2506.22776", "abs": "https://arxiv.org/abs/2506.22776", "authors": ["Sen Fang", "Weiyuan Ding", "Antonio Mastropaolo", "Bowen Xu"], "title": "Smaller = Weaker? Benchmarking Robustness of Quantized LLMs in Code Generation", "categories": ["cs.SE", "cs.AI", "cs.PL"], "comment": "13 pages, 6 figures", "summary": "Quantization has emerged as a mainstream method for compressing Large\nLanguage Models (LLMs), reducing memory requirements and accelerating inference\nwithout architectural modifications. While existing research primarily focuses\non evaluating the effectiveness of quantized LLMs compared to their original\ncounterparts, the impact on robustness remains largely unexplored.In this\npaper, we present the first systematic investigation of how quantization\naffects the robustness of LLMs in code generation tasks. Through extensive\nexperiments across four prominent LLM families (LLaMA, DeepSeek, CodeGen, and\nStarCoder) with parameter scales ranging from 350M to 33B, we evaluate\nrobustness from dual perspectives: adversarial attacks on input prompts and\nnoise perturbations on model architecture. Our findings challenge conventional\nwisdom by demonstrating that quantized LLMs often exhibit superior robustness\ncompared to their full-precision counterparts, with 51.59% versus 42.86% of our\nadversarial experiments showing better resilience in quantized LLMs. Similarly,\nour noise perturbation experiments also confirm that LLMs after quantitation\ngenerally withstand higher levels of weight disturbances. These results suggest\nthat quantization not only reduces computational requirements but can actually\nenhance LLMs' reliability in code generation tasks, providing valuable insights\nfor developing more robust and efficient LLM deployment strategies."}
{"id": "2506.23281", "pdf": "https://arxiv.org/pdf/2506.23281", "abs": "https://arxiv.org/abs/2506.23281", "authors": ["Xintong Zhou", "Zhenyang Xu", "Chengnian Sun"], "title": "On the Feasibility of Deduplicating Compiler Bugs with Bisection", "categories": ["cs.SE", "cs.PL"], "comment": null, "summary": "Random testing has proven to be an effective technique for compiler\nvalidation. However, the debugging of bugs identified through random testing\npresents a significant challenge due to the frequent occurrence of duplicate\ntest programs that expose identical compiler bugs. The process to identify\nduplicates is a practical research problem known as bug deduplication. Prior\nmethodologies for compiler bug deduplication primarily rely on program analysis\nto extract bug-related features for duplicate identification, which can result\nin substantial computational overhead and limited generalizability. This paper\ninvestigates the feasibility of employing bisection, a standard debugging\nprocedure largely overlooked in prior research on compiler bug deduplication,\nfor this purpose. Our study demonstrates that the utilization of bisection to\nlocate failure-inducing commits provides a valuable criterion for\ndeduplication, albeit one that requires supplementary techniques for more\naccurate identification. Building on these results, we introduce BugLens, a\nnovel deduplication method that primarily uses bisection, enhanced by the\nidentification of bug-triggering optimizations to minimize false negatives.\nEmpirical evaluations conducted on four real-world datasets demonstrate that\nBugLens significantly outperforms the state-of-the-art analysis-based\nmethodologies Tamer and D3 by saving an average of 26.98% and 9.64% human\neffort to identify the same number of distinct bugs. Given the inherent\nsimplicity and generalizability of bisection, it presents a highly practical\nsolution for compiler bug deduplication in real-world applications."}
{"id": "2506.23696", "pdf": "https://arxiv.org/pdf/2506.23696", "abs": "https://arxiv.org/abs/2506.23696", "authors": ["Francisco Oliveira", "Alexandra Mendes", "Carolina Carreira"], "title": "What Challenges Do Developers Face When Using Verification-Aware Programming Languages?", "categories": ["cs.SE", "cs.PL"], "comment": null, "summary": "Software reliability is critical in ensuring that the digital systems we\ndepend on function correctly. In software development, increasing software\nreliability often involves testing. However, for complex and critical systems,\ndevelopers can use Design by Contract (DbC) methods to define precise\nspecifications that software components must satisfy. Verification-Aware (VA)\nprogramming languages support DbC and formal verification at compile-time or\nrun-time, offering stronger correctness guarantees than traditional testing.\nHowever, despite the strong guarantees provided by VA languages, their adoption\nremains limited. In this study, we investigate the barriers to adopting VA\nlanguages by analyzing developer discussions on public forums using topic\nmodeling techniques. We complement this analysis with a developer survey to\nbetter understand the practical challenges associated with VA languages. Our\nfindings reveal key obstacles to adoption, including steep learning curves and\nusability issues. Based on these insights, we identify actionable\nrecommendations to improve the usability and accessibility of VA languages. Our\nfindings suggest that simplifying tool interfaces, providing better educational\nmaterials, and improving integration with everyday development environments\ncould improve the usability and adoption of these languages. Our work provides\nactionable insights for improving the usability of VA languages and making\nverification tools more accessible."}
