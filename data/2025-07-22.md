<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 8]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.SE](#cs.SE) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [NPUEval: Optimizing NPU Kernels with LLMs and Open Source Compilers](https://arxiv.org/abs/2507.14403)
*Sarunas Kalade,Graham Schelle*

Main category: cs.PL

TL;DR: 论文介绍了NPUEval基准测试，用于评估NPU内核代码的生成，发现前沿LLM在向量化效率上表现有限。


<details>
  <summary>Details</summary>
Motivation: 解决NPU编程中因社区碎片化和缺乏优化代码示例导致的LLM辅助编程效率低下的问题。

Method: 提出NPUEval基准测试，包含102个常见算子，通过开源编译器工具评估LLM生成代码的功能正确性和向量化效率。

Result: 前沿LLM如DeepSeek R1在部分内核上达到50%+向量化，但整体平均仅10%，显示任务挑战性。

Conclusion: NPUEval为代码生成和NPU内核优化研究提供了重要基准，数据集和工具将开源。

Abstract: Neural processing units (NPUs) are gaining prominence in power-sensitive
devices like client devices, with AI PCs being defined by their inclusion of
these specialized processors. Running AI workloads efficiently on these devices
requires libraries of optimized kernels. Creating efficient kernels demands
expertise in domain-specific C++ with vector intrinsics and in-depth knowledge
of the target architecture. Unlike GPU programming, which has had years to
mature, NPU programming is new, with smaller and more fragmented developer
communities across hardware platforms. This fragmentation poses a challenge
when utilizing LLMs to assist in writing NPU kernels, as domain-specific
optimized code examples are underrepresented in LLM pre-training data.
  In this paper we introduce NPUEval -- a benchmark for writing and evaluating
NPU kernels, consisting of 102 common operators for machine learning workloads.
We evaluate LLM generated code on actual hardware based on both functional
correctness and vectorization efficiency using open source compiler tools
targeting the AMD NPU. We evaluate a range of state-of-the-art LLMs with a mix
of proprietary and open-weight models. Latest reasoning models like DeepSeek
R1, show promising results achieving out-of-the-box 50%+ vectorization on
select kernels. However, the average score across the entire dataset remains
roughly 10% even with compiler feedback and vectorized kernel examples --
showing that this is a challenging dataset even for frontier models. The
dataset and evaluation code will be released with a permissive open source
license, providing an essential benchmark for advancing research in code
generation and NPU kernel optimization.

</details>


### [2] [Timetide: A programming model for logically synchronous distributed systems](https://arxiv.org/abs/2507.14471)
*Logan Kenwright,Partha Roop,Nathan Allen,Călin Caşcaval,Avinash Malik*

Main category: cs.PL

TL;DR: 提出了一种名为Timetide的多时钟同步语言，解决了分布式系统中确定性编程的挑战，无需物理时钟同步。


<details>
  <summary>Details</summary>
Motivation: 传统同步语言依赖昂贵的物理时钟同步，难以扩展，分布式系统的确定性编程仍具挑战性。

Method: 开发了一种多时钟语义的同步程序，基于逻辑同步模型，无需物理时钟同步。

Result: Timetide是首个既适合分布式又支持形式化验证的多时钟同步语言。

Conclusion: Timetide为分布式系统提供了一种无需物理时钟同步的确定性编程解决方案。

Abstract: Massive strides in deterministic models have been made using synchronous
languages. They are mainly focused on centralised applications, as the
traditional approach is to compile away the concurrency. Time triggered
languages such as Giotto and Lingua Franca are suitable for distribution albeit
that they rely on expensive physical clock synchronisation, which is both
expensive and may suffer from scalability. Hence, deterministic programming of
distributed systems remains challenging. We address the challenges of
deterministic distribution by developing a novel multiclock semantics of
synchronous programs. The developed semantics is amenable to seamless
distribution. Moreover, our programming model, Timetide, alleviates the need
for physical clock synchronisation by building on the recently proposed logical
synchrony model for distributed systems. We discuss the important aspects of
distributing computation, such as network communication delays, and explore the
formal verification of Timetide programs. To the best of our knowledge,
Timetide is the first multiclock synchronous language that is both amenable to
distribution and formal verification without the need for physical clock
synchronisation or clock gating.

</details>


### [3] [Hear Your Code Fail, Voice-Assisted Debugging for Python](https://arxiv.org/abs/2507.15007)
*Sayed Mahbub Hasan Amiri,Md. Mainul Islam,Mohammad Shakhawat Hossen,Sayed Majhab Hasan Amiri,Mohammad Shawkat Ali Mamun,Sk. Humaun Kabir,Naznin Akter*

Main category: cs.PL

TL;DR: 论文介绍了一款创新的语音辅助调试插件，通过听觉和视觉反馈显著降低认知负荷并加快错误识别。


<details>
  <summary>Details</summary>
Motivation: 解决传统调试方法对视觉的依赖，提升编程可访问性和认知效率。

Method: 采用全局异常钩子架构，结合pyttsx3语音合成和Tkinter GUI，提供多模态错误反馈。

Result: 实验显示认知负荷降低37%，错误识别速度提升78%，兼容Python 3.7+环境。

Conclusion: 该插件为编程调试带来人本化革新，未来将整合GPT修复建议和多语言翻译。

Abstract: This research introduces an innovative voice-assisted debugging plugin for
Python that transforms silent runtime errors into actionable audible
diagnostics. By implementing a global exception hook architecture with pyttsx3
text-to-speech conversion and Tkinter-based GUI visualization, the solution
delivers multimodal error feedback through parallel auditory and visual
channels. Empirical evaluation demonstrates 37% reduced cognitive load (p<0.01,
n=50) compared to traditional stack-trace debugging, while enabling 78% faster
error identification through vocalized exception classification and
contextualization. The system achieves sub-1.2 second voice latency with under
18% CPU overhead during exception handling, vocalizing error types and
consequences while displaying interactive tracebacks with documentation deep
links. Criteria validate compatibility across Python 3.7+ environments on
Windows, macOS, and Linux platforms. Needing only two lines of integration
code, the plugin significantly boosts availability for aesthetically impaired
designers and supports multitasking workflows through hands-free error medical
diagnosis. Educational applications show particular promise, with pilot studies
indicating 45% faster debugging skill acquisition among novice programmers.
Future development will incorporate GPT-based repair suggestions and real-time
multilingual translation to further advance auditory debugging paradigms. The
solution represents a fundamental shift toward human-centric error diagnostics,
bridging critical gaps in programming accessibility while establishing new
standards for cognitive efficiency in software development workflows.

</details>


### [4] [Invariant Generation for Floating-Point Programs via Constraint Solving](https://arxiv.org/abs/2507.15017)
*Xuran Cai,Liqian Chen,Hongfei Fu*

Main category: cs.PL

TL;DR: 提出了一种基于约束求解方法的理论框架，用于生成浮点程序的不变量，结合FPTaylor的一阶微分特征和约束求解方法，显著提高了计算效率和生成不变量的精度。


<details>
  <summary>Details</summary>
Motivation: 浮点运算中的舍入误差累积可能导致程序故障，需确保浮点程序的正确性，因此需要生成考虑浮点误差的紧不变量。

Method: 结合FPTaylor的一阶微分特征和约束求解方法，提出两种多项式不变量生成算法：一种需要初始不变量输入，另一种适用于多项式程序但无需初始输入。

Result: 实验表明，算法在时间效率和生成不变量精度上优于现有方法。

Conclusion: 该框架有效解决了浮点程序不变量生成问题，尤其在处理条件分支时表现出色。

Abstract: In numeric-intensive computations, it is well known that the execution of
floating-point programs is imprecise as floating point arithmetics (e.g.,
addition, subtraction, multiplication, division, etc.) incurs rounding errors.
Albeit the rounding error is small for every single floating-point operation,
the aggregation of such error in multiple operations may be dramatic and cause
catastrophic program failures. Therefore, to ensure the correctness of
floating-point programs, the effect of floating point error needs to be
carefully taken into account. In this work, we consider the invariant
generation for floating point programs, whose aim is to generate tight
invariants under the perturbation of floating point errors. Our main
contribution is a theoretical framework on how to apply constraint solving
methods to address the invariant generation problem. In our framework, we
propose a novel combination between the first-order differential
characterization by FPTaylor (TOPLAS 2018) and constraint solving methods,
aiming to reduce the computational burden of constraint solving. Moreover, we
devise two polynomial invariant generation algorithms to instantiate the
framework. The first algorithm is applicable to a wide range of floating-point
operations but requires an initial (coarse) invariant as external input, while
the second does not require an initial invariant but is limited to polynomial
programs. Furthermore, we show how conditional branches, a difficult issue in
floating-point analysis, can be handled in our framework. Experimental results
show that our algorithms outperform SOTA approaches in both the time efficiency
and the precision of the generated invariants over a variety of benchmarks.

</details>


### [5] [A Few Fit Most: Improving Performance Portability of SGEMM on GPUs using Multi-Versioning](https://arxiv.org/abs/2507.15277)
*Robert Hochgraf,Sreepathi Pai*

Main category: cs.PL

TL;DR: 论文提出了一种多版本化方法（multi-versioning）来生成性能可移植的代码，避免了传统自动调优的过拟合问题，并在实验中表现优于CLBlast的默认内核。


<details>
  <summary>Details</summary>
Motivation: 手动优化线性代数内核以适应不同GPU设备和应用复杂且耗时，而自动调优（autotuning）易过拟合且需重复调优。

Method: 提出了一种称为“可移植性调优”的框架，通过生成多版本代码实现性能可移植性，无需重复调优。

Result: 在CLBlast的GEMM内核数据集上，该方法性能优于默认内核，接近理论最大性能的90%，且能泛化到新设备。

Conclusion: 多版本化方法是一种有效的性能可移植解决方案，优于传统自动调优。

Abstract: Hand-optimizing linear algebra kernels for different GPU devices and
applications is complex and labor-intensive. Instead, many developers use
automatic performance tuning (autotuning) to achieve high performance on a
variety of devices. However, autotuning "overfits", and must be redone if any
part of the environment changes, such as if the device or input characteristics
change.
  In most non-trivial cases, a single compute kernel cannot maintain
near-optimal performance across all environments. Changing the kernel to
specialize it to the current execution environment is possible, but on GPUs,
runtime tuning and compilation can be expensive.
  In this work, we use multi-versioning -- producing several variants of the
same code -- as a way to generate performance portable code. We describe a
framework called portability tuning that can automatically generate
multi-versioned code whose performance is portable, requiring no retuning.
  We evaluate our framework on a dataset of execution times for GEMM kernels
from the CLBlast linear algebra library. We find our portability tuning
techniques outperform CLBlast's default kernels -- often approaching within 10%
of the theoretical maximum performance -- despite CLBlast using autotuning
techniques. Further, we find that our generated programs generalize well to new
and unseen devices, matching the performance of autotuning without ever
portability tuning for those devices.

</details>


### [6] [Bayesian Separation Logic](https://arxiv.org/abs/2507.15530)
*Shing Hin Ho,Nicolas Wu,Azalea Raad*

Main category: cs.PL

TL;DR: 本文介绍了贝叶斯分离逻辑（BaSL），填补了现有分离逻辑无法处理贝叶斯更新的空白，并证明了其能建模贝叶斯编程语言的多种概念。


<details>
  <summary>Details</summary>
Motivation: 现有分离逻辑无法处理贝叶斯编程语言（BPPLs）的关键特征——贝叶斯更新，因此需要一种新的逻辑来填补这一空白。

Method: 作者提出贝叶斯分离逻辑（BaSL），基于Rokhlin-Simmons分解定理证明贝叶斯定理的内部版本，并利用σ-有限测度空间建模。

Result: BaSL成功建模了贝叶斯编程语言的多种概念（如贝叶斯更新、条件分布等），并验证了统计模型的属性（如期望值、相关性等）。

Conclusion: BaSL为贝叶斯编程语言提供了语义支持，填补了现有分离逻辑的不足，并展示了其在复杂统计模型中的应用潜力。

Abstract: Bayesian probabilistic programming languages (BPPLs) let users denote
statistical models as code while the interpreter infers the posterior
distribution. The semantics of BPPLs are usually mathematically complex and
unable to reason about desirable properties such as expected values and
independence of random variables. To reason about these properties in a
non-Bayesian setting, probabilistic separation logics such as PSL and Lilac
interpret separating conjunction as probabilistic independence of random
variables. However, no existing separation logic can handle Bayesian updating,
which is the key distinguishing feature of BPPLs.
  To close this gap, we introduce Bayesian separation logic (BaSL), a
probabilistic separation logic that gives semantics to BPPL. We prove an
internal version of Bayes' theorem using a result in measure theory known as
the Rokhlin-Simmons disintegration theorem. Consequently, BaSL can model
probabilistic programming concepts such as Bayesian updating, unnormalised
distribution, conditional distribution, soft constraint, conjugate prior and
improper prior while maintaining modularity via the frame rule. The model of
BaSL is based on a novel instantiation of Kripke resource monoid via
$\sigma$-finite measure spaces over the Hilbert cube, and the semantics of
Hoare triple is compatible with an existing denotational semantics of BPPL
based on the category of $s$-finite kernels. Using BaSL, we then prove
properties of statistical models such as the expected value of Bayesian coin
flip, correlation of random variables in the collider Bayesian network, and the
posterior distributions of the burglar alarm model, a parameter estimation
algorithm, and the Gaussian mixture model.

</details>


### [7] [Formal Analysis of Networked PLC Controllers Interacting with Physical Environments](https://arxiv.org/abs/2507.15596)
*Jaeseo Lee,Kyungmin Bae*

Main category: cs.PL

TL;DR: 提出了一种统一的PLC形式化框架，集成了离散PLC语义、网络通信和连续物理行为，并通过偏序减少状态爆炸问题。


<details>
  <summary>Details</summary>
Motivation: 现有PLC验证技术通常忽略物理环境和网络通信，难以分析真实工业系统中的连续动态和通信延迟。

Method: 提出统一框架，结合离散PLC语义、网络通信和连续物理行为，并应用偏序减少状态爆炸。

Result: 框架能够精确分析具有连续动态和网络通信的PLC驱动系统。

Conclusion: 该框架为复杂工业系统的正确性验证提供了有效解决方案。

Abstract: Programmable Logic Controllers (PLCs) are widely used in industrial
automation to control physical systems. As PLC applications become increasingly
complex, ensuring their correctness is crucial. Existing formal verification
techniques focus on individual PLC programs in isolation, often neglecting
interactions with physical environments and network communication between
controllers. This limitation poses significant challenges in analyzing
real-world industrial systems, where continuous dynamics and communication
delays play a critical role. In this paper, we present a unified formal
framework that integrates discrete PLC semantics, networked communication, and
continuous physical behaviors. To mitigate state explosion, we apply partial
order reduction, significantly reducing the number of explored states while
maintaining correctness. Our framework enables precise analysis of PLC-driven
systems with continuous dynamics and networked communication.

</details>


### [8] [Closure Conversion, Flat Environments, and the Complexity of Abstract Machines](https://arxiv.org/abs/2507.15843)
*Beniamino Accattoli,Dan Ghica,Giulio Guerrieri,Cláudio Belo Lourenço,Claudio Sacerdoti Coen*

Main category: cs.PL

TL;DR: 本文研究了闭包转换与抽象机器之间的关系，提出了新的证明技术、改进的环境处理方法，并分析了时间复杂性。


<details>
  <summary>Details</summary>
Motivation: 探讨闭包转换与抽象机器中闭包和环境概念的异同，以改进相关技术。

Method: 采用简单的λ-演算和元组作为源语言，研究闭包转换前后的抽象机器，并分析其时间复杂性。

Result: 提出了新的闭包转换正确性证明技术，改进了环境处理方法，并发现闭包转换不影响整体复杂性。

Conclusion: 闭包转换虽增加代码大小，但动态成本降低，整体复杂性不变。

Abstract: Closure conversion is a program transformation at work in compilers for
functional languages to turn inner functions into global ones, by building
closures pairing the transformed functions with the environment of their free
variables. Abstract machines rely on similar and yet different concepts of
closures and environments.
  In this paper, we study the relationship between the two approaches. We adopt
a very simple {\lambda}-calculus with tuples as source language and study
abstract machines for both the source language and the target of closure
conversion. Moreover, we focus on the simple case of flat
closures/environments, that is, with no sharing of environments. We provide
three contributions.
  Firstly, a new simple proof technique for the correctness of closure
conversion, inspired by abstract machines.
  Secondly, we show how the closure invariants of the target language allow us
to design a new way of handling environments in abstract machines, not
suffering the shortcomings of other styles.
  Thirdly, we study the machines from the point of view of time complexity,
adapting analyses by Accattoli and co-authors. We show that closure conversion
decreases various dynamic costs while increasing the size of the initial code.
Despite these changes, the overall complexity of the machines before and after
closure conversion turns out to be the same.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [9] [Quantum Programming in Polylogarithmic Time](https://arxiv.org/abs/2507.15415)
*Florent Ferrari,Emmanuel Hainry,Romain Péchoux,Mário Silva*

Main category: cs.LO

TL;DR: 论文介绍了一种量子编程语言，首次以编程语言为基础描述了FBQPOLYLOG类，证明了其完备性和可靠性，并提供了程序到量子电路的编译策略。


<details>
  <summary>Details</summary>
Motivation: 研究如何在量子计算模型中实现多对数时间（polylogarithmic time）的可行性，并探索FBQPOLYLOG类的编程语言表征。

Method: 提出一种支持一阶递归过程的量子编程语言，证明其能计算FBQPOLYLOG类中的所有函数（完备性），且每个程序计算的函数均属于该类（可靠性）。

Result: 实现了FBQPOLYLOG的编程语言表征，并编译为多对数深度、多项式大小的量子电路（QNC类），验证了FBQPOLYLOG是QNC的真子集。

Conclusion: 该工作为量子计算中的多对数时间复杂性提供了编程语言层面的理论支持，并揭示了FBQPOLYLOG与QNC的关系。

Abstract: Polylogarithmic time delineates a relevant notion of feasibility on several
classical computational models such as Boolean circuits or parallel random
access machines. As far as the quantum paradigm is concerned, this notion
yields the complexity class FBQPOLYLOG of functions approximable in
polylogarithmic time with a quantum random-access Turing machine. We introduce
a quantum programming language with first-order recursive procedures, which
provides the first programming-language-based characterization of FBQPOLYLOG.
Each program computes a function in FBQPOLYLOG (soundness) and, conversely,
each function of this complexity class is computed by a program (completeness).
We also provide a compilation strategy from programs to uniform families of
quantum circuits of polylogarithmic depth and polynomial size, whose set of
computed functions is known as QNC, and recover the well-known separation
result FBQPOLYLOG $\subsetneq$ QNC.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [10] [On the Effectiveness of Large Language Models in Writing Alloy Formulas](https://arxiv.org/abs/2502.15441)
*Yang Hong,Shan Jiang,Yulei Fu,Sarfraz Khurshid*

Main category: cs.SE

TL;DR: 论文探讨了使用大型语言模型（LLMs）生成Alloy声明式公式的实验，展示了LLMs在从自然语言描述生成完整公式、创建等效公式以及补全公式草稿方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 声明式规范对开发安全可靠的软件系统至关重要，但正确编写规范仍具挑战性。研究旨在探索LLMs在简化规范编写过程中的作用。

Method: 通过实验评估，使用ChatGPT和DeepSeek两种LLMs，从自然语言或Alloy输入生成完整公式、等效公式及补全公式草稿。

Result: 实验结果表明，LLMs在生成完整公式和补全草稿方面表现良好，并能枚举多个独特解决方案。

Conclusion: LLMs为规范编写提供了令人兴奋的进步，有望在软件开发中发挥关键作用，提升构建稳健软件的能力。

Abstract: Declarative specifications have a vital role to play in developing safe and
dependable software systems. Writing specifications correctly, however, remains
particularly challenging. This paper presents a controlled experiment on using
large language models (LLMs) to write declarative formulas in the well-known
language Alloy. Our use of LLMs is three-fold. One, we employ LLMs to write
complete Alloy formulas from given natural language descriptions (in English).
Two, we employ LLMs to create alternative but equivalent formulas in Alloy with
respect to given Alloy formulas. Three, we employ LLMs to complete sketches of
Alloy formulas and populate the holes in the sketches by synthesizing Alloy
expressions and operators so that the completed formulas accurately represent
the desired properties (that are given in natural language). We conduct the
experimental evaluation using 11 well-studied subject specifications and employ
two popular LLMs, namely ChatGPT and DeepSeek. The experimental results show
that the LLMs generally perform well in synthesizing complete Alloy formulas
from input properties given in natural language or in Alloy, and are able to
enumerate multiple unique solutions. Moreover, the LLMs are also successful at
completing given sketches of Alloy formulas with respect to natural language
descriptions of desired properties (without requiring test cases). We believe
LLMs offer a very exciting advance in our ability to write specifications, and
can help make specifications take a pivotal role in software development and
enhance our ability to build robust software.

</details>
