<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.MS](#cs.MS) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Higher-Order Behavioural Conformances via Fibrations](https://arxiv.org/abs/2507.18509)
*Henning Urbat*

Main category: cs.PL

TL;DR: 本文提出了一种统一的范畴化方法（Howe's method），用于验证高阶语言中行为一致性的程序同余性，适用于定量（如概率）特征的语言。


<details>
  <summary>Details</summary>
Motivation: 随着具有定量特征的语言的兴起，需要扩展共归纳方法以支持更精细的行为一致性（如行为距离），并确保其作为程序同余性的正确性。

Method: 通过抽象高阶规范（AHOS）建模语言，并使用纤维化建模行为一致性，提出了一种通用的范畴化框架。

Result: 在自然条件下，证明了行为一致性的最大双一致性形成同余性，并应用于概率高阶语言的双相似性和行为伪度量。

Conclusion: 该方法为验证行为一致性的同余性提供了统一的框架，适用于多种语言和行为一致性概念。

Abstract: Coinduction is a widely used technique for establishing behavioural
equivalence of programs in higher-order languages. In recent years, the rise of
languages with quantitative (e.g.~probabilistic) features has led to extensions
of coinductive methods to more refined types of behavioural conformances, most
notably notions of behavioural distance. To guarantee soundness of coinductive
reasoning, one needs to show that the behavioural conformance at hand forms a
program congruence, i.e. it is suitably compatible with the operations of the
language. This is usually achieved by a complex proof technique known as
\emph{Howe's method}, which needs to be carefully adapted to both the specific
language and the targeted notion of behavioural conformance. We develop a
uniform categorical approach to Howe's method that features two orthogonal
dimensions of abstraction: (1) the underlying higher-order language is modelled
by an \emph{abstract higher-order specification} (AHOS), a novel and very
general categorical account of operational semantics, and (2) notions of
behavioural conformance (such as relations or metrics) are modelled via
fibrations over the base category of an AHOS. Our main result is a fundamental
congruence theorem at this level of generality: Under natural conditions on the
categorical ingredients and the operational rules of a language modelled by an
AHOS, the greatest behavioural (bi)conformance on its operational model forms a
congruence. We illustrate our theory by deriving congruence of bisimilarity and
behavioural pseudometrics for probabilistic higher-order languages.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [2] [Sandwich: Separating Prefill-Decode Compilation for Efficient CPU LLM Serving](https://arxiv.org/abs/2507.18454)
*Juntao Zhao,Jiuru Li,Chuan Wu*

Main category: cs.AR

TL;DR: Sandwich是一种基于CPU的LLM服务引擎，通过为预填充和解码阶段设计不同的执行计划并分别优化，显著提升了性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于CPU的解决方案忽视了LLM推理中预填充和解码阶段的工作负载差异，采用静态的NUMA节点模型分区和供应商库，效果不佳。

Method: 提出Sandwich引擎，针对预填充和解码阶段分别优化执行计划，并在多种CPU平台上进行评估。

Result: Sandwich在吞吐量、延迟和资源需求方面显著优于现有方案，生成的GEMM内核性能接近静态编译器，但调优成本低得多。

Conclusion: Sandwich为CPU上的LLM服务提供了一种高效、低成本的解决方案，具有广泛的应用潜力。

Abstract: Utilizing CPUs to serve large language models (LLMs) is a resource-friendly
alternative to GPU serving. Existing CPU-based solutions ignore workload
differences between the prefill and the decode phases of LLM inference,
applying a static per-NUMA (Non-Uniform Memory Access) node model partition and
utilizing vendor libraries for operator-level execution, which is suboptimal.
We propose Sandwich, a hardware-centric CPU-based LLM serving engine that uses
different execution plans for the prefill and decode phases and optimizes them
separately.
  We evaluate Sandwich across diverse baselines and datasets on five CPU
platforms, including x86 with AVX-2 and AVX-512, as well as ARM with NEON.
Sandwich achieves an average 2.01x throughput improvement and 90% satisfactory
time-to-first-token (TTFT) and time-per-output-token (TPOT) latencies with up
to 3.40x lower requirements in single sequence serving, and significant
improvement in Goodput in continuous-batching serving. The GEMM kernels
generated by Sandwich outperform representative vendor kernels and other
dynamic shape solutions, achieving performance comparable to static compilers
with three orders of magnitude less kernel tuning costs.

</details>


<div id='cs.MS'></div>

# cs.MS [[Back]](#toc)

### [3] [Building an Accelerated OpenFOAM Proof-of-Concept Application using Modern C++](https://arxiv.org/abs/2507.18268)
*Giulio Malenza,Giovanni Stabile,Filippo Spiga,Robert Birke,Marco Aldinucci*

Main category: cs.MS

TL;DR: 论文探讨了利用现代ISO C++并行结构加速OpenFOAM应用，通过GPU卸载提升性能。


<details>
  <summary>Details</summary>
Motivation: 高性能计算（HPC）中，GPU等加速器的使用日益普及，但如何优化其软件实现以提升性能和易用性仍是一个挑战。

Method: 采用现代ISO C++并行结构，结合NVIDIA HPC SDK编译器运行时栈，实现多核执行和GPU卸载的单代码库。

Result: 通过GPU卸载，成功提升了OpenFOAM中laplacianFoam应用的性能。

Conclusion: 研究表明，利用C++并行结构可以高效实现GPU加速，为OpenFOAM等应用的性能优化提供了可行方案。

Abstract: The modern trend in High-Performance Computing (HPC) involves the use of
accelerators such as Graphics Processing Units (GPUs) alongside Central
Processing Units (CPUs) to speed up numerical operations in various
applications. Leading manufacturers such as NVIDIA, Intel, and AMD are
constantly advancing these architectures, augmenting them with features such as
mixed precision, enhanced memory hierarchies, and specialised accelerator
silicon blocks (e.g., Tensor Cores on GPU or AMX/SME engines on CPU) to enhance
compute performance. At the same time, significant efforts in software
development are aimed at optimizing the use of these innovations, seeking to
improve usability and accessibility. This work contributes to the
state-of-the-art of OpenFOAM development by presenting a working
Proof-Of-Concept application built using modern ISO C++ parallel constructs.
This approach, combined with an appropriate compiler runtime stack, like the
one provided by the NVIDIA HPC SDK, makes it possible to accelerate
well-defined kernels, allowing multi-core execution and GPU offloading using a
single codebase. The study demonstrates that it is possible to increase the
performance of the OpenFOAM laplacianFoam application by offloading the
computations on NVIDIA GPUs using the C++ parallel construct.

</details>
