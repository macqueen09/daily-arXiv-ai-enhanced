<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.MS](#cs.MS) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Higher-Order Behavioural Conformances via Fibrations](https://arxiv.org/abs/2507.18509)
*Henning Urbat*

Main category: cs.PL

TL;DR: 该论文提出了一种统一的范畴化方法（Howe's method），用于证明高阶语言中行为一致性的程序同余性，适用于概率等高阶语言。


<details>
  <summary>Details</summary>
Motivation: 随着具有定量特征（如概率）的语言兴起，需要扩展共归纳方法以支持更精细的行为一致性概念（如行为距离）。传统的Howe方法需要针对特定语言和行为一致性进行复杂调整，因此需要一种更通用的方法。

Method: 提出了一种基于抽象高阶规范（AHOS）的范畴化框架，将语言操作语义和行为一致性（如关系或度量）通过纤维化建模。

Result: 在AHOS框架下，证明了在自然条件下，行为一致性的最大双一致性形成同余性。

Conclusion: 该方法为高阶语言的行为一致性提供了通用证明框架，并通过概率高阶语言的实例验证了其有效性。

Abstract: Coinduction is a widely used technique for establishing behavioural
equivalence of programs in higher-order languages. In recent years, the rise of
languages with quantitative (e.g.~probabilistic) features has led to extensions
of coinductive methods to more refined types of behavioural conformances, most
notably notions of behavioural distance. To guarantee soundness of coinductive
reasoning, one needs to show that the behavioural conformance at hand forms a
program congruence, i.e. it is suitably compatible with the operations of the
language. This is usually achieved by a complex proof technique known as
\emph{Howe's method}, which needs to be carefully adapted to both the specific
language and the targeted notion of behavioural conformance. We develop a
uniform categorical approach to Howe's method that features two orthogonal
dimensions of abstraction: (1) the underlying higher-order language is modelled
by an \emph{abstract higher-order specification} (AHOS), a novel and very
general categorical account of operational semantics, and (2) notions of
behavioural conformance (such as relations or metrics) are modelled via
fibrations over the base category of an AHOS. Our main result is a fundamental
congruence theorem at this level of generality: Under natural conditions on the
categorical ingredients and the operational rules of a language modelled by an
AHOS, the greatest behavioural (bi)conformance on its operational model forms a
congruence. We illustrate our theory by deriving congruence of bisimilarity and
behavioural pseudometrics for probabilistic higher-order languages.

</details>


<div id='cs.MS'></div>

# cs.MS [[Back]](#toc)

### [2] [Building an Accelerated OpenFOAM Proof-of-Concept Application using Modern C++](https://arxiv.org/abs/2507.18268)
*Giulio Malenza,Giovanni Stabile,Filippo Spiga,Robert Birke,Marco Aldinucci*

Main category: cs.MS

TL;DR: 论文探讨了利用现代ISO C++并行结构加速OpenFOAM应用，通过GPU卸载提升性能。


<details>
  <summary>Details</summary>
Motivation: 高性能计算（HPC）中，GPU等加速器与CPU结合使用已成为趋势，但需要优化软件以充分利用硬件创新。

Method: 采用现代ISO C++并行结构，结合NVIDIA HPC SDK编译器运行时栈，实现多核执行和GPU卸载。

Result: 成功提升OpenFOAM的laplacianFoam应用性能，验证了方法的有效性。

Conclusion: 通过C++并行结构和GPU卸载，可以显著提升OpenFOAM应用的性能。

Abstract: The modern trend in High-Performance Computing (HPC) involves the use of
accelerators such as Graphics Processing Units (GPUs) alongside Central
Processing Units (CPUs) to speed up numerical operations in various
applications. Leading manufacturers such as NVIDIA, Intel, and AMD are
constantly advancing these architectures, augmenting them with features such as
mixed precision, enhanced memory hierarchies, and specialised accelerator
silicon blocks (e.g., Tensor Cores on GPU or AMX/SME engines on CPU) to enhance
compute performance. At the same time, significant efforts in software
development are aimed at optimizing the use of these innovations, seeking to
improve usability and accessibility. This work contributes to the
state-of-the-art of OpenFOAM development by presenting a working
Proof-Of-Concept application built using modern ISO C++ parallel constructs.
This approach, combined with an appropriate compiler runtime stack, like the
one provided by the NVIDIA HPC SDK, makes it possible to accelerate
well-defined kernels, allowing multi-core execution and GPU offloading using a
single codebase. The study demonstrates that it is possible to increase the
performance of the OpenFOAM laplacianFoam application by offloading the
computations on NVIDIA GPUs using the C++ parallel construct.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [3] [Sandwich: Separating Prefill-Decode Compilation for Efficient CPU LLM Serving](https://arxiv.org/abs/2507.18454)
*Juntao Zhao,Jiuru Li,Chuan Wu*

Main category: cs.AR

TL;DR: Sandwich是一种基于CPU的LLM服务引擎，通过为预填充和解码阶段设计不同的执行计划，显著提升了性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有CPU解决方案忽略了LLM推理中预填充和解码阶段的工作负载差异，导致性能不佳。

Method: 提出Sandwich引擎，针对预填充和解码阶段分别优化执行计划，并在多种CPU平台上进行评估。

Result: Sandwich在吞吐量、延迟和资源需求方面均有显著提升，生成的GEMM内核性能优于现有解决方案。

Conclusion: Sandwich通过硬件优化和动态执行计划，为CPU上的LLM服务提供了高效且低成本的解决方案。

Abstract: Utilizing CPUs to serve large language models (LLMs) is a resource-friendly
alternative to GPU serving. Existing CPU-based solutions ignore workload
differences between the prefill and the decode phases of LLM inference,
applying a static per-NUMA (Non-Uniform Memory Access) node model partition and
utilizing vendor libraries for operator-level execution, which is suboptimal.
We propose Sandwich, a hardware-centric CPU-based LLM serving engine that uses
different execution plans for the prefill and decode phases and optimizes them
separately.
  We evaluate Sandwich across diverse baselines and datasets on five CPU
platforms, including x86 with AVX-2 and AVX-512, as well as ARM with NEON.
Sandwich achieves an average 2.01x throughput improvement and 90% satisfactory
time-to-first-token (TTFT) and time-per-output-token (TPOT) latencies with up
to 3.40x lower requirements in single sequence serving, and significant
improvement in Goodput in continuous-batching serving. The GEMM kernels
generated by Sandwich outperform representative vendor kernels and other
dynamic shape solutions, achieving performance comparable to static compilers
with three orders of magnitude less kernel tuning costs.

</details>
