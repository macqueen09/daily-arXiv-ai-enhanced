<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.MS](#cs.MS) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Higher-Order Behavioural Conformances via Fibrations](https://arxiv.org/abs/2507.18509)
*Henning Urbat*

Main category: cs.PL

TL;DR: 论文提出了一种统一的范畴化方法，扩展了Howe方法，用于证明高阶语言中行为一致性的程序同余性。


<details>
  <summary>Details</summary>
Motivation: 随着具有定量特征（如概率）的语言兴起，需要扩展共归纳方法以支持更精细的行为一致性概念，如行为距离。

Method: 采用抽象的范畴化框架（AHOS）和纤维化模型，统一处理行为一致性，并证明其同余性。

Result: 在AHOS框架下，证明了最大行为（双）一致性形成同余性，适用于概率高阶语言。

Conclusion: 该理论为行为一致性的同余性证明提供了通用方法，并成功应用于概率高阶语言的案例分析。

Abstract: Coinduction is a widely used technique for establishing behavioural
equivalence of programs in higher-order languages. In recent years, the rise of
languages with quantitative (e.g.~probabilistic) features has led to extensions
of coinductive methods to more refined types of behavioural conformances, most
notably notions of behavioural distance. To guarantee soundness of coinductive
reasoning, one needs to show that the behavioural conformance at hand forms a
program congruence, i.e. it is suitably compatible with the operations of the
language. This is usually achieved by a complex proof technique known as
\emph{Howe's method}, which needs to be carefully adapted to both the specific
language and the targeted notion of behavioural conformance. We develop a
uniform categorical approach to Howe's method that features two orthogonal
dimensions of abstraction: (1) the underlying higher-order language is modelled
by an \emph{abstract higher-order specification} (AHOS), a novel and very
general categorical account of operational semantics, and (2) notions of
behavioural conformance (such as relations or metrics) are modelled via
fibrations over the base category of an AHOS. Our main result is a fundamental
congruence theorem at this level of generality: Under natural conditions on the
categorical ingredients and the operational rules of a language modelled by an
AHOS, the greatest behavioural (bi)conformance on its operational model forms a
congruence. We illustrate our theory by deriving congruence of bisimilarity and
behavioural pseudometrics for probabilistic higher-order languages.

</details>


<div id='cs.MS'></div>

# cs.MS [[Back]](#toc)

### [2] [Building an Accelerated OpenFOAM Proof-of-Concept Application using Modern C++](https://arxiv.org/abs/2507.18268)
*Giulio Malenza,Giovanni Stabile,Filippo Spiga,Robert Birke,Marco Aldinucci*

Main category: cs.MS

TL;DR: 论文探讨了利用现代ISO C++并行结构加速OpenFOAM应用的方法，通过GPU卸载提升性能。


<details>
  <summary>Details</summary>
Motivation: 现代高性能计算趋势中，加速器（如GPU）与CPU结合使用以提升计算性能，但需要优化软件以充分利用硬件创新。

Method: 采用现代ISO C++并行结构，结合NVIDIA HPC SDK编译器运行时栈，实现多核执行和GPU卸载的单代码库。

Result: 通过GPU卸载，成功提升了OpenFOAM中laplacianFoam应用的性能。

Conclusion: 研究表明，利用C++并行结构可以高效实现GPU加速，为OpenFOAM开发提供了新思路。

Abstract: The modern trend in High-Performance Computing (HPC) involves the use of
accelerators such as Graphics Processing Units (GPUs) alongside Central
Processing Units (CPUs) to speed up numerical operations in various
applications. Leading manufacturers such as NVIDIA, Intel, and AMD are
constantly advancing these architectures, augmenting them with features such as
mixed precision, enhanced memory hierarchies, and specialised accelerator
silicon blocks (e.g., Tensor Cores on GPU or AMX/SME engines on CPU) to enhance
compute performance. At the same time, significant efforts in software
development are aimed at optimizing the use of these innovations, seeking to
improve usability and accessibility. This work contributes to the
state-of-the-art of OpenFOAM development by presenting a working
Proof-Of-Concept application built using modern ISO C++ parallel constructs.
This approach, combined with an appropriate compiler runtime stack, like the
one provided by the NVIDIA HPC SDK, makes it possible to accelerate
well-defined kernels, allowing multi-core execution and GPU offloading using a
single codebase. The study demonstrates that it is possible to increase the
performance of the OpenFOAM laplacianFoam application by offloading the
computations on NVIDIA GPUs using the C++ parallel construct.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [3] [Sandwich: Separating Prefill-Decode Compilation for Efficient CPU LLM Serving](https://arxiv.org/abs/2507.18454)
*Juntao Zhao,Jiuru Li,Chuan Wu*

Main category: cs.AR

TL;DR: Sandwich是一种基于CPU的LLM服务引擎，通过为预填充和解码阶段设计不同的执行计划，显著提升了性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于CPU的解决方案忽略了LLM推理中预填充和解码阶段的工作负载差异，导致性能不佳。

Method: 提出Sandwich引擎，针对预填充和解码阶段分别优化执行计划，并在多种CPU平台上进行评估。

Result: Sandwich在吞吐量、延迟和资源需求方面显著优于现有解决方案，生成的GEMM内核性能接近静态编译器。

Conclusion: Sandwich为CPU上的LLM服务提供了一种高效且资源友好的解决方案。

Abstract: Utilizing CPUs to serve large language models (LLMs) is a resource-friendly
alternative to GPU serving. Existing CPU-based solutions ignore workload
differences between the prefill and the decode phases of LLM inference,
applying a static per-NUMA (Non-Uniform Memory Access) node model partition and
utilizing vendor libraries for operator-level execution, which is suboptimal.
We propose Sandwich, a hardware-centric CPU-based LLM serving engine that uses
different execution plans for the prefill and decode phases and optimizes them
separately.
  We evaluate Sandwich across diverse baselines and datasets on five CPU
platforms, including x86 with AVX-2 and AVX-512, as well as ARM with NEON.
Sandwich achieves an average 2.01x throughput improvement and 90% satisfactory
time-to-first-token (TTFT) and time-per-output-token (TPOT) latencies with up
to 3.40x lower requirements in single sequence serving, and significant
improvement in Goodput in continuous-batching serving. The GEMM kernels
generated by Sandwich outperform representative vendor kernels and other
dynamic shape solutions, achieving performance comparable to static compilers
with three orders of magnitude less kernel tuning costs.

</details>
