{"id": "2507.18792", "pdf": "https://arxiv.org/pdf/2507.18792", "abs": "https://arxiv.org/abs/2507.18792", "authors": ["Zixu Zhou"], "title": "Decompiling Rust: An Empirical Study of Compiler Optimizations and Reverse Engineering Challenges", "categories": ["cs.PL", "cs.SE"], "comment": null, "summary": "Decompiling Rust binaries is challenging due to the language's rich type\nsystem, aggressive compiler optimizations, and widespread use of high-level\nabstractions. In this work, we conduct a benchmark-driven evaluation of\ndecompilation quality across core Rust features and compiler build modes. Our\nautomated scoring framework shows that generic types, trait methods, and error\nhandling constructs significantly reduce decompilation quality, especially in\nrelease builds. Through representative case studies, we analyze how specific\nlanguage constructs affect control flow, variable naming, and type information\nrecovery. Our findings provide actionable insights for tool developers and\nhighlight the need for Rust-aware decompilation strategies."}
{"id": "2507.18885", "pdf": "https://arxiv.org/pdf/2507.18885", "abs": "https://arxiv.org/abs/2507.18885", "authors": ["Qiyuan Xu", "Renxi Wang", "Haonan Li", "David Sanan", "Conrad Watt"], "title": "IsaMini: Redesigned Isabelle Proof Lanugage for Machine Learning", "categories": ["cs.PL"], "comment": null, "summary": "Neural Theorem Proving (NTP) employs deep learning methods, particularly\nLarge Language Models (LLMs), to automate formal proofs in proof assistants.\nThis approach holds promise for reducing the dramatic labor costs or\ncomputation costs required in proof engineering, which is fundamental to formal\nverification and other software engineering methods. The paper explores the\npotential of improving NTP by redesigning the proof language, given that LLMs'\ncapabilities depend highly on representations. We introduce \\emph{MiniLang}, a\nredesigned proof language for Isabelle/HOL incorporating an improved version of\nSledgehammer. Experiments show MiniLang benefits two fine-tuned LLMs by\nimproving the success rate on the PISA benchmark by up to 29\\% in comparison to\ngeneration of Isar proof script. The success rate under one attempt (so-called\n\\emph{pass@1}) reaches 69.1\\%, exceeding the previous Baldur's pass@64\n(65.7\\%); The pass@8 reaches 79.2\\%, exceeding the state-of-the-art on PISA\n(71.0\\%) achieved by Magnushammer."}
{"id": "2507.19015", "pdf": "https://arxiv.org/pdf/2507.19015", "abs": "https://arxiv.org/abs/2507.19015", "authors": ["Samuel Xifaras", "Panagiotis Manolios", "Andrew T. Walter", "William Robertson"], "title": "An Enumerative Embedding of the Python Type System in ACL2s", "categories": ["cs.PL", "cs.LO", "cs.SE"], "comment": "In Proceedings ACL2 2025, arXiv:2507.18567", "summary": "Python is a high-level interpreted language that has become an industry\nstandard in a wide variety of applications. In this paper, we take a first step\ntowards using ACL2s to reason about Python code by developing an embedding of a\nsubset of the Python type system in ACL2s. The subset of Python types we\nsupport includes many of the most commonly used type annotations as well as\nuser-defined types comprised of supported types. We provide ACL2s definitions\nof these types, as well as defdata enumerators that are customized to provide\ncode coverage and identify errors in Python programs. Using the ACL2s\nembedding, we can generate instances of types that can then be used as inputs\nto fuzz Python programs, which allows us to identify bugs in Python code that\nare not detected by state-of-the-art Python type checkers. We evaluate our work\nagainst four open-source repositories, extracting their type information and\ngenerating inputs for fuzzing functions with type signatures that are in the\nsupported subset of Python types. Note that we only use the type signatures of\nfunctions to generate inputs and treat the bodies of functions as black boxes.\nWe measure code coverage, which ranges from about 68% to more than 80%, and\nidentify code patterns that hinder coverage such as complex branch conditions\nand external file system dependencies. We conclude with a discussion of the\nresults and recommendations for future work."}
{"id": "2507.19176", "pdf": "https://arxiv.org/pdf/2507.19176", "abs": "https://arxiv.org/abs/2507.19176", "authors": ["Weijun Chen", "Yuxi Fu", "Huan Long"], "title": "A Programming Language for Feasible Solutions", "categories": ["cs.PL"], "comment": null, "summary": "Runtime efficiency and termination are crucial properties in the studies of\nprogram verification. Instead of dealing with these issues in an ad hoc manner,\nit would be useful to develop a robust framework in which such properties are\nguaranteed by design. This paper introduces a new imperative programming\nlanguage whose design is grounded in a static type system that ensures the\nfollowing equivalence property: All definable programs are guaranteed to run in\npolynomial time; Conversely, all problems solvable in polynomial time can be\nsolved by some programs of the language. The contribution of this work is\ntwofold. On the theoretical side, the foundational equivalence property is\nestablished, and the proof of the equivalence theorem is non-trivial. On the\npractical side, a programming approach is proposed that can streamline program\nanalysis and verification for feasible computations. An interpreter for the\nlanguage has been implemented, demonstrating the feasibility of the approach in\npractice."}
{"id": "2507.18755", "pdf": "https://arxiv.org/pdf/2507.18755", "abs": "https://arxiv.org/abs/2507.18755", "authors": ["Chandra Maddila", "Adam Tait", "Claire Chang", "Daniel Cheng", "Nauman Ahmad", "Vijayaraghavan Murali", "Marshall Roch", "Arnaud Avondet", "Aaron Meltzer", "Victor Montalvao", "Michael Hopko", "Chris Waterson", "Parth Thakkar", "Renuka Fernandez", "Kristian Kristensen", "Sivan Barzily", "Sherry Chen", "Rui Abreu", "Nachiappan Nagappan", "Payam Shodjai", "Killian Murphy", "James Everingham", "Aparna Ramani", "Peter C. Rigby"], "title": "Agentic Program Repair from Test Failures at Scale: A Neuro-symbolic approach with static analysis and test execution feedback", "categories": ["cs.SE", "cs.AI", "cs.PL"], "comment": null, "summary": "Aim: With the advent of LLMs, sophisticated agentic program repair has become\nviable at large organizations with large codebases. In this work, we develop an\nEngineering Agent that fixes the source code based on test failures at scale\nacross diverse software offerings internally.\n  Method: Using Llama as the base, we employ the ReAct harness to develop an\nagent. We start with a test failure that was triaged by a rule-based test\nfailure bot. We then set up an agentic harness and allow the agent to reason\nand run a set of 15 actions from reading a file to generating a patch. We\nprovide feedback to the agent through static analysis and test failures so it\ncan refine its solution. We leverage an LLM-as-a-Judge to ensure that the patch\nconforms to the standards followed by a human review to land fixes.\n  Benchmark Findings: We curated offline benchmarks for our patch generator,\nthe Engineering Agent loop, and the LLM-as-a-Judge. In offline evaluations we\nfound that a specialized 70B model is highly competitive with the much larger\nbut vanilla Llama-405B. In an ablation study, we found that the ReAct harness\n(neural model) benefited from the symbolic information from static analysis\ntools and test execution traces. A model that strikes a balance between the\nsolve rate and error rate vs the cost and latency has a benchmark solve rate of\n42.3% using an average 11.8 feedback iterations.\n  Production Findings: In a three month period, 80% of the generated fixes were\nreviewed, of which 31.5% were landed (25.5% of the total number of generated\nfixes).\n  Feedback from Engineers: We used open coding to extract qualitative themes\nfrom engineers' feedback. We saw positive feedback in the form of quick\napprovals, gratitude, and surprise. We also found mixed feedback when the\nEngineering Agent's solution was partially correct and it served as a good\nstarting point."}
{"id": "2507.19012", "pdf": "https://arxiv.org/pdf/2507.19012", "abs": "https://arxiv.org/abs/2507.19012", "authors": ["Alessandro Coglio", "Eric McCarthy"], "title": "A Formalization of the Yul Language and Some Verified Yul Code Transformations", "categories": ["cs.LO", "cs.PL"], "comment": "In Proceedings ACL2 2025, arXiv:2507.18567", "summary": "Yul is an intermediate language used in the compilation of the Solidity\nprogramming language for Ethereum smart contracts. The compiler applies\ncustomizable sequences of transformations to Yul code. To help ensure the\ncorrectness of these transformations and their sequencing, we used the ACL2\ntheorem prover to develop a formalization of the syntax and semantics of Yul,\nproofs relating static and dynamic semantics, a formalization of some Yul code\ntransformations, and correctness proofs for these transformations."}
{"id": "2507.19271", "pdf": "https://arxiv.org/pdf/2507.19271", "abs": "https://arxiv.org/abs/2507.19271", "authors": ["Igli Begolli", "Meltem Aksoy", "Daniel Neider"], "title": "Fine-Tuning Multilingual Language Models for Code Review: An Empirical Study on Industrial C# Projects", "categories": ["cs.SE", "cs.AI", "cs.PL"], "comment": null, "summary": "Code review is essential for maintaining software quality but often\ntime-consuming and cognitively demanding, especially in industrial\nenvironments. Recent advancements in language models (LMs) have opened new\navenues for automating core review tasks. This study presents the empirical\nevaluation of monolingual fine-tuning on the performance of open-source LMs\nacross three key automated code review tasks: Code Change Quality Estimation,\nReview Comment Generation, and Code Refinement. We fine-tuned three distinct\nmodels, CodeReviewer, CodeLlama-7B, and DeepSeek-R1-Distill, on a C\\# specific\ndataset combining public benchmarks with industrial repositories. Our study\ninvestigates how different configurations of programming languages and natural\nlanguages in the training data affect LM performance, particularly in comment\ngeneration. Additionally, we benchmark the fine-tuned models against an\nautomated software analysis tool (ASAT) and human reviewers to evaluate their\npractical utility in real-world settings. Our results show that monolingual\nfine-tuning improves model accuracy and relevance compared to multilingual\nbaselines. While LMs can effectively support code review workflows, especially\nfor routine or repetitive tasks, human reviewers remain superior in handling\nsemantically complex or context-sensitive changes. Our findings highlight the\nimportance of language alignment and task-specific adaptation in optimizing LMs\nfor automated code review."}
