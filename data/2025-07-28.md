<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 4]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.SE](#cs.SE) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Decompiling Rust: An Empirical Study of Compiler Optimizations and Reverse Engineering Challenges](https://arxiv.org/abs/2507.18792)
*Zixu Zhou*

Main category: cs.PL

TL;DR: 论文评估了Rust二进制文件反编译的挑战，发现泛型、特质方法和错误处理结构显著降低反编译质量，尤其是在发布版本中。


<details>
  <summary>Details</summary>
Motivation: 由于Rust丰富的类型系统、编译器优化和高层抽象，反编译其二进制文件具有挑战性。

Method: 通过基准测试驱动的评估框架，分析核心Rust功能和编译器构建模式对反编译质量的影响。

Result: 泛型、特质方法和错误处理结构显著降低反编译质量，尤其是在发布版本中。

Conclusion: 研究为工具开发者提供了实用见解，并强调需要Rust感知的反编译策略。

Abstract: Decompiling Rust binaries is challenging due to the language's rich type
system, aggressive compiler optimizations, and widespread use of high-level
abstractions. In this work, we conduct a benchmark-driven evaluation of
decompilation quality across core Rust features and compiler build modes. Our
automated scoring framework shows that generic types, trait methods, and error
handling constructs significantly reduce decompilation quality, especially in
release builds. Through representative case studies, we analyze how specific
language constructs affect control flow, variable naming, and type information
recovery. Our findings provide actionable insights for tool developers and
highlight the need for Rust-aware decompilation strategies.

</details>


### [2] [IsaMini: Redesigned Isabelle Proof Lanugage for Machine Learning](https://arxiv.org/abs/2507.18885)
*Qiyuan Xu,Renxi Wang,Haonan Li,David Sanan,Conrad Watt*

Main category: cs.PL

TL;DR: 论文提出了一种名为MiniLang的改进证明语言，用于提升神经定理证明（NTP）的性能，实验显示其在PISA基准测试中显著提高了成功率。


<details>
  <summary>Details</summary>
Motivation: 通过改进证明语言的表示形式，利用大型语言模型（LLMs）的能力，降低形式化验证中的劳动力和计算成本。

Method: 设计了MiniLang，一种针对Isabelle/HOL的改进证明语言，并结合了增强版的Sledgehammer工具，通过实验验证其效果。

Result: MiniLang将PISA基准测试的成功率提高了29%，pass@1达到69.1%，pass@8达到79.2%，均超过现有技术。

Conclusion: MiniLang通过优化证明语言表示，显著提升了NTP的性能，为形式化验证提供了更高效的解决方案。

Abstract: Neural Theorem Proving (NTP) employs deep learning methods, particularly
Large Language Models (LLMs), to automate formal proofs in proof assistants.
This approach holds promise for reducing the dramatic labor costs or
computation costs required in proof engineering, which is fundamental to formal
verification and other software engineering methods. The paper explores the
potential of improving NTP by redesigning the proof language, given that LLMs'
capabilities depend highly on representations. We introduce \emph{MiniLang}, a
redesigned proof language for Isabelle/HOL incorporating an improved version of
Sledgehammer. Experiments show MiniLang benefits two fine-tuned LLMs by
improving the success rate on the PISA benchmark by up to 29\% in comparison to
generation of Isar proof script. The success rate under one attempt (so-called
\emph{pass@1}) reaches 69.1\%, exceeding the previous Baldur's pass@64
(65.7\%); The pass@8 reaches 79.2\%, exceeding the state-of-the-art on PISA
(71.0\%) achieved by Magnushammer.

</details>


### [3] [An Enumerative Embedding of the Python Type System in ACL2s](https://arxiv.org/abs/2507.19015)
*Samuel Xifaras,Panagiotis Manolios,Andrew T. Walter,William Robertson*

Main category: cs.PL

TL;DR: 本文提出了一种在ACL2s中嵌入Python类型系统子集的方法，用于生成输入以模糊测试Python程序，并识别未被现有类型检查器发现的错误。


<details>
  <summary>Details</summary>
Motivation: Python作为一种广泛使用的高级语言，其类型系统的复杂性需要更强大的工具进行验证。本文旨在利用ACL2s对Python代码进行形式化验证。

Method: 在ACL2s中嵌入Python类型系统的子集，包括常用类型注解和用户定义类型，并生成输入以模糊测试Python程序。

Result: 在四个开源仓库中测试，代码覆盖率介于68%至80%之间，并识别了影响覆盖率的代码模式。

Conclusion: 该方法有效识别了Python代码中的错误，并提出了未来改进方向。

Abstract: Python is a high-level interpreted language that has become an industry
standard in a wide variety of applications. In this paper, we take a first step
towards using ACL2s to reason about Python code by developing an embedding of a
subset of the Python type system in ACL2s. The subset of Python types we
support includes many of the most commonly used type annotations as well as
user-defined types comprised of supported types. We provide ACL2s definitions
of these types, as well as defdata enumerators that are customized to provide
code coverage and identify errors in Python programs. Using the ACL2s
embedding, we can generate instances of types that can then be used as inputs
to fuzz Python programs, which allows us to identify bugs in Python code that
are not detected by state-of-the-art Python type checkers. We evaluate our work
against four open-source repositories, extracting their type information and
generating inputs for fuzzing functions with type signatures that are in the
supported subset of Python types. Note that we only use the type signatures of
functions to generate inputs and treat the bodies of functions as black boxes.
We measure code coverage, which ranges from about 68% to more than 80%, and
identify code patterns that hinder coverage such as complex branch conditions
and external file system dependencies. We conclude with a discussion of the
results and recommendations for future work.

</details>


### [4] [A Programming Language for Feasible Solutions](https://arxiv.org/abs/2507.19176)
*Weijun Chen,Yuxi Fu,Huan Long*

Main category: cs.PL

TL;DR: 本文提出了一种新的命令式编程语言，其静态类型系统确保所有可定义程序均在多项式时间内运行，且所有多项式时间可解问题均可由该语言解决。


<details>
  <summary>Details</summary>
Motivation: 为解决程序验证中的运行时效率和终止性问题，设计一个保证这些性质的稳健框架。

Method: 基于静态类型系统设计新语言，确保程序运行时间多项式化，并证明其与多项式时间问题的等价性。

Result: 理论证明了等价性定理，实践上实现了语言解释器，验证了方法的可行性。

Conclusion: 该语言为程序分析和验证提供了高效且可靠的方法。

Abstract: Runtime efficiency and termination are crucial properties in the studies of
program verification. Instead of dealing with these issues in an ad hoc manner,
it would be useful to develop a robust framework in which such properties are
guaranteed by design. This paper introduces a new imperative programming
language whose design is grounded in a static type system that ensures the
following equivalence property: All definable programs are guaranteed to run in
polynomial time; Conversely, all problems solvable in polynomial time can be
solved by some programs of the language. The contribution of this work is
twofold. On the theoretical side, the foundational equivalence property is
established, and the proof of the equivalence theorem is non-trivial. On the
practical side, a programming approach is proposed that can streamline program
analysis and verification for feasible computations. An interpreter for the
language has been implemented, demonstrating the feasibility of the approach in
practice.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [5] [A Formalization of the Yul Language and Some Verified Yul Code Transformations](https://arxiv.org/abs/2507.19012)
*Alessandro Coglio,Eric McCarthy*

Main category: cs.LO

TL;DR: Yul是Solidity编程语言用于以太坊智能合约的中间语言，通过ACL2定理证明器验证其语法、语义及代码转换的正确性。


<details>
  <summary>Details</summary>
Motivation: 确保Yul代码转换及其序列的正确性。

Method: 使用ACL2定理证明器形式化Yul的语法和语义，验证静态与动态语义关系，并证明代码转换的正确性。

Result: 成功形式化Yul的语法、语义及部分代码转换，并验证其正确性。

Conclusion: 通过形式化方法验证了Yul代码转换的正确性，为智能合约编译提供了可靠性保障。

Abstract: Yul is an intermediate language used in the compilation of the Solidity
programming language for Ethereum smart contracts. The compiler applies
customizable sequences of transformations to Yul code. To help ensure the
correctness of these transformations and their sequencing, we used the ACL2
theorem prover to develop a formalization of the syntax and semantics of Yul,
proofs relating static and dynamic semantics, a formalization of some Yul code
transformations, and correctness proofs for these transformations.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [6] [Agentic Program Repair from Test Failures at Scale: A Neuro-symbolic approach with static analysis and test execution feedback](https://arxiv.org/abs/2507.18755)
*Chandra Maddila,Adam Tait,Claire Chang,Daniel Cheng,Nauman Ahmad,Vijayaraghavan Murali,Marshall Roch,Arnaud Avondet,Aaron Meltzer,Victor Montalvao,Michael Hopko,Chris Waterson,Parth Thakkar,Renuka Fernandez,Kristian Kristensen,Sivan Barzily,Sherry Chen,Rui Abreu,Nachiappan Nagappan,Payam Shodjai,Killian Murphy,James Everingham,Aparna Ramani,Peter C. Rigby*

Main category: cs.SE

TL;DR: 开发了一个基于LLM的工程代理，用于大规模修复代码中的测试失败问题，结合ReAct框架和静态分析工具，取得了42.3%的解决率，并在生产中落地了25.5%的修复。


<details>
  <summary>Details</summary>
Motivation: 随着LLM的发展，大规模代码修复成为可能，目标是开发一个能够高效修复测试失败的工程代理。

Method: 以Llama为基础，使用ReAct框架开发代理，结合静态分析和测试反馈进行迭代修复，并利用LLM-as-a-Judge确保修复质量。

Result: 离线评估中，70B模型表现接近405B模型，解决率为42.3%；生产中，25.5%的修复被落地。

Conclusion: 工程代理在代码修复中表现良好，但仍有改进空间，尤其是在部分修复的准确性上。

Abstract: Aim: With the advent of LLMs, sophisticated agentic program repair has become
viable at large organizations with large codebases. In this work, we develop an
Engineering Agent that fixes the source code based on test failures at scale
across diverse software offerings internally.
  Method: Using Llama as the base, we employ the ReAct harness to develop an
agent. We start with a test failure that was triaged by a rule-based test
failure bot. We then set up an agentic harness and allow the agent to reason
and run a set of 15 actions from reading a file to generating a patch. We
provide feedback to the agent through static analysis and test failures so it
can refine its solution. We leverage an LLM-as-a-Judge to ensure that the patch
conforms to the standards followed by a human review to land fixes.
  Benchmark Findings: We curated offline benchmarks for our patch generator,
the Engineering Agent loop, and the LLM-as-a-Judge. In offline evaluations we
found that a specialized 70B model is highly competitive with the much larger
but vanilla Llama-405B. In an ablation study, we found that the ReAct harness
(neural model) benefited from the symbolic information from static analysis
tools and test execution traces. A model that strikes a balance between the
solve rate and error rate vs the cost and latency has a benchmark solve rate of
42.3% using an average 11.8 feedback iterations.
  Production Findings: In a three month period, 80% of the generated fixes were
reviewed, of which 31.5% were landed (25.5% of the total number of generated
fixes).
  Feedback from Engineers: We used open coding to extract qualitative themes
from engineers' feedback. We saw positive feedback in the form of quick
approvals, gratitude, and surprise. We also found mixed feedback when the
Engineering Agent's solution was partially correct and it served as a good
starting point.

</details>


### [7] [Fine-Tuning Multilingual Language Models for Code Review: An Empirical Study on Industrial C# Projects](https://arxiv.org/abs/2507.19271)
*Igli Begolli,Meltem Aksoy,Daniel Neider*

Main category: cs.SE

TL;DR: 论文研究了单语言微调对开源语言模型在代码审查任务中的性能影响，发现其优于多语言基线，但人类审查员在复杂任务中仍更优。


<details>
  <summary>Details</summary>
Motivation: 代码审查对软件质量至关重要，但耗时且认知负担重，语言模型为自动化审查任务提供了新途径。

Method: 对CodeReviewer、CodeLlama-7B和DeepSeek-R1-Distill进行单语言微调，评估其在代码变更质量估计、审查评论生成和代码优化任务中的表现。

Result: 单语言微调提高了模型的准确性和相关性，但人类审查员在语义复杂或上下文敏感的任务中表现更佳。

Conclusion: 语言对齐和任务特定适配对优化语言模型在自动化代码审查中的性能至关重要。

Abstract: Code review is essential for maintaining software quality but often
time-consuming and cognitively demanding, especially in industrial
environments. Recent advancements in language models (LMs) have opened new
avenues for automating core review tasks. This study presents the empirical
evaluation of monolingual fine-tuning on the performance of open-source LMs
across three key automated code review tasks: Code Change Quality Estimation,
Review Comment Generation, and Code Refinement. We fine-tuned three distinct
models, CodeReviewer, CodeLlama-7B, and DeepSeek-R1-Distill, on a C\# specific
dataset combining public benchmarks with industrial repositories. Our study
investigates how different configurations of programming languages and natural
languages in the training data affect LM performance, particularly in comment
generation. Additionally, we benchmark the fine-tuned models against an
automated software analysis tool (ASAT) and human reviewers to evaluate their
practical utility in real-world settings. Our results show that monolingual
fine-tuning improves model accuracy and relevance compared to multilingual
baselines. While LMs can effectively support code review workflows, especially
for routine or repetitive tasks, human reviewers remain superior in handling
semantically complex or context-sensitive changes. Our findings highlight the
importance of language alignment and task-specific adaptation in optimizing LMs
for automated code review.

</details>
