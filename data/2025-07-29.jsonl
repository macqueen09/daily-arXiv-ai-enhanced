{"id": "2507.19728", "pdf": "https://arxiv.org/pdf/2507.19728", "abs": "https://arxiv.org/abs/2507.19728", "authors": ["Lalita Na Nongkhai", "Jingyun Wang", "Takahiko Mendori"], "title": "Development and Evaluation of Adaptive LearningSupport System Based on Ontology of MultipleProgramming Languages", "categories": ["cs.PL"], "comment": "This document provides corrections to the published article.\n  Corrections include clarifying figure legends and addressing grammatical\n  issues to enhance clarity. The authors state that the scientific conclusions\n  are unaffected", "summary": "This paper introduces an ontology-based approach within an adaptive learning\nsupport system for computer programming. This system (named ADVENTURE) is\ndesigned to deliver personalized programming exercises that are tailored to\nindividual learners' skill levels. ADVENTURE utilizes an ontology, named\nCONTINUOUS, which encompasses common concepts across multiple programming\nlanguages. The system leverages this ontology not only to visualize programming\nconcepts but also to provide hints during practice programming exercises and\nrecommend subsequent programming concepts. The adaptive mechanism is driven by\nthe Elo Rating System, applied in an educational context to dynamically\nestimate the most appropriate exercise difficulty for each learner. An\nexperimental study compared two instructional modes, adaptive and random, based\non six features derived from 1,186 code submissions across all the experimental\ngroups. The results indicate significant differences in four of six analyzed\nfeatures between these two modes. Notably, the adaptive mode demonstrates a\nsignificant difference over the random mode in two features, the submission of\ncorrect answers and the number of pass concepts. Therefore, these results\nunderscore that this adaptive learning support system may support learners in\npracticing programming exercises."}
{"id": "2507.20251", "pdf": "https://arxiv.org/pdf/2507.20251", "abs": "https://arxiv.org/abs/2507.20251", "authors": ["Angelos Charalambidis", "Babis Kostopoulos", "Christos Nomikos", "Panos Rondogiannis"], "title": "The Power of Negation in Higher-Order Datalog", "categories": ["cs.PL", "cs.CC", "cs.DB", "cs.LO"], "comment": null, "summary": "We investigate the expressive power of Higher-Order Datalog$^\\neg$ under both\nthe well-founded and the stable model semantics, establishing tight connections\nwith complexity classes. We prove that under the well-founded semantics, for\nall $k\\geq 1$, $(k+1)$-Order Datalog$^\\neg$ captures k-EXP, a result that holds\nwithout explicit ordering of the input database. The proof of this fact can be\nperformed either by using the powerful existential predicate variables of the\nlanguage or by using partially applied relations and relation enumeration.\nFurthermore, we demonstrate that this expressive power is retained within a\nstratified fragment of the language. Under the stable model semantics, we show\nthat $(k+1)$-Order Datalog$^\\neg$ captures co-(k-NEXP) using cautious reasoning\nand k-NEXP using brave reasoning, again with analogous results for the\nstratified fragment augmented with choice rules. Our results establish a\nhierarchy of expressive power, highlighting an interesting trade-off between\norder and non-determinism in the context of higher-order logic programming:\nincreasing the order of programs under the well-founded semantics can surpass\nthe expressive power of lower-order programs under the stable model semantics."}
{"id": "2507.20672", "pdf": "https://arxiv.org/pdf/2507.20672", "abs": "https://arxiv.org/abs/2507.20672", "authors": ["Yannis Smaragdakis", "Neville Grech", "Sifis Lagouvardos", "Konstantinos Triantafyllou", "Ilias Tsatiris", "Yannis Bollanos", "Tony Rocco Valentine"], "title": "Program Analysis for High-Value Smart Contract Vulnerabilities: Techniques and Insights", "categories": ["cs.CR", "cs.PL"], "comment": null, "summary": "A widespread belief in the blockchain security community is that automated\ntechniques are only good for detecting shallow bugs, typically of small value.\nIn this paper, we present the techniques and insights that have led us to\nrepeatable success in automatically discovering high-value smart contract\nvulnerabilities. Our vulnerability disclosures have yielded 10 bug bounties,\nfor a total of over $3M, over high-profile deployed code, as well as hundreds\nof bugs detected in pre-deployment or under-audit code.\n  We argue that the elements of this surprising success are a) a very\nhigh-completeness static analysis approach that manages to maintain acceptable\nprecision; b) domain knowledge, provided by experts or captured via statistical\ninference. We present novel techniques for automatically inferring domain\nknowledge from statistical analysis of a large corpus of deployed contracts, as\nwell as discuss insights on the ideal precision and warning rate of a promising\nvulnerability detector. In contrast to academic literature in program analysis,\nwhich routinely expects false-positive rates below 50% for publishable results,\nwe posit that a useful analysis for high-value real-world vulnerabilities will\nlikely flag very few programs (under 1%) and will do so with a high\nfalse-positive rate (e.g., 95%, meaning that only one-of-twenty human\ninspections will yield an exploitable vulnerability)."}
{"id": "2507.20674", "pdf": "https://arxiv.org/pdf/2507.20674", "abs": "https://arxiv.org/abs/2507.20674", "authors": ["Nima Karimipour", "Michael Pradel", "Martin Kellogg", "Manu Sridharan"], "title": "LLM-Based Repair of Static Nullability Errors", "categories": ["cs.SE", "cs.PL"], "comment": null, "summary": "Modern Java projects increasingly adopt static analysis tools that prevent\nnull-pointer exceptions by treating nullness as a type property. However,\nintegrating such tools into large, existing codebases remains a significant\nchallenge. While annotation inference can eliminate many errors automatically,\na subset of residual errors -- typically a mix of real bugs and false positives\n-- often persist and can only be resolved via code changes. Manually addressing\nthese errors is tedious and error-prone. Large language models (LLMs) offer a\npromising path toward automating these repairs, but naively-prompted LLMs often\ngenerate incorrect, contextually-inappropriate edits. Resolving a nullability\nerror demands a deep understanding of how a symbol is used across the codebase,\noften spanning methods, classes, and packages. We present NullRepair, a system\nthat integrates LLMs into a structured workflow for resolving the errors from a\nnullability checker. NullRepair's decision process follows a flowchart derived\nfrom manual analysis of 200 real-world errors. It leverages static analysis to\nidentify safe and unsafe usage regions of symbols, using error-free usage\nexamples to contextualize model prompts. Patches are generated through an\niterative interaction with the LLM that incorporates project-wide context and\ndecision logic. Our evaluation on 12 real-world Java projects shows that\nNullRepair resolves an average of 72% of the errors that remain after applying\na state-of-the-art annotation inference technique. Unlike a naively-prompted\nLLM, NullRepair also largely preserves program semantics, with all unit tests\npassing in 10/12 projects after applying every edit proposed by NullRepair, and\n98% or more tests passing in the remaining two projects."}
