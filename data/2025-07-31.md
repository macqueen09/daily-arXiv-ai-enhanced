<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.SE](#cs.SE) [Total: 3]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [A Compute-Matched Re-Evaluation of TroVE on MATH](https://arxiv.org/abs/2507.22069)
*Tobias Sesterhenn,Ian Berlot-Attwell,Janis Zenkner,Christian Bartelt*

Main category: cs.PL

TL;DR: TroVE方法声称通过生成和复用工具箱在MATH基准上优于直接生成代码的PRIMITIVE基线，但研究发现其优势主要源于更高的计算预算而非工具箱机制。


<details>
  <summary>Details</summary>
Motivation: 探讨TroVE方法在MATH基准上的实际效果，验证其工具箱机制是否真正带来性能提升。

Method: 重新评估TroVE，分析其三种模式（直接生成代码、创建工具、复用工具）的影响，并修正其选择机制。

Result: 修正后TroVE性能提升3%，但在匹配计算预算后优势仅剩1%，表明工具箱机制效果有限。

Conclusion: TroVE的性能提升主要源于计算预算增加，工具箱机制对MATH基准的贡献不显著。

Abstract: Reusing established theorems and formulas is central to mathematical problem
solving, serving as essential building blocks for tackling increasingly complex
challenges. Recent work, TroVE, argues that code-generating Large Language
Models (LLMs) can benefit similarly on the MATH benchmark by inducing and
reusing higher-level toolboxes. By allocating computational budget across an
ensemble of three modes -- directly generating code, creating tools, and
reusing tools -- TroVE claims to outperform a PRIMITIVE baseline that only
performs direct generation. However, recent analysis (Berlot-Attwell et al.,
2024) casts doubt on these gains, noting that the tools created are often
trivial or rarely reused, suggesting that improvements may stem from
self-consistency or self-correction. In this work, we re-evaluate TroVE on
MATH, analyze the impact of each of its modes, and show that its benefit does
not come from these mechanisms, but simply from a higher computational budget
spent for TroVE compared to PRIMITIVE. To this end, we also perform a small
correction in the original implementation of TroVE's selection mechanism,
boosting TroVE's performance on MATH by 3\% in accuracy. After matching for
compute, the benefit of TroVE reduces to a marginal improvement of 1\%,
suggesting that this toolbox approach does not provide a significant benefit on
MATH.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [2] [Fuzzing: Randomness? Reasoning! Efficient Directed Fuzzing via Large Language Models](https://arxiv.org/abs/2507.22065)
*Xiaotao Feng,Xiaogang Zhu,Kun Hu,Jincheng Wang,Yingjie Cao,Guang Gong,Jianfeng Pan*

Main category: cs.SE

TL;DR: 论文提出RandLuzz，利用大语言模型（LLMs）减少模糊测试中的随机性，提高种子和变异器的质量，从而更高效地暴露漏洞。


<details>
  <summary>Details</summary>
Motivation: 模糊测试中的随机性虽然有助于发现漏洞，但降低了效率。即使定向模糊测试减少了随机性，种子和变异器的随机性仍是一个挑战。

Method: 利用LLMs生成可达种子和构建针对特定漏洞的变异器，通过分析函数调用链或功能指导LLMs生成种子，并通过漏洞分析生成变异器。

Result: RandLuzz在四种定向模糊测试工具上测试，种子生成速度提升2.1×至4.8×，个别漏洞暴露速度提升2.7×，8个漏洞可在60秒内暴露。

Conclusion: RandLuzz通过LLMs显著减少了模糊测试中的随机性，提高了漏洞暴露效率。

Abstract: Fuzzing is highly effective in detecting bugs due to the key contribution of
randomness. However, randomness significantly reduces the efficiency of
fuzzing, causing it to cost days or weeks to expose bugs. Even though directed
fuzzing reduces randomness by guiding fuzzing towards target buggy locations,
the dilemma of randomness still challenges directed fuzzers. Two critical
components, which are seeds and mutators, contain randomness and are closely
tied to the conditions required for triggering bugs. Therefore, to address the
challenge of randomness, we propose to use large language models (LLMs) to
remove the randomness in seeds and reduce the randomness in mutators. With
their strong reasoning and code generation capabilities, LLMs can be used to
generate reachable seeds that target pre-determined locations and to construct
bug-specific mutators tailored for specific bugs. We propose RandLuzz, which
integrates LLMs and directed fuzzing, to improve the quality of seeds and
mutators, resulting in efficient bug exposure. RandLuzz analyzes function call
chain or functionality to guide LLMs in generating reachable seeds. To
construct bug-specific mutators, RandLuzz uses LLMs to perform bug analysis,
obtaining information such as bug causes and mutation suggestions, which
further help generate code that performs bug-specific mutations. We evaluate
RandLuzz by comparing it with four state-of-the-art directed fuzzers, AFLGo,
Beacon, WindRanger, and SelectFuzz. With RandLuzz-generated seeds, the fuzzers
achieve an average speedup ranging from 2.1$\times$ to 4.8$\times$ compared to
using widely-used initial seeds. Additionally, when evaluated on individual
bugs, RandLuzz achieves up to a 2.7$\times$ speedup compared to the
second-fastest exposure. On 8 bugs, RandLuzz can even expose them within 60
seconds.

</details>


### [3] [Automated Test Data Generation for Enterprise Protobuf Systems: A Metaclass-Enhanced Statistical Approach](https://arxiv.org/abs/2507.22070)
*Y. Du*

Main category: cs.SE

TL;DR: 提出了一种基于Python元类系统和生产日志统计分析的测试数据生成框架，显著提升了企业级protobuf系统的测试效率和覆盖率。


<details>
  <summary>Details</summary>
Motivation: 企业级protobuf系统因其复杂的嵌套数据结构，传统测试数据生成方法难以应对，亟需高效解决方案。

Method: 结合动态类型增强、统计分析和递归下降算法，自动处理深层嵌套结构。

Result: 实验显示测试准备时间减少95%，覆盖率提升80%，支持15层嵌套结构，快速生成10万+测试用例。

Conclusion: 该框架高效解决了复杂protobuf结构的测试数据生成问题，显著提升测试效率。

Abstract: Large-scale enterprise systems utilizing Protocol Buffers (protobuf) present
significant challenges for performance testing, particularly when targeting
intermediate business interfaces with complex nested data structures.
Traditional test data generation approaches are inadequate for handling the
intricate hierarchical and graph-like structures inherent in enterprise
protobuf schemas. This paper presents a novel test data generation framework
that leverages Python's metaclass system for dynamic type enhancement and
statistical analysis of production logs for realistic value domain extraction.
Our approach combines automatic schema introspection, statistical value
distribution analysis, and recursive descent algorithms for handling deeply
nested structures. Experimental evaluation on three real-world enterprise
systems demonstrates up to 95\% reduction in test data preparation time and
80\% improvement in test coverage compared to existing approaches. The
framework successfully handles protobuf structures with up to 15 levels of
nesting and generates comprehensive test suites containing over 100,000 test
cases within seconds.

</details>


### [4] [TypyBench: Evaluating LLM Type Inference for Untyped Python Repositories](https://arxiv.org/abs/2507.22086)
*Honghua Dong,Jiacheng Yang,Xun Deng,Yuhe Jiang,Gennady Pekhimenko,Fan Long,Xujie Si*

Main category: cs.SE

TL;DR: TypyBench是一个评估大型语言模型（LLMs）在Python代码库中类型推断能力的基准，提出了TypeSim和TypeCheck两种新指标，发现LLMs在复杂嵌套类型和类型一致性上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 动态语言（如Python）的类型推断是一个持续挑战，LLMs的类型推断能力尚未充分探索。

Method: 引入TypyBench基准，评估LLMs在50个高质量Python代码库中的类型推断能力，使用TypeSim和TypeCheck指标。

Result: LLMs在TypeSim上表现尚可，但在复杂嵌套类型和类型一致性上存在显著问题。

Conclusion: 未来研究应关注代码库级别的类型一致性，TypyBench为此提供了基础。

Abstract: Type inference for dynamic languages like Python is a persistent challenge in
software engineering. While large language models (LLMs) have shown promise in
code understanding, their type inference capabilities remain underexplored. We
introduce TypyBench, a benchmark designed to evaluate LLMs' type inference
across entire Python repositories. TypyBench features two novel metrics:
TypeSim, which captures nuanced semantic relationships between predicted and
ground truth types, and TypeCheck, which assesses type consistency across
codebases. Our evaluation of various LLMs on a curated dataset of 50
high-quality Python repositories reveals that, although LLMs achieve decent
TypeSim scores, they struggle with complex nested types and exhibit significant
type consistency errors. These findings suggest that future research should
shift focus from improving type similarity to addressing repository-level
consistency. TypyBench provides a foundation for this new direction, offering
insights into model performance across different type complexities and usage
contexts. Our code and data are available at
https://github.com/typybench/typybench.

</details>
