{"id": "2507.22069", "pdf": "https://arxiv.org/pdf/2507.22069", "abs": "https://arxiv.org/abs/2507.22069", "authors": ["Tobias Sesterhenn", "Ian Berlot-Attwell", "Janis Zenkner", "Christian Bartelt"], "title": "A Compute-Matched Re-Evaluation of TroVE on MATH", "categories": ["cs.PL", "cs.AI"], "comment": null, "summary": "Reusing established theorems and formulas is central to mathematical problem\nsolving, serving as essential building blocks for tackling increasingly complex\nchallenges. Recent work, TroVE, argues that code-generating Large Language\nModels (LLMs) can benefit similarly on the MATH benchmark by inducing and\nreusing higher-level toolboxes. By allocating computational budget across an\nensemble of three modes -- directly generating code, creating tools, and\nreusing tools -- TroVE claims to outperform a PRIMITIVE baseline that only\nperforms direct generation. However, recent analysis (Berlot-Attwell et al.,\n2024) casts doubt on these gains, noting that the tools created are often\ntrivial or rarely reused, suggesting that improvements may stem from\nself-consistency or self-correction. In this work, we re-evaluate TroVE on\nMATH, analyze the impact of each of its modes, and show that its benefit does\nnot come from these mechanisms, but simply from a higher computational budget\nspent for TroVE compared to PRIMITIVE. To this end, we also perform a small\ncorrection in the original implementation of TroVE's selection mechanism,\nboosting TroVE's performance on MATH by 3\\% in accuracy. After matching for\ncompute, the benefit of TroVE reduces to a marginal improvement of 1\\%,\nsuggesting that this toolbox approach does not provide a significant benefit on\nMATH.", "AI": {"tldr": "TroVE\u65b9\u6cd5\u58f0\u79f0\u901a\u8fc7\u751f\u6210\u548c\u590d\u7528\u5de5\u5177\u7bb1\u5728MATH\u57fa\u51c6\u4e0a\u4f18\u4e8e\u76f4\u63a5\u751f\u6210\u4ee3\u7801\u7684PRIMITIVE\u57fa\u7ebf\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u5176\u4f18\u52bf\u4e3b\u8981\u6e90\u4e8e\u66f4\u9ad8\u7684\u8ba1\u7b97\u9884\u7b97\u800c\u975e\u5de5\u5177\u7bb1\u673a\u5236\u3002", "motivation": "\u63a2\u8ba8TroVE\u65b9\u6cd5\u5728MATH\u57fa\u51c6\u4e0a\u7684\u5b9e\u9645\u6548\u679c\uff0c\u9a8c\u8bc1\u5176\u5de5\u5177\u7bb1\u673a\u5236\u662f\u5426\u771f\u6b63\u5e26\u6765\u6027\u80fd\u63d0\u5347\u3002", "method": "\u91cd\u65b0\u8bc4\u4f30TroVE\uff0c\u5206\u6790\u5176\u4e09\u79cd\u6a21\u5f0f\uff08\u76f4\u63a5\u751f\u6210\u4ee3\u7801\u3001\u521b\u5efa\u5de5\u5177\u3001\u590d\u7528\u5de5\u5177\uff09\u7684\u5f71\u54cd\uff0c\u5e76\u4fee\u6b63\u5176\u9009\u62e9\u673a\u5236\u3002", "result": "\u4fee\u6b63\u540eTroVE\u6027\u80fd\u63d0\u53473%\uff0c\u4f46\u5728\u5339\u914d\u8ba1\u7b97\u9884\u7b97\u540e\u4f18\u52bf\u4ec5\u52691%\uff0c\u8868\u660e\u5de5\u5177\u7bb1\u673a\u5236\u6548\u679c\u6709\u9650\u3002", "conclusion": "TroVE\u7684\u6027\u80fd\u63d0\u5347\u4e3b\u8981\u6e90\u4e8e\u8ba1\u7b97\u9884\u7b97\u589e\u52a0\uff0c\u5de5\u5177\u7bb1\u673a\u5236\u5bf9MATH\u57fa\u51c6\u7684\u8d21\u732e\u4e0d\u663e\u8457\u3002"}}
{"id": "2507.22065", "pdf": "https://arxiv.org/pdf/2507.22065", "abs": "https://arxiv.org/abs/2507.22065", "authors": ["Xiaotao Feng", "Xiaogang Zhu", "Kun Hu", "Jincheng Wang", "Yingjie Cao", "Guang Gong", "Jianfeng Pan"], "title": "Fuzzing: Randomness? Reasoning! Efficient Directed Fuzzing via Large Language Models", "categories": ["cs.SE", "cs.AI", "cs.CR", "cs.PL"], "comment": null, "summary": "Fuzzing is highly effective in detecting bugs due to the key contribution of\nrandomness. However, randomness significantly reduces the efficiency of\nfuzzing, causing it to cost days or weeks to expose bugs. Even though directed\nfuzzing reduces randomness by guiding fuzzing towards target buggy locations,\nthe dilemma of randomness still challenges directed fuzzers. Two critical\ncomponents, which are seeds and mutators, contain randomness and are closely\ntied to the conditions required for triggering bugs. Therefore, to address the\nchallenge of randomness, we propose to use large language models (LLMs) to\nremove the randomness in seeds and reduce the randomness in mutators. With\ntheir strong reasoning and code generation capabilities, LLMs can be used to\ngenerate reachable seeds that target pre-determined locations and to construct\nbug-specific mutators tailored for specific bugs. We propose RandLuzz, which\nintegrates LLMs and directed fuzzing, to improve the quality of seeds and\nmutators, resulting in efficient bug exposure. RandLuzz analyzes function call\nchain or functionality to guide LLMs in generating reachable seeds. To\nconstruct bug-specific mutators, RandLuzz uses LLMs to perform bug analysis,\nobtaining information such as bug causes and mutation suggestions, which\nfurther help generate code that performs bug-specific mutations. We evaluate\nRandLuzz by comparing it with four state-of-the-art directed fuzzers, AFLGo,\nBeacon, WindRanger, and SelectFuzz. With RandLuzz-generated seeds, the fuzzers\nachieve an average speedup ranging from 2.1$\\times$ to 4.8$\\times$ compared to\nusing widely-used initial seeds. Additionally, when evaluated on individual\nbugs, RandLuzz achieves up to a 2.7$\\times$ speedup compared to the\nsecond-fastest exposure. On 8 bugs, RandLuzz can even expose them within 60\nseconds.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faRandLuzz\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u51cf\u5c11\u6a21\u7cca\u6d4b\u8bd5\u4e2d\u7684\u968f\u673a\u6027\uff0c\u63d0\u9ad8\u79cd\u5b50\u548c\u53d8\u5f02\u5668\u7684\u8d28\u91cf\uff0c\u4ece\u800c\u66f4\u9ad8\u6548\u5730\u66b4\u9732\u6f0f\u6d1e\u3002", "motivation": "\u6a21\u7cca\u6d4b\u8bd5\u4e2d\u7684\u968f\u673a\u6027\u867d\u7136\u6709\u52a9\u4e8e\u53d1\u73b0\u6f0f\u6d1e\uff0c\u4f46\u964d\u4f4e\u4e86\u6548\u7387\u3002\u5373\u4f7f\u5b9a\u5411\u6a21\u7cca\u6d4b\u8bd5\u51cf\u5c11\u4e86\u968f\u673a\u6027\uff0c\u79cd\u5b50\u548c\u53d8\u5f02\u5668\u7684\u968f\u673a\u6027\u4ecd\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u5229\u7528LLMs\u751f\u6210\u53ef\u8fbe\u79cd\u5b50\u548c\u6784\u5efa\u9488\u5bf9\u7279\u5b9a\u6f0f\u6d1e\u7684\u53d8\u5f02\u5668\uff0c\u901a\u8fc7\u5206\u6790\u51fd\u6570\u8c03\u7528\u94fe\u6216\u529f\u80fd\u6307\u5bfcLLMs\u751f\u6210\u79cd\u5b50\uff0c\u5e76\u901a\u8fc7\u6f0f\u6d1e\u5206\u6790\u751f\u6210\u53d8\u5f02\u5668\u3002", "result": "RandLuzz\u5728\u56db\u79cd\u5b9a\u5411\u6a21\u7cca\u6d4b\u8bd5\u5de5\u5177\u4e0a\u6d4b\u8bd5\uff0c\u79cd\u5b50\u751f\u6210\u901f\u5ea6\u63d0\u53472.1\u00d7\u81f34.8\u00d7\uff0c\u4e2a\u522b\u6f0f\u6d1e\u66b4\u9732\u901f\u5ea6\u63d0\u53472.7\u00d7\uff0c8\u4e2a\u6f0f\u6d1e\u53ef\u572860\u79d2\u5185\u66b4\u9732\u3002", "conclusion": "RandLuzz\u901a\u8fc7LLMs\u663e\u8457\u51cf\u5c11\u4e86\u6a21\u7cca\u6d4b\u8bd5\u4e2d\u7684\u968f\u673a\u6027\uff0c\u63d0\u9ad8\u4e86\u6f0f\u6d1e\u66b4\u9732\u6548\u7387\u3002"}}
{"id": "2507.22070", "pdf": "https://arxiv.org/pdf/2507.22070", "abs": "https://arxiv.org/abs/2507.22070", "authors": ["Y. Du"], "title": "Automated Test Data Generation for Enterprise Protobuf Systems: A Metaclass-Enhanced Statistical Approach", "categories": ["cs.SE", "cs.CE", "cs.PL"], "comment": "7 pages", "summary": "Large-scale enterprise systems utilizing Protocol Buffers (protobuf) present\nsignificant challenges for performance testing, particularly when targeting\nintermediate business interfaces with complex nested data structures.\nTraditional test data generation approaches are inadequate for handling the\nintricate hierarchical and graph-like structures inherent in enterprise\nprotobuf schemas. This paper presents a novel test data generation framework\nthat leverages Python's metaclass system for dynamic type enhancement and\nstatistical analysis of production logs for realistic value domain extraction.\nOur approach combines automatic schema introspection, statistical value\ndistribution analysis, and recursive descent algorithms for handling deeply\nnested structures. Experimental evaluation on three real-world enterprise\nsystems demonstrates up to 95\\% reduction in test data preparation time and\n80\\% improvement in test coverage compared to existing approaches. The\nframework successfully handles protobuf structures with up to 15 levels of\nnesting and generates comprehensive test suites containing over 100,000 test\ncases within seconds.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8ePython\u5143\u7c7b\u7cfb\u7edf\u548c\u751f\u4ea7\u65e5\u5fd7\u7edf\u8ba1\u5206\u6790\u7684\u6d4b\u8bd5\u6570\u636e\u751f\u6210\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f01\u4e1a\u7ea7protobuf\u7cfb\u7edf\u7684\u6d4b\u8bd5\u6548\u7387\u548c\u8986\u76d6\u7387\u3002", "motivation": "\u4f01\u4e1a\u7ea7protobuf\u7cfb\u7edf\u56e0\u5176\u590d\u6742\u7684\u5d4c\u5957\u6570\u636e\u7ed3\u6784\uff0c\u4f20\u7edf\u6d4b\u8bd5\u6570\u636e\u751f\u6210\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\uff0c\u4e9f\u9700\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u52a8\u6001\u7c7b\u578b\u589e\u5f3a\u3001\u7edf\u8ba1\u5206\u6790\u548c\u9012\u5f52\u4e0b\u964d\u7b97\u6cd5\uff0c\u81ea\u52a8\u5904\u7406\u6df1\u5c42\u5d4c\u5957\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u6d4b\u8bd5\u51c6\u5907\u65f6\u95f4\u51cf\u5c1195%\uff0c\u8986\u76d6\u7387\u63d0\u534780%\uff0c\u652f\u630115\u5c42\u5d4c\u5957\u7ed3\u6784\uff0c\u5feb\u901f\u751f\u621010\u4e07+\u6d4b\u8bd5\u7528\u4f8b\u3002", "conclusion": "\u8be5\u6846\u67b6\u9ad8\u6548\u89e3\u51b3\u4e86\u590d\u6742protobuf\u7ed3\u6784\u7684\u6d4b\u8bd5\u6570\u636e\u751f\u6210\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6d4b\u8bd5\u6548\u7387\u3002"}}
{"id": "2507.22086", "pdf": "https://arxiv.org/pdf/2507.22086", "abs": "https://arxiv.org/abs/2507.22086", "authors": ["Honghua Dong", "Jiacheng Yang", "Xun Deng", "Yuhe Jiang", "Gennady Pekhimenko", "Fan Long", "Xujie Si"], "title": "TypyBench: Evaluating LLM Type Inference for Untyped Python Repositories", "categories": ["cs.SE", "cs.AI", "cs.PL"], "comment": null, "summary": "Type inference for dynamic languages like Python is a persistent challenge in\nsoftware engineering. While large language models (LLMs) have shown promise in\ncode understanding, their type inference capabilities remain underexplored. We\nintroduce TypyBench, a benchmark designed to evaluate LLMs' type inference\nacross entire Python repositories. TypyBench features two novel metrics:\nTypeSim, which captures nuanced semantic relationships between predicted and\nground truth types, and TypeCheck, which assesses type consistency across\ncodebases. Our evaluation of various LLMs on a curated dataset of 50\nhigh-quality Python repositories reveals that, although LLMs achieve decent\nTypeSim scores, they struggle with complex nested types and exhibit significant\ntype consistency errors. These findings suggest that future research should\nshift focus from improving type similarity to addressing repository-level\nconsistency. TypyBench provides a foundation for this new direction, offering\ninsights into model performance across different type complexities and usage\ncontexts. Our code and data are available at\nhttps://github.com/typybench/typybench.", "AI": {"tldr": "TypyBench\u662f\u4e00\u4e2a\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728Python\u4ee3\u7801\u5e93\u4e2d\u7c7b\u578b\u63a8\u65ad\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u63d0\u51fa\u4e86TypeSim\u548cTypeCheck\u4e24\u79cd\u65b0\u6307\u6807\uff0c\u53d1\u73b0LLMs\u5728\u590d\u6742\u5d4c\u5957\u7c7b\u578b\u548c\u7c7b\u578b\u4e00\u81f4\u6027\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u52a8\u6001\u8bed\u8a00\uff08\u5982Python\uff09\u7684\u7c7b\u578b\u63a8\u65ad\u662f\u4e00\u4e2a\u6301\u7eed\u6311\u6218\uff0cLLMs\u7684\u7c7b\u578b\u63a8\u65ad\u80fd\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5f15\u5165TypyBench\u57fa\u51c6\uff0c\u8bc4\u4f30LLMs\u572850\u4e2a\u9ad8\u8d28\u91cfPython\u4ee3\u7801\u5e93\u4e2d\u7684\u7c7b\u578b\u63a8\u65ad\u80fd\u529b\uff0c\u4f7f\u7528TypeSim\u548cTypeCheck\u6307\u6807\u3002", "result": "LLMs\u5728TypeSim\u4e0a\u8868\u73b0\u5c1a\u53ef\uff0c\u4f46\u5728\u590d\u6742\u5d4c\u5957\u7c7b\u578b\u548c\u7c7b\u578b\u4e00\u81f4\u6027\u4e0a\u5b58\u5728\u663e\u8457\u95ee\u9898\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u5e94\u5173\u6ce8\u4ee3\u7801\u5e93\u7ea7\u522b\u7684\u7c7b\u578b\u4e00\u81f4\u6027\uff0cTypyBench\u4e3a\u6b64\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
