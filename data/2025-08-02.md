<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.LG](#cs.LG) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Abstractions of Sequences, Functions and Operators](https://arxiv.org/abs/2507.23151)
*Louis Rustenholz,Pedro Lopez-Garcia,Manuel V. Hermenegildo*

Main category: cs.PL

TL;DR: 论文提出了一种新的约束抽象域B-bound domains，用于抽象数值函数，并引入域抽象技术，支持从符号函数到数值函数的抽象。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决递归定义函数的闭式边界推断问题，应用于程序分析和混合系统。

Method: 方法包括构建B-bound domains抽象域和域抽象技术，基于简单的操作语言设计转移函数。

Result: 结果展示了B-bound domains能推断高度非线性数值不变量，并揭示了约束空间的凸性特性。

Conclusion: 结论是该方法简化了转移函数设计，并在某些情况下实现了完全自动化。

Abstract: We present theoretical and practical results on the order theory of lattices
of functions, focusing on Galois connections that abstract (sets of) functions
- a topic known as higher-order abstract interpretation.
  We are motivated by the challenge of inferring closed-form bounds on
functions which are defined recursively, i.e. as the fixed point of an operator
or, equivalently, as the solution to a functional equation. This has multiple
applications in program analysis (e.g. cost analysis, loop acceleration,
declarative language analysis) and in hybrid systems governed by differential
equations.
  Our main contribution is a new family of constraint-based abstract domains
for abstracting numerical functions, B-bound domains, which abstract a function
f by a conjunction of bounds from a preselected set of boundary functions. They
allow inferring highly non-linear numerical invariants, which classical
numerical abstract domains struggle with. We uncover a convexity property in
the constraint space that simplifies, and, in some cases, fully automates,
transfer function design.
  We also introduce domain abstraction, a functor that lifts arbitrary mappings
in value space to Galois connections in function space. This supports
abstraction from symbolic to numerical functions (i.e. size abstraction), and
enables dimensionality reduction of equations.
  We base our constructions of transfer functions on a simple operator
language, starting with sequences, and extending to more general functions,
including multivariate, piecewise, and non-discrete domains.

</details>


### [2] [Kernel-FFI: Transparent Foreign Function Interfaces for Interactive Notebooks](https://arxiv.org/abs/2507.23205)
*Hebi Li,Forrest Sheng Bao,Qi Xiao,Jin Tian*

Main category: cs.PL

TL;DR: Kernel-FFI是一个透明、语言无关的框架，用于在交互式笔记本中实现无缝的跨语言函数调用和对象操作，解决了现有FFI解决方案在动态交互环境中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有FFI解决方案在动态交互式笔记本环境（如Jupyter）中表现不佳，需要大量手动配置和样板代码，且缺乏对递归调用和面向对象编程的支持。

Method: Kernel-FFI通过源代码级转换自动重写跨语言调用，无需手动绑定或样板代码，并引入侧通道通信机制以支持递归和异步调用。

Result: Kernel-FFI实现了无缝的跨语言函数调用和对象操作，支持面向对象编程和自动资源管理。

Conclusion: Kernel-FFI为交互式笔记本环境提供了一种高效的跨语言互操作解决方案，解决了现有FFI的局限性。

Abstract: Foreign Function Interfaces (FFIs) are essential for enabling
interoperability between programming languages, yet existing FFI solutions are
ill-suited for the dynamic, interactive workflows prevalent in modern notebook
environments such as Jupyter. Current approaches require extensive manual
configuration, introduce significant boilerplate, and often lack support for
recursive calls and object-oriented programming (OOP) constructs-features
critical for productive, multi-language development.
  We present Kernel-FFI, a transparent, language-agnostic framework that
enables seamless cross-language function calls and object manipulation within
interactive notebooks. Kernel-FFI employs source-level transformation to
automatically rewrite cross-language invocations, eliminating the need for
manual bindings or boilerplate. Kernel-FFI provides robust support for OOP by
enabling foreign object referencing and automatic resource management across
language boundaries. Furthermore, to address the blocking nature of Jupyter
kernels and support recursive and asynchronous foreign calls, we introduce a
novel side-channel communication mechanism. Our tool will be open-sourced and
available at https://codepod.io/docs/kernel-ffi

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [3] [NaN-Propagation: A Novel Method for Sparsity Detection in Black-Box Computational Functions](https://arxiv.org/abs/2507.23186)
*Peter Sharpe*

Main category: cs.LG

TL;DR: 通过NaN传播技术检测黑盒函数的稀疏性，避免传统有限差分方法的假阴性问题，显著提升梯度计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有有限差分方法因偶然零梯度导致假阴性，可能破坏梯度计算，难以诊断。

Method: 利用IEEE 754 NaN的普遍污染特性，通过系统污染输入并观察输出NaN，重建保守稀疏模式。

Result: 在航空航天机翼重量模型中实现1.52倍加速，检测到传统方法遗漏的依赖关系。

Conclusion: NaN传播技术跨语言兼容，无需修改黑盒代码，显著优化稀疏性检测。

Abstract: Sparsity detection in black-box functions enables significant computational
speedups in gradient-based optimization through Jacobian compression, but
existing finite-difference methods suffer from false negatives due to
coincidental zero gradients. These false negatives can silently corrupt
gradient calculations, leading to difficult-to-diagnose errors. We introduce
NaN-propagation, which exploits the universal contamination property of IEEE
754 Not-a-Number floating-point values to trace input-output dependencies
through floating-point numerical computations. By systematically contaminating
inputs with NaN and observing which outputs become NaN, the method reconstructs
conservative sparsity patterns that eliminate false negatives. We demonstrate
the approach on an aerospace wing weight model, achieving a 1.52x speedup while
detecting dozens of dependencies missed by conventional methods -- a
significant improvement since gradient computation is the bottleneck in many
optimization workflows. The technique leverages IEEE 754 compliance to work
across programming languages and math libraries without modifying existing
black-box codes. Advanced strategies including NaN payload encoding enable
faster-than-linear time complexity, improving upon existing black-box sparsity
detection methods. Practical algorithms are also proposed to mitigate
challenges from branching code execution common in engineering applications.

</details>


### [4] [SequenceLayers: Sequence Processing and Streaming Neural Networks Made Easy](https://arxiv.org/abs/2507.23292)
*RJ Skerry-Ryan,Julian Salazar,Soroosh Mariooryad,David Kao,Daisy Stanton,Eric Battenberg,Matt Shannon,Ron J. Weiss,Robin Scheibler,Jonas Rothfuss,Tom Bagby*

Main category: cs.LG

TL;DR: 介绍了一种用于序列建模的神经网络层API和库，支持逐层和逐步执行，确保状态一致性和流式处理能力。


<details>
  <summary>Details</summary>
Motivation: 简化序列模型的创建，支持流式处理和并行序列处理，减少常见错误。

Method: 定义显式状态表示和逐步更新方法，确保与逐层调用结果一致。

Result: 实现了可立即流式处理的复杂模型，提供了丰富的层和组合器。

Conclusion: SequenceLayers API和库为生产级模型提供了简单、可组合且正确的构建方式。

Abstract: We introduce a neural network layer API and library for sequence modeling,
designed for easy creation of sequence models that can be executed both
layer-by-layer (e.g., teacher-forced training) and step-by-step (e.g.,
autoregressive sampling). To achieve this, layers define an explicit
representation of their state over time (e.g., a Transformer KV cache, a
convolution buffer, an RNN hidden state), and a step method that evolves that
state, tested to give identical results to a stateless layer-wise invocation.
This and other aspects of the SequenceLayers contract enables complex models to
be immediately streamable, mitigates a wide range of common bugs arising in
both streaming and parallel sequence processing, and can be implemented in any
deep learning library. A composable and declarative API, along with a
comprehensive suite of layers and combinators, streamlines the construction of
production-scale models from simple streamable components while preserving
strong correctness guarantees. Our current implementations of SequenceLayers
(JAX, TensorFlow 2) are available at https://github.com/google/sequence-layers.

</details>
