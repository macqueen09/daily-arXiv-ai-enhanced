<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.LG](#cs.LG) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Abstractions of Sequences, Functions and Operators](https://arxiv.org/abs/2507.23151)
*Louis Rustenholz,Pedro Lopez-Garcia,Manuel V. Hermenegildo*

Main category: cs.PL

TL;DR: 本文研究了函数格序理论，提出了一种新的约束抽象域B-bound domains，用于推断递归定义的函数的闭式边界，并引入了域抽象技术。


<details>
  <summary>Details</summary>
Motivation: 动机在于解决递归定义函数的闭式边界推断问题，应用于程序分析和混合系统等领域。

Method: 方法包括提出B-bound domains抽象域，利用凸性简化传递函数设计，并引入域抽象技术。

Result: 结果表明B-bound domains能推断高度非线性数值不变量，优于经典数值抽象域。

Conclusion: 结论是这些方法在程序分析和混合系统中具有广泛应用潜力。

Abstract: We present theoretical and practical results on the order theory of lattices
of functions, focusing on Galois connections that abstract (sets of) functions
- a topic known as higher-order abstract interpretation.
  We are motivated by the challenge of inferring closed-form bounds on
functions which are defined recursively, i.e. as the fixed point of an operator
or, equivalently, as the solution to a functional equation. This has multiple
applications in program analysis (e.g. cost analysis, loop acceleration,
declarative language analysis) and in hybrid systems governed by differential
equations.
  Our main contribution is a new family of constraint-based abstract domains
for abstracting numerical functions, B-bound domains, which abstract a function
f by a conjunction of bounds from a preselected set of boundary functions. They
allow inferring highly non-linear numerical invariants, which classical
numerical abstract domains struggle with. We uncover a convexity property in
the constraint space that simplifies, and, in some cases, fully automates,
transfer function design.
  We also introduce domain abstraction, a functor that lifts arbitrary mappings
in value space to Galois connections in function space. This supports
abstraction from symbolic to numerical functions (i.e. size abstraction), and
enables dimensionality reduction of equations.
  We base our constructions of transfer functions on a simple operator
language, starting with sequences, and extending to more general functions,
including multivariate, piecewise, and non-discrete domains.

</details>


### [2] [Kernel-FFI: Transparent Foreign Function Interfaces for Interactive Notebooks](https://arxiv.org/abs/2507.23205)
*Hebi Li,Forrest Sheng Bao,Qi Xiao,Jin Tian*

Main category: cs.PL

TL;DR: Kernel-FFI是一个透明、语言无关的框架，用于在交互式笔记本中实现无缝跨语言函数调用和对象操作，解决了现有FFI解决方案在动态工作流中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有FFI解决方案在动态交互式笔记本环境中存在配置繁琐、缺乏递归调用和面向对象编程支持等问题，限制了多语言开发的效率。

Method: Kernel-FFI通过源代码级转换自动重写跨语言调用，无需手动绑定或冗余代码，并引入侧通道通信机制支持递归和异步调用。

Result: Kernel-FFI提供了对面向对象编程的全面支持，包括跨语言对象引用和自动资源管理，显著提升了多语言开发的便捷性。

Conclusion: Kernel-FFI为交互式笔记本环境中的跨语言开发提供了高效、透明的解决方案，并将开源以促进广泛应用。

Abstract: Foreign Function Interfaces (FFIs) are essential for enabling
interoperability between programming languages, yet existing FFI solutions are
ill-suited for the dynamic, interactive workflows prevalent in modern notebook
environments such as Jupyter. Current approaches require extensive manual
configuration, introduce significant boilerplate, and often lack support for
recursive calls and object-oriented programming (OOP) constructs-features
critical for productive, multi-language development.
  We present Kernel-FFI, a transparent, language-agnostic framework that
enables seamless cross-language function calls and object manipulation within
interactive notebooks. Kernel-FFI employs source-level transformation to
automatically rewrite cross-language invocations, eliminating the need for
manual bindings or boilerplate. Kernel-FFI provides robust support for OOP by
enabling foreign object referencing and automatic resource management across
language boundaries. Furthermore, to address the blocking nature of Jupyter
kernels and support recursive and asynchronous foreign calls, we introduce a
novel side-channel communication mechanism. Our tool will be open-sourced and
available at https://codepod.io/docs/kernel-ffi

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [3] [NaN-Propagation: A Novel Method for Sparsity Detection in Black-Box Computational Functions](https://arxiv.org/abs/2507.23186)
*Peter Sharpe*

Main category: cs.LG

TL;DR: 论文提出了一种基于NaN传播的方法，用于检测黑盒函数中的稀疏性，避免了传统有限差分方法中的假阴性问题，显著提升了梯度计算的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统有限差分方法在检测黑盒函数稀疏性时存在假阴性问题，导致梯度计算错误且难以诊断。

Method: 通过系统性地用NaN污染输入并观察输出是否变为NaN，重建保守的稀疏模式，消除假阴性。

Result: 在航空航天机翼重量模型中，实现了1.52倍的加速，并检测到传统方法遗漏的数十个依赖关系。

Conclusion: NaN传播方法利用IEEE 754标准，无需修改现有黑盒代码即可跨语言和数学库工作，显著提升了稀疏性检测的效率和准确性。

Abstract: Sparsity detection in black-box functions enables significant computational
speedups in gradient-based optimization through Jacobian compression, but
existing finite-difference methods suffer from false negatives due to
coincidental zero gradients. These false negatives can silently corrupt
gradient calculations, leading to difficult-to-diagnose errors. We introduce
NaN-propagation, which exploits the universal contamination property of IEEE
754 Not-a-Number floating-point values to trace input-output dependencies
through floating-point numerical computations. By systematically contaminating
inputs with NaN and observing which outputs become NaN, the method reconstructs
conservative sparsity patterns that eliminate false negatives. We demonstrate
the approach on an aerospace wing weight model, achieving a 1.52x speedup while
detecting dozens of dependencies missed by conventional methods -- a
significant improvement since gradient computation is the bottleneck in many
optimization workflows. The technique leverages IEEE 754 compliance to work
across programming languages and math libraries without modifying existing
black-box codes. Advanced strategies including NaN payload encoding enable
faster-than-linear time complexity, improving upon existing black-box sparsity
detection methods. Practical algorithms are also proposed to mitigate
challenges from branching code execution common in engineering applications.

</details>


### [4] [SequenceLayers: Sequence Processing and Streaming Neural Networks Made Easy](https://arxiv.org/abs/2507.23292)
*RJ Skerry-Ryan,Julian Salazar,Soroosh Mariooryad,David Kao,Daisy Stanton,Eric Battenberg,Matt Shannon,Ron J. Weiss,Robin Scheibler,Jonas Rothfuss,Tom Bagby*

Main category: cs.LG

TL;DR: 介绍了一个用于序列建模的神经网络层API和库，支持逐层和逐步执行，确保状态一致性。


<details>
  <summary>Details</summary>
Motivation: 简化序列模型的创建，支持流式处理和并行序列处理，减少常见错误。

Method: 定义显式状态表示和逐步方法，确保与无状态逐层调用结果一致。

Result: 实现了可立即流式处理的复杂模型，提供了全面的层和组合器。

Conclusion: SequenceLayers库在JAX和TensorFlow 2中可用，支持生产级模型的构建。

Abstract: We introduce a neural network layer API and library for sequence modeling,
designed for easy creation of sequence models that can be executed both
layer-by-layer (e.g., teacher-forced training) and step-by-step (e.g.,
autoregressive sampling). To achieve this, layers define an explicit
representation of their state over time (e.g., a Transformer KV cache, a
convolution buffer, an RNN hidden state), and a step method that evolves that
state, tested to give identical results to a stateless layer-wise invocation.
This and other aspects of the SequenceLayers contract enables complex models to
be immediately streamable, mitigates a wide range of common bugs arising in
both streaming and parallel sequence processing, and can be implemented in any
deep learning library. A composable and declarative API, along with a
comprehensive suite of layers and combinators, streamlines the construction of
production-scale models from simple streamable components while preserving
strong correctness guarantees. Our current implementations of SequenceLayers
(JAX, TensorFlow 2) are available at https://github.com/google/sequence-layers.

</details>
