{"id": "2508.00005", "pdf": "https://arxiv.org/pdf/2508.00005", "abs": "https://arxiv.org/abs/2508.00005", "authors": ["Tilman Hinnerichs", "Bart Swinkels", "Jaap de Jong", "Reuben Gardos Reid", "Tudor Magirescu", "Neil Yorke-Smith", "Sebastijan Dumancic"], "title": "Modelling Program Spaces in Program Synthesis with Constraints", "categories": ["cs.PL", "cs.AI"], "comment": null, "summary": "A core challenge in program synthesis is taming the large space of possible\nprograms. Since program synthesis is essentially a combinatorial search, the\ncommunity has sought to leverage powerful combinatorial constraint solvers.\nHere, constraints are used to express the program semantics, but not as a\npotentially potent tool to remove unwanted programs. Recent inductive logic\nprogramming approaches introduce constraints on the program's syntax to be\nsynthesized. These syntactic constraints allow for checking and propagating a\nconstraint without executing the program, and thus for arbitrary operators. In\nthis work, we leverage syntactic constraints to model program spaces, defining\nnot just solutions that are feasible, but also ones that are likely useful. To\ndemonstrate this idea, we introduce BART, a solver that efficiently propagates\nand solves these constraints. We evaluate BART on program space enumeration\ntasks, finding that the constraints eliminate up to 99 percent of the program\nspace, and that modeling program spaces significantly reduces enumeration time."}
{"id": "2508.00013", "pdf": "https://arxiv.org/pdf/2508.00013", "abs": "https://arxiv.org/abs/2508.00013", "authors": ["Zurabi Kobaladze", "Anna Arnania", "Tamar Sanikidze"], "title": "From Provable Correctness to Probabilistic Generation: A Comparative Review of Program Synthesis Paradigms", "categories": ["cs.PL", "I.2.6; F.1.1"], "comment": "78 pages. Undergraduate thesis project submitted in partial\n  fulfillment of the requirements for the Bachelor's degree in Computer Science\n  at Kutaisi International University", "summary": "Program synthesis--the automated generation of executable code from\nhigh-level specifications--has been a central goal of computer science for over\nfifty years. This thesis provides a comparative literature review of the main\nparadigms that have shaped the field, tracing its evolution from formal logic\nbased methods to recent advances using large scale neural models. We examine\nfive key approaches: logic based (deductive) synthesis, inductive (example\nbased) synthesis, sketch/schema based synthesis, large language model based\nsynthesis, and neuro-symbolic hybrids. For each, we analyze foundational\nprinciples, notable systems, and practical applications, highlighting trade\noffs between correctness guarantees, specification requirements, search\ncomplexity, and expressive power. By reviewing developments from formally\nverified synthesis tools such as KIDS and Coq to data driven models generating\nprobabilistic code from natural language like Codex, we present a comprehensive\nnarrative of progress and ongoing challenges. This work emphasizes the\ntransition from symbolic to hybrid neuro-symbolic methods and outlines future\ndirections for reliable and scalable program synthesis."}
{"id": "2508.00016", "pdf": "https://arxiv.org/pdf/2508.00016", "abs": "https://arxiv.org/abs/2508.00016", "authors": ["Matt Kaufmann", "Yahya Sohail", "Warren A. Hunt Jr"], "title": "Extended Abstract: Mutable Objects with Several Implementations", "categories": ["cs.PL", "cs.LO"], "comment": "In Proceedings ACL2 2025, arXiv:2507.18567", "summary": "This extended abstract outlines an ACL2 feature, attach-stobj, that first\nappeared in ACL2 Version 8.6 (October, 2024). This feature supports different\nexecutable operations for a given abstract stobj, without requiring\nrecertification of the book that introduces that stobj or theorems about it.\nThe paper provides background as well as a user-level overview and some\nimplementation notes."}
{"id": "2508.00422", "pdf": "https://arxiv.org/pdf/2508.00422", "abs": "https://arxiv.org/abs/2508.00422", "authors": ["Varun Bharti", "Shashwat Jha", "Dhruv Kumar", "Pankaj Jalote"], "title": "Automated Type Annotation in Python Using Large Language Models", "categories": ["cs.PL", "cs.LG"], "comment": "Under Review", "summary": "Type annotations in Python enhance maintainability and error detection.\nHowever, generating these annotations manually is error prone and requires\nextra effort. Traditional automation approaches like static analysis, machine\nlearning, and deep learning struggle with limited type vocabularies, behavioral\nover approximation, and reliance on large labeled datasets. In this work, we\nexplore the use of LLMs for generating type annotations in Python. We develop a\ngenerate check repair pipeline: the LLM proposes annotations guided by a\nConcrete Syntax Tree representation, a static type checker (Mypy) verifies\nthem, and any errors are fed back for iterative refinement. We evaluate four\nLLM variants: GPT 4oMini, GPT 4.1mini (general-purpose), and O3Mini, O4Mini\n(reasoning optimized), on 6000 code snippets from the ManyTypes4Py benchmark.\nWe first measure the proportion of code snippets annotated by LLMs for which\nMyPy reported no errors (i.e., consistent results): GPT 4oMini achieved\nconsistency on 65.9% of cases (34.1% inconsistent), while GPT 4.1mini, O3Mini,\nand O4Mini each reached approximately 88.6% consistency (around 11.4%\nfailures). To measure annotation quality, we then compute exact-match and\nbase-type match accuracies over all 6000 snippets: GPT 4.1mini and O3Mini\nperform the best, achieving up to 70.5% exact match and 79.1% base type\naccuracy, requiring under one repair iteration on average. Our results\ndemonstrate that general-purpose and reasoning optimized LLMs, without any task\nspecific fine tuning or additional training can be effective in generating\nconsistent type annotations.They perform competitively with traditional deep\nlearning techniques which require large labeled dataset for training. While our\nwork focuses on Python, the pipeline can be extended to other optionally typed\nimperative languages like Ruby"}
{"id": "2508.00482", "pdf": "https://arxiv.org/pdf/2508.00482", "abs": "https://arxiv.org/abs/2508.00482", "authors": ["Erdem Yildirim", "Albert Schimpf", "Stefan Wehr", "Annette Bieniusa"], "title": "Semantic Subtyping for Maps in Erlang", "categories": ["cs.PL"], "comment": null, "summary": "In this paper we will construct a set-theoretic model of types featuring type\nvariables, base types, set-theoretic types and map types. Syntax of map types\nspans all the map types available in Erlang. The model of types is used to\ndefine a semantic subtyping relation based on set containment. The novelty of\nthis work is the definition of subtyping over parameteric map types."}
{"id": "2508.00534", "pdf": "https://arxiv.org/pdf/2508.00534", "abs": "https://arxiv.org/abs/2508.00534", "authors": ["Mikel Vandeloise"], "title": "Towards a unified framework for programming paradigms: A systematic review of classification formalisms and methodological foundations", "categories": ["cs.PL", "cs.CL", "D.3.2; F.3.2; D.3.1"], "comment": "Preprint submitted to the Journal of Object Technology on July 29,\n  2025. Data available upon request until peer-review is completed", "summary": "The rise of multi-paradigm languages challenges traditional classification\nmethods, leading to practical software engineering issues like interoperability\ndefects. This systematic literature review (SLR) maps the formal foundations of\nprogramming paradigms. Our objective is twofold: (1) to assess the state of the\nart of classification formalisms and their limitations, and (2) to identify the\nconceptual primitives and mathematical frameworks for a more powerful,\nreconstructive approach.\n  Based on a synthesis of 74 primary studies, we find that existing taxonomies\nlack conceptual granularity, a unified formal basis, and struggle with hybrid\nlanguages. In response, our analysis reveals a strong convergence toward a\ncompositional reconstruction of paradigms. This approach identifies a minimal\nset of orthogonal, atomic primitives and leverages mathematical frameworks,\npredominantly Type theory, Category theory and Unifying Theories of Programming\n(UTP), to formally guarantee their compositional properties.\n  We conclude that the literature reflects a significant intellectual shift\naway from classification towards these promising formal, reconstructive\nframeworks. This review provides a map of this evolution and proposes a\nresearch agenda for their unification."}
{"id": "2508.00244", "pdf": "https://arxiv.org/pdf/2508.00244", "abs": "https://arxiv.org/abs/2508.00244", "authors": ["Briza Mel Dias de Sousa", "Renato Cordeiro Ferreira", "Alfredo Goldman"], "title": "Functional vs. Object-Oriented: Comparing How Programming Paradigms Affect the Architectural Characteristics of Systems", "categories": ["cs.SE", "cs.PL", "D.3.2; D.2.11; D.2.13"], "comment": "11 pages, 16 figures (1 table, 3 diagrams, 5 graphics, 7 listings),\n  submitted to CTICQS capstone project competition at SBQS 2025", "summary": "After decades of dominance by object-oriented programming (OOP), functional\nprogramming (FP) is gaining increasing attention in the software industry. This\nstudy compares the impact of OOP and FP on the architectural characteristics of\nsoftware systems. For that, it examines the design and implementation of a\nDigital Wallet system, developed in Kotlin (representing OOP) and Scala\n(representing FP). The comparison is made through both qualitative and\nquantitative analyses to explore how each paradigm influences the system's\narchitectural characteristics. The self-ethnographic qualitative analysis\nprovides a side-by-side comparison of both implementations, revealing the\nperspective of those writing such code. The survey-based quantitative analysis\ngathers feedback from developers with diverse backgrounds, showing their\nimpressions of those reading this code. Hopefully, these results may be useful\nfor developers or organizations seeking to make more informed decisions about\nwhich paradigm is best suited for their next project."}
{"id": "2508.00419", "pdf": "https://arxiv.org/pdf/2508.00419", "abs": "https://arxiv.org/abs/2508.00419", "authors": ["Varun Bharti", "Shashwat Jha", "Dhruv Kumar", "Pankaj Jalote"], "title": "Loop Invariant Generation: A Hybrid Framework of Reasoning optimised LLMs and SMT Solvers", "categories": ["cs.LO", "cs.LG", "cs.PL"], "comment": "Under Review", "summary": "Loop invariants are essential for proving the correctness of programs with\nloops. Developing loop invariants is challenging, and fully automatic synthesis\ncannot be guaranteed for arbitrary programs. Some approaches have been proposed\nto synthesize loop invariants using symbolic techniques and more recently using\nneural approaches. These approaches are able to correctly synthesize loop\ninvariants only for subsets of standard benchmarks. In this work, we\ninvestigate whether modern, reasoning-optimized large language models can do\nbetter. We integrate OpenAI's O1, O1-mini, and O3-mini into a tightly coupled\ngenerate-and-check pipeline with the Z3 SMT solver, using solver\ncounterexamples to iteratively guide invariant refinement. We use Code2Inv\nbenchmark, which provides C programs along with their formal preconditions and\npostconditions. On this benchmark of 133 tasks, our framework achieves 100%\ncoverage (133 out of 133), outperforming the previous best of 107 out of 133,\nwhile requiring only 1-2 model proposals per instance and 14-55 seconds of\nwall-clock time. These results demonstrate that LLMs possess latent logical\nreasoning capabilities which can help automate loop invariant synthesis. While\nour experiments target C-specific programs, this approach should be\ngeneralizable to other imperative languages."}
{"id": "2508.00508", "pdf": "https://arxiv.org/pdf/2508.00508", "abs": "https://arxiv.org/abs/2508.00508", "authors": ["Panagiotis Diamantakis", "Thanassis Avgerinos", "Yannis Smaragdakis"], "title": "Desyan: A Platform for Seamless Value-Flow and Symbolic Analysis", "categories": ["cs.SE", "cs.PL"], "comment": null, "summary": "Over the past two decades, two different types of static analyses have\nemerged as dominant paradigms both in academia and industry: value-flow\nanalysis (e.g., data-flow analysis or points-to analysis) and symbolic analysis\n(e.g., symbolic execution). Despite their individual successes in numerous\napplication fields, the two approaches have remained largely separate; an\nartifact of the simple reality that there is no broadly adopted unifying\nplatform for effortless and efficient integration of symbolic techniques with\nhigh-performance data-flow reasoning.\n  To bridge this gap, we introduce Desyan: a platform for writing program\nanalyses with seamless integration of value-flow and symbolic reasoning. Desyan\nexpands a production-ready Datalog fixpoint engine (Souffl\\'e) with\nfull-fledged SMT solving invoking industry-leading SMT engines. Desyan provides\nconstructs for automatically (and efficiently!) handling typical patterns that\ncome up in program analysis. At the same time, the integration is agnostic with\nrespect to the solving technology, and supports Datalog-native symbolic\nreasoning, via a bottom-up algebraic reasoning module.\n  The result is an engine that allows blending different kinds of reasoning, as\nneeded for the underlying analysis. For value-flow analysis, the engine is the\nbest-in-class Datalog evaluator (often by a factor of over 20x in execution\ntime); for applications that require full SMT (e.g., a concolic execution\nengine or other symbolic evaluator that needs to solve arbitrarily complex\nconditions), the engine is leveraging the leading SMT solvers; for lightweight\nsymbolic evaluation (e.g., solving simple conditionals in the context of a\npath-sensitive analysis), the engine can use Datalog-native symbolic reasoning,\nachieving large speedups (often of over 2x) compared to eagerly appealing to an\nSMT solver."}
{"id": "2508.00772", "pdf": "https://arxiv.org/pdf/2508.00772", "abs": "https://arxiv.org/abs/2508.00772", "authors": ["Md Imranur Rahman Akib", "Fathima Binthe Muhammed", "Umit Saha", "Md Fazlul Karim Patwary", "Mehrin Anannya", "Md Alomgeer Hussein", "Md Biplob Hosen"], "title": "From Code to Career: Assessing Competitive Programmers for Industry Placement", "categories": ["cs.SE", "cs.PL"], "comment": null, "summary": "In today's fast-paced tech industry, there is a growing need for tools that\nevaluate a programmer's job readiness based on their coding performance. This\nstudy focuses on predicting the potential of Codeforces users to secure various\nlevels of software engineering jobs. The primary objective is to analyze how a\nuser's competitive programming activity correlates with their chances of\nobtaining positions, ranging from entry-level roles to jobs at major tech\ncompanies. We collect user data using the Codeforces API, process key\nperformance metrics, and build a prediction model using a Random Forest\nclassifier. The model categorizes users into four levels of employability,\nranging from those needing further development to those ready for top-tier tech\njobs. The system is implemented using Flask and deployed on Render for\nreal-time predictions. Our evaluation demonstrates that the approach\neffectively distinguishes between different skill levels based on coding\nproficiency and participation. This work lays a foundation for the use of\nmachine learning in career assessment and could be extended to predict job\nreadiness in broader technical fields."}
