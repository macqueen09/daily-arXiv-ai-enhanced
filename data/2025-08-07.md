<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 4]
- [cs.DB](#cs.DB) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [If-T: A Benchmark for Type Narrowing](https://arxiv.org/abs/2508.03830)
*Hanwen Guo,Ben Greenman*

Main category: cs.PL

TL;DR: If-T是一个语言无关的设计基准，用于评估类型缩窄系统的能力，通过简单程序揭示其优缺点，帮助研究者和语言设计者在精度、性能和复杂性之间找到平衡。


<details>
  <summary>Details</summary>
Motivation: 动态类型程序的设计缺乏统一标准，类型缩窄系统的复杂性与其实际收益不明确，需要一种基准来评估和比较不同实现。

Method: 基于文献、语言文档和实验，If-T定义了核心技术维度，并为每个维度提供正反例程序，用于测试类型缩窄系统的能力。

Result: If-T已应用于五种类型检查器（如TypeScript、Flow等），揭示了它们在逻辑推理和用户定义缩窄谓词等方面的差异。

Conclusion: If-T为类型缩窄系统提供了评估标准，帮助未来设计在精度、性能和复杂性之间取得更好平衡。

Abstract: **Context:** The design of static type systems that can validate
dynamically-typed programs (**gradually**) is an ongoing challenge. A key
difficulty is that dynamic code rarely follows datatype-driven design. Programs
instead use runtime tests to narrow down the proper usage of incoming data.
Type systems for dynamic languages thus need a **type narrowing** mechanism
that refines the type environment along individual control paths based on
dominating tests, a form of flow-sensitive typing. In order to express
refinements, the type system must have some notion of sets and subsets. Since
set-theoretic types are computationally and ergonomically complex, the need for
type narrowing raises design questions about how to balance precision and
performance. **Inquiry:** To date, the design of type narrowing systems has
been driven by intuition, past experience, and examples from users in various
language communities. There is no standard that captures desirable and
undesirable behaviors. Prior formalizations of narrowing are also significantly
more complex than a standard type system, and it is unclear how the extra
complexity pays off in terms of concrete examples. This paper addresses the
problems through If-T, a language-agnostic **design benchmark** for type
narrowing that characterizes the abilities of implementations using simple
programs that draw attention to fundamental questions. Unlike a traditional
performance-focused benchmark, If-T measures a narrowing system's ability to
validate correct code and reject incorrect code. Unlike a test suite, systems
are not required to fully conform to If-T. Deviations are acceptable provided
they are justified by well-reasoned design considerations, such as compile-time
performance. **Approach:** If-T is guided by the literature on type narrowing,
the documentation of gradual languages such as TypeScript, and experiments with
typechecker implementations. We have identified a set of core technical
dimensions for type narrowing. For each dimension, the benchmark contains a set
of topics and (at least) two characterizing programs per topic: one that should
typecheck and one that should not typecheck. **Knowledge:** If-T provides a
baseline to measure type narrowing systems. For researchers, it provides
criteria to categorize future designs via its collection of positive and
negative examples. For language designers, the benchmark demonstrates the
payoff of typechecker complexity in terms of concrete examples. Designers can
use the examples to decide whether supporting a particular example is
worthwhile. Both the benchmark and its implementations are freely available
online. **Grounding:** We have implemented the benchmark for five typecheckers:
TypeScript, Flow, Typed Racket, mypy, and Pyright. The results highlight
important differences, such as the ability to track logical implications among
program variables and typechecking for user-defined narrowing predicates.
**Importance:** Type narrowing is essential for gradual type systems, but the
tradeoffs between systems with different complexity have been unclear. If-T
clarifies these tradeoffs by illustrating the benefits and limitations of each
level of complexity. With If-T as a way to assess implementations in a fair,
cross-language manner, future type system designs can strive for a better
balance among precision, annotation burden, and performance.

</details>


### [2] [A Type System for Data Privacy Compliance in Active Object Languages](https://arxiv.org/abs/2508.03831)
*Chinmayi Prabhu Baramashetru,Paola Giannini,Silvia Lizeth Tapia Tarifa,Olaf Owe*

Main category: cs.PL

TL;DR: 本文提出了一种基于语言的隐私集成方法，结合静态和运行时技术，通过类型检查和推断确保GDPR合规性。


<details>
  <summary>Details</summary>
Motivation: GDPR等数据保护法规要求系统设计嵌入隐私保护，但将抽象原则转化为具体方法仍具挑战性。

Method: 采用基于类型检查和推断的主动对象语言框架，跟踪授权数据流并自动生成运行时约束。

Result: 通过类型系统整合合规性检查和用户同意变更，验证了方法的可行性，并满足GDPR常见要求。

Conclusion: 该工作为隐私感知系统设计提供了系统化、自动化的GDPR合规集成方法，适用于医疗和金融等领域。

Abstract: Data protection laws such as GDPR aim to give users unprecedented control
over their personal data. Compliance with these regulations requires
systematically considering information flow and interactions among entities
handling sensitive data. Privacy-by-design principles advocate embedding data
protection into system architectures as a default. However, translating these
abstract principles into concrete, explicit methods remains a significant
challenge. This paper addresses this gap by proposing a language-based approach
to privacy integration, combining static and runtime techniques. By employing
type checking and type inference in an active object language, the framework
enables the tracking of authorised data flows and the automatic generation of
constraints checked at runtime based on user consent. This ensures that
personal data is processed in compliance with GDPR constraints. The key
contribution of this work is a type system that gather the compliance checks
and the changes to users consent and integrates data privacy compliance
verification into system execution. The paper demonstrates the feasibility of
this approach through a soundness proof and several examples, illustrating how
the proposed language addresses common GDPR requirements, such as user consent,
purpose limitation, and data subject rights. This work advances the state of
the art in privacy-aware system design by offering a systematic and automated
method for integrating GDPR compliance into programming languages. This
capability has implications for building trustworthy systems in domains such as
healthcare or finance, where data privacy is crucial.

</details>


### [3] [Generating Inputs for Grammar Mining using Dynamic Symbolic Execution](https://arxiv.org/abs/2508.03832)
*Andreas Pointner,Josef Pichler,Herbert Prähofer*

Main category: cs.PL

TL;DR: 论文提出了一种基于动态符号执行（DSE）的自动化输入生成方法，用于改进语法挖掘的完整性和准确性，解决了现有方法因输入数据不足而遗漏边缘案例的问题。


<details>
  <summary>Details</summary>
Motivation: 软件组件在演化过程中可能偏离原始规范，导致语法挖掘无法覆盖完整输入语言。现有方法依赖操作数据，难以捕捉边缘案例或不再使用的功能。

Method: 结合动态符号执行（DSE），提出了一种三阶段输入生成方法，逐步扩展输入并分离解析函数。基于语法挖掘工具Mimid实现。

Result: 在11个基准应用中验证，新方法提取的语法精度和召回率接近现有工具，且能发现传统工具遗漏的细微特征和边缘案例。

Conclusion: 该方法为语法挖掘提供了自动化、可扩展且精确的解决方案，显著减少人工输入生成需求，提升语法完整性和鲁棒性。

Abstract: A vast number of software systems include components that parse and process
structured input. In addition to programming languages, which are analyzed by
compilers or interpreters, there are numerous components that process
standardized or proprietary data formats of varying complexity. Even if such
components were initially developed and tested based on a specification, such
as a grammar, numerous modifications and adaptations over the course of
software evolution can make it impossible to precisely determine which inputs
they actually accept. In this situation, grammar mining can be used to
reconstruct the specification in the form of a grammar. Established approaches
already produce useful results, provided that sufficient input data is
available to fully cover the input language. However, achieving this
completeness is a major challenge. In practice, only input data recorded during
the operation of the software systems is available. If this data is used for
grammar mining, the resulting grammar reflects only the actual processed inputs
but not the complete grammar of the input language accepted by the software
component. As a result, edge cases or previously supported features that no
longer appear in the available input data are missing from the generated
grammar. This work addresses this challenge by introducing a novel approach for
the automatic generation of inputs for grammar mining. Although input
generators have already been used for fuzz testing, it remains unclear whether
they are also suitable for grammar miners. Building on the grammar miner Mimid,
this work presents a fully automated approach to input generation. The approach
leverages Dynamic Symbolic Execution (DSE) and extends it with two mechanisms
to overcome the limitations of DSE regarding structured input parsers. First,
the search for new inputs is guided by an iterative expansion that starts with
a single-character input and gradually extends it. Second, input generation is
structured into a novel three-phase approach, which separates the generation of
inputs for parser functions. The proposed method was evaluated against a
diverse set of eleven benchmark applications from the existing literature.
Results demonstrate that the approach achieves precision and recall for
extracted grammars close to those derived from state-of-the-art grammar miners
such as Mimid. Notably, it successfully uncovers subtle features and edge cases
in parsers that are typically missed by such grammar miners. The effectiveness
of the method is supported by empirical evidence, showing that it can achieve
high performance in various domains without requiring prior input samples. This
contribution is significant for researchers and practitioners in software
engineering, offering an automated, scalable, and precise solution for grammar
mining. By eliminating the need for manual input generation, the approach not
only reduces workload but also enhances the robustness and comprehensiveness of
the extracted grammars. Following this approach, software engineers can
reconstruct specification from existing (legacy) parsers.

</details>


### [4] [Weak Memory Model Formalisms: Introduction and Survey](https://arxiv.org/abs/2508.04115)
*Roger C. Su,Robert J. Colvin*

Main category: cs.PL

TL;DR: 本文综述了弱内存模型的规范化研究，包括其定义、执行影响及推理工具，并介绍了两种常见的形式化表示方法。


<details>
  <summary>Details</summary>
Motivation: 并发编程中弱内存效应增加了开发复杂性，需要严格的规范来支持安全和关键软件开发。

Method: 通过操作语义和公理语义两种形式化方法，结合简化版Intel x86示例，分析弱内存模型。

Result: 综述了硬件特性、历史发展、理论成果及未来研究方向。

Conclusion: 弱内存模型的规范化研究对开发安全和关键软件至关重要，未来需进一步探索。

Abstract: Memory consistency models define the order in which accesses to shared memory
in a concurrent system may be observed to occur. Such models are a necessity
since program order is not a reliable indicator of execution order, due to
microarchitectural features or compiler transformations. Concurrent
programming, already a challenging task, is thus made even harder when weak
memory effects must be addressed. A rigorous specification of weak memory
models is therefore essential to make this problem tractable for developers of
safety- and security-critical, low-level software.
  In this paper we survey the field of formalisations of weak memory models,
including their specification, their effects on execution, and tools and
inference systems for reasoning about code. To assist the discussion we also
provide an introduction to two styles of formal representation found commonly
in the literature (using a much simplified version of Intel's x86 as the
example): a step-by-step construction of traces of the system (operational
semantics); and with respect to relations between memory events (axiomatic
semantics). The survey covers some long-standing hardware features that lead to
observable weak behaviours, a description of historical developments in
practice and in theory, an overview of computability and complexity results,
and outlines current and future directions in the field.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [5] [Raqlet: Cross-Paradigm Compilation for Recursive Queries](https://arxiv.org/abs/2508.03978)
*Amir Shaikhha,Youning Xia,Meisam Tarabkhah,Jazal Saleem,Anna Herlihy*

Main category: cs.DB

TL;DR: Raqlet是一个源到源的编译框架，用于解决递归查询引擎在关系型、图型和演绎型系统中的碎片化问题。


<details>
  <summary>Details</summary>
Motivation: 由于不同系统对递归查询的支持不一致，Raqlet旨在通过统一的中间表示（IRs）实现跨范式的递归查询翻译。

Method: Raqlet通过将Cypher或SQL/PGQ转换为PGIR，再转为DLIR，最后转为SQIR，实现查询的跨范式翻译。

Result: Raqlet提供了一个共享的语义基础，可作为语言标准的参考实现，并支持静态分析和性能优化。

Conclusion: Raqlet的目标是成为一个支持跨范式原型设计、便携式递归查询和形式化推理的健壮平台。

Abstract: We introduce Raqlet, a source-to-source compilation framework that addresses
the fragmentation of recursive querying engines spanning relational (recursive
SQL), graph (Cypher, GQL), and deductive (Datalog) systems. Recent standards
such as SQL:2023's SQL/PGQ and the GQL standard provide a common foundation for
querying graph data within relational and graph databases; however, real-world
support remains inconsistent across systems. Raqlet bridges this gap by
translating recursive queries across paradigms through leveraging intermediate
representations (IRs) grounded in well-defined semantics; it translates Cypher
or SQL/PGQ to PGIR (inspired by Cypher), then into DLIR (inspired by Datalog),
and finally to SQIR (inspired by recursive SQL). Raqlet provides a shared
semantic basis that can serve as a golden reference implementation for language
standards, while supporting static analysis and transformations (e.g.,
magic-set transformation) for performance tuning. Our vision is to make Raqlet
a robust platform that enables rapid cross-paradigm prototyping, portable
recursive queries, and formal reasoning about recursion even when targeting
diverse query execution engines.

</details>
