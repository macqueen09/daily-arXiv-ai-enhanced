{"id": "2508.04829", "pdf": "https://arxiv.org/pdf/2508.04829", "abs": "https://arxiv.org/abs/2508.04829", "authors": ["Devora Chait-Roth", "Kedar S. Namjoshi", "Thomas Wies"], "title": "Consistent Updates for Scalable Microservices", "categories": ["cs.PL"], "comment": null, "summary": "Online services are commonly implemented with a scalable microservice\narchitecture, where isomorphic worker processes service client requests,\nrecording persistent state in a backend data store. To maintain service, any\nmodifications to the service functionality must be made on the fly -- i.e., as\nthe service continues to process client requests -- but doing so is\nchallenging. The central difficulty is that of avoiding potential\ninconsistencies caused by ''mixed mode'' operation, where workers of current\nand new versions are concurrently active and interact via the data store. Some\nupdate methods avoid mixed mode altogether, but only at the cost of substantial\ninefficiency -- by doubling resources (memory and compute), or by halving\nthroughput. The alternative is a so-called ''rolling'' update, which is\nuncontrolled and runs the risk of serious service failures arising from\ninconsistent mixed-mode behavior.\n  In this paper, we present the first algorithms that guarantee consistency for\nmixed mode updates. The algorithms rely on semantic properties of service\nactions, such as commutativity. We show that semantic awareness is required, by\nproving that any semantically oblivious, mixed-mode update method cannot avoid\ninconsistencies. Ideally, it should appear to every client that a service\nupdate takes effect atomically; this ensures that a client is not exposed to\ninconsistent mixed-mode behavior. We introduce a framework that formalizes this\nintuition and develop foundational theory for reasoning about the consistency\nof mixed-mode updates, applying that theory to derive the new algorithms and\nestablish their correctness.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4fdd\u8bc1\u6df7\u5408\u6a21\u5f0f\u66f4\u65b0\u4e00\u81f4\u6027\u7684\u7b97\u6cd5\uff0c\u57fa\u4e8e\u670d\u52a1\u64cd\u4f5c\u7684\u8bed\u4e49\u5c5e\u6027\uff08\u5982\u4ea4\u6362\u6027\uff09\uff0c\u5e76\u8bc1\u660e\u8bed\u4e49\u611f\u77e5\u662f\u907f\u514d\u4e0d\u4e00\u81f4\u7684\u5fc5\u8981\u6761\u4ef6\u3002", "motivation": "\u5728\u7ebf\u670d\u52a1\u901a\u5e38\u91c7\u7528\u5fae\u670d\u52a1\u67b6\u6784\uff0c\u4f46\u52a8\u6001\u66f4\u65b0\u529f\u80fd\u65f6\u53ef\u80fd\u56e0\u6df7\u5408\u6a21\u5f0f\u64cd\u4f5c\u5bfc\u81f4\u4e0d\u4e00\u81f4\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u6548\u7387\u4f4e\u4e0b\uff0c\u8981\u4e48\u5b58\u5728\u4e25\u91cd\u98ce\u9669\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u670d\u52a1\u52a8\u4f5c\u8bed\u4e49\u5c5e\u6027\u7684\u7b97\u6cd5\uff0c\u5e76\u5f00\u53d1\u7406\u8bba\u6846\u67b6\uff0c\u7528\u4e8e\u63a8\u7406\u6df7\u5408\u6a21\u5f0f\u66f4\u65b0\u7684\u4e00\u81f4\u6027\u3002", "result": "\u8bc1\u660e\u4e86\u8bed\u4e49\u611f\u77e5\u662f\u907f\u514d\u4e0d\u4e00\u81f4\u7684\u5fc5\u8981\u6761\u4ef6\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u7b97\u6cd5\uff0c\u786e\u4fdd\u66f4\u65b0\u5bf9\u5ba2\u6237\u7aef\u8868\u73b0\u4e3a\u539f\u5b50\u64cd\u4f5c\u3002", "conclusion": "\u8bba\u6587\u4e3a\u6df7\u5408\u6a21\u5f0f\u66f4\u65b0\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u548c\u5b9e\u7528\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u670d\u52a1\u52a8\u6001\u66f4\u65b0\u4e2d\u7684\u4e00\u81f4\u6027\u95ee\u9898\u3002"}}
{"id": "2508.04865", "pdf": "https://arxiv.org/pdf/2508.04865", "abs": "https://arxiv.org/abs/2508.04865", "authors": ["Aleksander Boruch-Gruszecki", "Yangtian Zi", "Zixuan Wu", "Tejas Oberoi", "Carolyn Jane Anderson", "Joydeep Biswas", "Arjun Guha"], "title": "Agnostics: Learning to Code in Any Programming Language via Reinforcement with a Universal Learning Environment", "categories": ["cs.LG", "cs.PL"], "comment": "18 pages, 19 figures. For artifacts, see https://agnostics.abgru.me", "summary": "Large language models (LLMs) already excel at writing code in high-resource\nlanguages such as Python and JavaScript, yet stumble on low-resource languages\nthat remain essential to science and engineering. Besides the obvious shortage\nof pre-training data, post-training itself is a bottleneck: every new language\nseems to require new datasets, test harnesses, and reinforcement-learning (RL)\ninfrastructure.\n  We introduce Agnostics, a language-agnostic post-training pipeline that\neliminates this per-language engineering. The key idea is to judge code solely\nby its externally observable behavior, so a single verifier can test solutions\nwritten in any language. Concretely, we (i) use an LLM to rewrite existing\nunit-test datasets into an I/O format, (ii) supply a short configuration that\ntells the verifier how to compile and run a target language, and (iii) apply\nreinforcement learning with verifiable rewards (RLVR) in a robust code\nexecution environment.\n  Applied to five low-resource languages--Lua, Julia, R, OCaml, and\nFortran--Agnostics (1) improves Qwen-3 4B to performance that rivals other\n16B-70B open-weight models; (2) scales cleanly to larger and diverse model\nfamilies (Qwen-3 8B, DeepSeek Coder 6.7B Instruct, Phi 4 Mini); and (3) for\n${\\le} 16$B parameter models, sets new state-of-the-art pass@1 results on\nMultiPL-E and a new multi-language version LiveCodeBench that we introduce.\n  We will release the language-agnostic training datasets (Ag-MBPP-X,\nAg-Codeforces-X, Ag-LiveCodeBench-X), training code, and ready-to-use\nconfigurations, making RL post-training in any programming language as simple\nas editing a short YAML file.", "AI": {"tldr": "Agnostics\u662f\u4e00\u79cd\u8bed\u8a00\u65e0\u5173\u7684\u540e\u8bad\u7ec3\u6d41\u7a0b\uff0c\u901a\u8fc7\u89c2\u5bdf\u4ee3\u7801\u7684\u5916\u90e8\u884c\u4e3a\u6765\u9a8c\u8bc1\u591a\u8bed\u8a00\u4ee3\u7801\uff0c\u907f\u514d\u4e86\u4e3a\u6bcf\u79cd\u8bed\u8a00\u5b9a\u5236\u5de5\u7a0b\u3002", "motivation": "\u89e3\u51b3\u4f4e\u8d44\u6e90\u7f16\u7a0b\u8bed\u8a00\u5728LLMs\u4e2d\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\uff0c\u51cf\u5c11\u4e3a\u6bcf\u79cd\u8bed\u8a00\u5b9a\u5236\u540e\u8bad\u7ec3\u7684\u5de5\u4f5c\u91cf\u3002", "method": "1. \u5c06\u73b0\u6709\u5355\u5143\u6d4b\u8bd5\u6570\u636e\u96c6\u91cd\u5199\u4e3aI/O\u683c\u5f0f\uff1b2. \u63d0\u4f9b\u7b80\u77ed\u7684\u914d\u7f6e\u6307\u5bfc\u9a8c\u8bc1\u5668\u7f16\u8bd1\u548c\u8fd0\u884c\u76ee\u6807\u8bed\u8a00\uff1b3. \u5728\u7a33\u5065\u7684\u4ee3\u7801\u6267\u884c\u73af\u5883\u4e2d\u5e94\u7528RLVR\uff08\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff09\u3002", "result": "\u5728\u4e94\u79cd\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08Lua\u3001Julia\u3001R\u3001OCaml\u3001Fortran\uff09\u4e0a\uff0cAgnostics\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u751a\u81f3\u8d85\u8d8a\u4e86\u4e00\u4e9b\u66f4\u5927\u7684\u5f00\u6e90\u6a21\u578b\uff0c\u5e76\u5728\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u521b\u4e0b\u65b0\u8bb0\u5f55\u3002", "conclusion": "Agnostics\u7b80\u5316\u4e86\u591a\u8bed\u8a00\u4ee3\u7801\u7684\u540e\u8bad\u7ec3\u6d41\u7a0b\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5f00\u6e90\u4e86\u76f8\u5173\u6570\u636e\u96c6\u548c\u5de5\u5177\u3002"}}
