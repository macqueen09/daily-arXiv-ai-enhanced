<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Consistent Updates for Scalable Microservices](https://arxiv.org/abs/2508.04829)
*Devora Chait-Roth,Kedar S. Namjoshi,Thomas Wies*

Main category: cs.PL

TL;DR: 论文提出了一种保证混合模式更新一致性的算法，基于服务操作的语义属性（如交换性），并证明语义感知是避免不一致的必要条件。


<details>
  <summary>Details</summary>
Motivation: 在线服务通常采用可扩展的微服务架构，但在动态更新时，新旧版本工作进程的混合操作可能导致不一致问题。现有方法要么效率低下，要么无法避免不一致。

Method: 提出基于服务操作语义属性的算法，通过形式化框架和理论推导，确保更新对客户端表现为原子性。

Result: 证明了语义感知是避免不一致的必要条件，并开发了新的算法，确保混合模式更新的正确性。

Conclusion: 通过语义感知和形式化理论，论文解决了混合模式更新的不一致问题，为动态服务更新提供了可靠方法。

Abstract: Online services are commonly implemented with a scalable microservice
architecture, where isomorphic worker processes service client requests,
recording persistent state in a backend data store. To maintain service, any
modifications to the service functionality must be made on the fly -- i.e., as
the service continues to process client requests -- but doing so is
challenging. The central difficulty is that of avoiding potential
inconsistencies caused by ''mixed mode'' operation, where workers of current
and new versions are concurrently active and interact via the data store. Some
update methods avoid mixed mode altogether, but only at the cost of substantial
inefficiency -- by doubling resources (memory and compute), or by halving
throughput. The alternative is a so-called ''rolling'' update, which is
uncontrolled and runs the risk of serious service failures arising from
inconsistent mixed-mode behavior.
  In this paper, we present the first algorithms that guarantee consistency for
mixed mode updates. The algorithms rely on semantic properties of service
actions, such as commutativity. We show that semantic awareness is required, by
proving that any semantically oblivious, mixed-mode update method cannot avoid
inconsistencies. Ideally, it should appear to every client that a service
update takes effect atomically; this ensures that a client is not exposed to
inconsistent mixed-mode behavior. We introduce a framework that formalizes this
intuition and develop foundational theory for reasoning about the consistency
of mixed-mode updates, applying that theory to derive the new algorithms and
establish their correctness.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [2] [Agnostics: Learning to Code in Any Programming Language via Reinforcement with a Universal Learning Environment](https://arxiv.org/abs/2508.04865)
*Aleksander Boruch-Gruszecki,Yangtian Zi,Zixuan Wu,Tejas Oberoi,Carolyn Jane Anderson,Joydeep Biswas,Arjun Guha*

Main category: cs.LG

TL;DR: Agnostics是一种语言无关的后训练管道，通过观察代码的外部行为来验证多语言代码，避免了为每种语言单独设计测试框架的麻烦。


<details>
  <summary>Details</summary>
Motivation: 解决低资源编程语言在代码生成任务中因缺乏训练数据和测试基础设施而表现不佳的问题。

Method: 使用LLM将现有单元测试数据集转换为I/O格式，提供简短的配置指导验证器如何编译和运行目标语言，并应用带可验证奖励的强化学习（RLVR）。

Result: 在五种低资源语言（Lua、Julia、R、OCaml、Fortran）上，Agnostics显著提升了模型性能，甚至媲美更大规模的模型，并在MultiPL-E和LiveCodeBench上创下新纪录。

Conclusion: Agnostics通过简化多语言代码的后训练流程，为低资源语言的代码生成任务提供了高效解决方案，并开源了相关数据集和工具。

Abstract: Large language models (LLMs) already excel at writing code in high-resource
languages such as Python and JavaScript, yet stumble on low-resource languages
that remain essential to science and engineering. Besides the obvious shortage
of pre-training data, post-training itself is a bottleneck: every new language
seems to require new datasets, test harnesses, and reinforcement-learning (RL)
infrastructure.
  We introduce Agnostics, a language-agnostic post-training pipeline that
eliminates this per-language engineering. The key idea is to judge code solely
by its externally observable behavior, so a single verifier can test solutions
written in any language. Concretely, we (i) use an LLM to rewrite existing
unit-test datasets into an I/O format, (ii) supply a short configuration that
tells the verifier how to compile and run a target language, and (iii) apply
reinforcement learning with verifiable rewards (RLVR) in a robust code
execution environment.
  Applied to five low-resource languages--Lua, Julia, R, OCaml, and
Fortran--Agnostics (1) improves Qwen-3 4B to performance that rivals other
16B-70B open-weight models; (2) scales cleanly to larger and diverse model
families (Qwen-3 8B, DeepSeek Coder 6.7B Instruct, Phi 4 Mini); and (3) for
${\le} 16$B parameter models, sets new state-of-the-art pass@1 results on
MultiPL-E and a new multi-language version LiveCodeBench that we introduce.
  We will release the language-agnostic training datasets (Ag-MBPP-X,
Ag-Codeforces-X, Ag-LiveCodeBench-X), training code, and ready-to-use
configurations, making RL post-training in any programming language as simple
as editing a short YAML file.

</details>
