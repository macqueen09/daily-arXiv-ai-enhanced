<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 8]
- [cs.LG](#cs.LG) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Homomorphism Calculus for User-Defined Aggregations](https://arxiv.org/abs/2508.15109)
*Ziteng Wang,Ruijie Fang,Linus Zheng,Dixin Tang,Isil Dillig*

Main category: cs.PL

TL;DR: 这篇论文提出了一种新的同态式微分法，用于验证和拒绝用户定义聚合函数是否满足同态性质，以支持高效的并行执行和增量计算。


<details>
  <summary>Details</summary>
Motivation: 虽然Spark和Flink等框架支持UDAF，但要实现高效执行需要函数满足同态性质。这个问题驱动了对同态性验证方法的研究。

Method: 论文提出了一种新的同态式微分法，这种计算方法可以验证或拒绝UDAF是否为数据框同态式。如果满足条件，还能构造相应的合并运算符。

Result: 实现了基于该计算法的算法，并在真实世界的UDAF上进行评估。结果显示，该方法在性能上显著超过了两个领先的综合器。

Conclusion: 这种新的同态式微分法为UDAF的同态性验证提供了有效的解决方案，能够支持更高效的并行执行和增量计算。

Abstract: Data processing frameworks like Apache Spark and Flink provide built-in
support for user-defined aggregation functions (UDAFs), enabling the
integration of domain-specific logic. However, for these frameworks to support
\emph{efficient} UDAF execution, the function needs to satisfy a
\emph{homomorphism property}, which ensures that partial results from
independent computations can be merged correctly. Motivated by this problem,
this paper introduces a novel \emph{homomorphism calculus} that can both verify
and refute whether a UDAF is a dataframe homomorphism. If so, our calculus also
enables the construction of a corresponding merge operator which can be used
for incremental computation and parallel execution. We have implemented an
algorithm based on our proposed calculus and evaluate it on real-world UDAFs,
demonstrating that our approach significantly outperforms two leading
synthesizers.

</details>


### [2] [Software Model Checking via Summary-Guided Search (Extended Version)](https://arxiv.org/abs/2508.15137)
*Ruijie Fang,Zachary Kincaid,Thomas Reps*

Main category: cs.PL

TL;DR: GPS是一种新的软件模型检测算法，结合了定向状态搜索和组合式静态分析，能够高效发现程序安全证明和反例，在性能上优于现有最先进的模型检测器。


<details>
  <summary>Details</summary>
Motivation: 现有的软件模型检测方法在处理具有长输入依赖错误路径的程序时效率不高，需要一种既能保证完备性又能保持高性能的新方法。

Method: GPS采用定向搜索程序状态的方法，通过组合式、基于摘要的静态分析来指导搜索。静态分析摘要用于剪除不可行路径并驱动测试生成以探索新状态。采用两层搜索策略和仪器化技术来实现反驳完备性。

Result: 在包括SV-COMP和先前文献的基准测试中，GPS在解决的基准数量和运行时间方面都优于最先进的软件模型检测器（包括SV-COMP ReachSafety-Loops类别的顶级性能工具）。

Conclusion: GPS算法通过结合静态分析和定向搜索，在保持高性能的同时实现了反驳完备性，为处理复杂程序错误路径提供了一种有效的模型检测解决方案。

Abstract: In this work, we describe a new software model-checking algorithm called GPS.
GPS treats the task of model checking a program as a directed search of the
program states, guided by a compositional, summary-based static analysis. The
summaries produced by static analysis are used both to prune away infeasible
paths and to drive test generation to reach new, unexplored program states. GPS
can find both proofs of safety and counter-examples to safety (i.e., inputs
that trigger bugs), and features a novel two-layered search strategy that
renders it particularly efficient at finding bugs in programs featuring long,
input-dependent error paths. To make GPS refutationally complete (in the sense
that it will find an error if one exists, if it is allotted enough time), we
introduce an instrumentation technique and show that it helps GPS achieve
refutation-completeness without sacrificing overall performance. We benchmarked
GPS on a suite of benchmarks including both programs from the Software
Verification Competition (SV-COMP) and from prior literature, and found that
our implementation of GPS outperforms state-of-the-art software model checkers
(including the top performers in SV-COMP ReachSafety-Loops category), both in
terms of the number of benchmarks solved and in terms of running time.

</details>


### [3] [Big-Stop Semantics: A Simple Way to Get the Benefits of Small-Step Semantics in a Big-Step Judgment](https://arxiv.org/abs/2508.15157)
*David M Kahn,Jan Hoffmann,Runming Li*

Main category: cs.PL

TL;DR: 提出了一种称为big-stop语义的扩展方法，通过引入归纳定义来捕获发散计算，解决了传统big-step语义无法处理程序发散的问题，同时保持了big-step语义的简洁性和易用性。


<details>
  <summary>Details</summary>
Motivation: 传统big-step语义虽然简洁易用，但无法描述程序发散行为，而small-step语义虽然功能强大但规则复杂。需要找到一种既能保持big-step语义的简洁性，又能处理发散计算的解决方案。

Method: 扩展标准big-step推理规则，添加少量额外规则来定义评估判断，该判断等价于small-step转换的自反传递闭包。使用归纳定义来捕获发散计算，无需引入错误状态。

Result: 提出的big-stop语义能够处理类型化、非类型化和带效果的PCF变体，以及基于while循环的命令式语言中的发散计算，与small-step语义在表达能力上等价。

Conclusion: big-stop语义提供了一种简洁且功能完备的语义描述方法，既保持了big-step语义的易用性，又获得了small-step语义的表达能力，避免了其他解决方案中复杂的推理规则和状态管理。

Abstract: As evident in the programming language literature, many practitioners favor
specifying dynamic program behavior using big-step over small-step semantics.
Unlike small-step semantics, which must dwell on every intermediate program
state, big-step semantics conveniently jump directly to the ever-important
result of the computation. Big-step semantics also typically involve fewer
inference rules than their small-step counterparts. However, in exchange for
ergonomics, big-step semantics give up power: Small-step semantics describes
program behaviors that are outside the grasp of big-step semantics, notably
divergence. This work presents a little-known extension of big-step semantics
with inductive definitions that captures diverging computations without
introducing error states. This big-stop semantics is illustrated for typed,
untyped, and effectful variants of PCF, as well as a while-loop-based
imperative language. Big-stop semantics extends the standard big-step inference
rules with a few additional rules to define an evaluation judgment that is
equivalent to the reflexive-transitive closure of small-step transitions. This
simple extension contrasts with other solutions in the literature which
sacrifice ergonomics by introducing many additional inference rules, global
state, and/or less-commonly-understood reasoning principles like coinduction.

</details>


### [4] [Probabilistic Inference for Datalog with Correlated Inputs](https://arxiv.org/abs/2508.15166)
*Jingbo Wang,Shashin Halalingaiah,Weiyi Chen,Chao Wang,Isil Dillig*

Main category: cs.PL

TL;DR: Praline是Datalog的新扩展，用于处理输入相关性下的精确概率推理，通过约束优化和δ-精确算法实现可扩展的紧概率边界计算


<details>
  <summary>Details</summary>
Motivation: 现有的概率逻辑编程语言（如ProbLog）无法处理输入事实之间的统计相关性，限制了概率推理的精确性

Method: 将推理任务建模为约束优化问题，提出δ-精确推理算法，结合约束求解、静态分析和迭代优化来提高可扩展性

Result: 在包括侧信道分析在内的真实基准测试中，该方法不仅具有良好可扩展性，还能提供紧致的概率边界

Conclusion: Praline成功解决了输入相关性下的概率推理问题，通过创新的优化方法和算法设计实现了精确且可扩展的推理

Abstract: Probabilistic extensions of logic programming languages, such as ProbLog,
integrate logical reasoning with probabilistic inference to evaluate
probabilities of output relations; however, prior work does not account for
potential statistical correlations among input facts. This paper introduces
Praline, a new extension to Datalog designed for precise probabilistic
inference in the presence of (partially known) input correlations. We formulate
the inference task as a constrained optimization problem, where the solution
yields sound and precise probability bounds for output facts. However, due to
the complexity of the resulting optimization problem, this approach alone often
does not scale to large programs. To address scalability, we propose a more
efficient $\delta$-exact inference algorithm that leverages constraint solving,
static analysis, and iterative refinement. Our empirical evaluation on
challenging real-world benchmarks, including side-channel analysis,
demonstrates that our method not only scales effectively but also delivers
tight probability bounds.

</details>


### [5] [Exploring the Theory and Practice of Concurrency in the Entity-Component-System Pattern](https://arxiv.org/abs/2508.15264)
*Patrick Redmond,Jonathan Castello,José Manuel Calderón Trilla,Lindsey Kuper*

Main category: cs.PL

TL;DR: 本文提出了Core ECS形式化模型，抽象ECS模式的本质，识别出确定性并发行为类别，并发现现有ECS框架未充分利用确定性并发机会。


<details>
  <summary>Details</summary>
Motivation: ECS模式在游戏开发中广泛使用但外界了解不足，现有解释过于具体或模糊，需要建立严谨的形式化模型来揭示其本质。

Method: 设计Core ECS形式化模型，抽象具体实现细节，识别确定性并发程序类别，并调研多个现实ECS框架进行比较分析。

Result: 发现了一类无论调度如何都能保持确定性的Core ECS程序，现有ECS框架均未充分利用这种确定性并发机会。

Conclusion: 研究指出了新的ECS实现技术空间，可以更好地利用确定性并发机会，为构建确定性并发编程模型提供基础。

Abstract: The Entity-Component-System (ECS) software design pattern, long used in game
development, encourages a clean separation of identity (entities), data
properties (components), and computational behaviors (systems). Programs
written using the ECS pattern are naturally concurrent, and the pattern offers
modularity, flexibility, and performance benefits that have led to a
proliferation of ECS frameworks. Nevertheless, the ECS pattern is little-known
and not well understood outside of a few domains. Existing explanations of the
ECS pattern tend to be mired in the concrete details of particular ECS
frameworks, or they explain the pattern in terms of imperfect metaphors or in
terms of what it is not. We seek a rigorous understanding of the ECS pattern
via the design of a formal model, Core ECS, that abstracts away the details of
specific implementations to reveal the essence of software using the ECS
pattern. We identify a class of Core ECS programs that behave deterministically
regardless of scheduling, enabling use of the ECS pattern as a
deterministic-by-construction concurrent programming model. With Core ECS as a
point of comparison, we then survey several real-world ECS frameworks and find
that they all leave opportunities for deterministic concurrency unexploited.
Our findings point out a space for new ECS implementation techniques that
better leverage such opportunities.

</details>


### [6] [Fair Termination for Resource-Aware Active Objects](https://arxiv.org/abs/2508.15333)
*Francesco Dagnino,Paola Giannini,Violet Ka I Pun,Ulises Torrella*

Main category: cs.PL

TL;DR: 开发了一个资源感知的主动对象核心演算和类型系统，确保良类型程序能够公平终止


<details>
  <summary>Details</summary>
Motivation: 主动对象系统是分布式计算模型，需要资源感知的形式化方法来处理并发和资源管理问题

Method: 结合分级语义和类型系统技术（来自顺序程序）与公平终止技术（来自同步会话）

Result: 提出了一个核心演算和类型系统，能够保证程序的公平终止性

Conclusion: 成功将顺序程序的分级语义技术与同步会话的公平终止技术相结合，为资源感知主动对象系统提供了形式化基础

Abstract: Active object systems are a model of distributed computation that has been
adopted for modelling distributed systems and business process workflows. This
field of modelling is, in essence, concurrent and resource-aware, motivating
the development of resource-aware formalisations on the active object model.
The contributions of this work are the development of a core calculus for
resource-aware active objects together with a type system ensuring that
well-typed programs are fairly terminating, i.e., they can always eventually
terminate. To achieve this, we combine techniques from graded semantics and
type systems, which are quite well understood for sequential programs, with
those for fair termination, which have been developed for synchronous~sessions.

</details>


### [7] [Compositional Symbolic Execution for the Next 700 Memory Models (Extended Version)](https://arxiv.org/abs/2508.15576)
*Andreas Lööw,Seung Hoon Park,Daniele Nantes-Sobrinho,Sacha-Élie Ayoun,Opale Sjöstedt,Philippa Gardner*

Main category: cs.PL

TL;DR: 本文为内存模型参数化的组合符号执行平台提供了新的形式化基础，支持分离逻辑和错误分离逻辑分析，并在Rocq定理证明器中实现和验证。


<details>
  <summary>Details</summary>
Motivation: 现有的组合符号执行工具虽然利用了分离逻辑和错误分离逻辑，但缺乏对内存模型参数化平台的形式化基础，限制了灵活性和适用范围。

Method: 基于Gillian平台的启发，在Rocq定理证明器中机械化形式化基础，支持多种内存模型（包括C和CHERI），涵盖SL和ISL分析。

Result: 成功建立了内存模型参数化的形式化框架，验证了其在多种内存模型上的适用性，并确保了与其他基于标准定义工具的互操作性。

Conclusion: 该工作为内存模型参数化的组合符号执行平台提供了坚实的形式化基础，增强了灵活性、适用范围和工具互操作性。

Abstract: Multiple successful compositional symbolic execution (CSE) tools and
platforms exploit separation logic (SL) for compositional verification and/or
incorrectness separation logic (ISL) for compositional bug-finding, including
VeriFast, Viper, Gillian, CN, and Infer-Pulse. Previous work on the Gillian
platform, the only CSE platform that is parametric on the memory model, meaning
that it can be instantiated to different memory models, suggests that the
ability to use custom memory models allows for more flexibility in supporting
analysis of a wide range of programming languages, for implementing custom
automation, and for improving performance. However, the literature lacks a
satisfactory formal foundation for memory-model-parametric CSE platforms.
  In this paper, inspired by Gillian, we provide a new formal foundation for
memory-model-parametric CSE platforms. Our foundation advances the state of the
art in four ways. First, we mechanise our foundation (in the interactive
theorem prover Rocq). Second, we validate our foundation by instantiating it to
a broad range of memory models, including models for C and CHERI. Third,
whereas previous memory-model-parametric work has only covered SL analyses, we
cover both SL and ISL analyses. Fourth, our foundation is based on standard
definitions of SL and ISL (including definitions of function specification
validity, to ensure sound interoperation with other tools and platforms also
based on standard definitions).

</details>


### [8] [Active Learning for Neurosymbolic Program Synthesis](https://arxiv.org/abs/2508.15750)
*Celeste Barnaby,Qiaochu Chen,Ramya Ramalingam,Osbert Bastani,Isil Dillig*

Main category: cs.PL

TL;DR: 本文提出了一种新的主动学习方法SmartLabel，通过约束保形评估(CCE)技术处理神经符号程序合成中的神经网络误预测问题，显著提高了合成正确程序的成功率


<details>
  <summary>Details</summary>
Motivation: 现有的主动学习方法在纯符号设置中有效，但在神经符号程序合成中由于神经网络组件的误预测会导致返回错误程序，需要新的技术来处理这一挑战

Method: 提出约束保形评估(CCE)策略，考虑神经网络误预测并整合用户反馈，通过迭代优化CCE精度直到所有剩余程序保证观测等价

Result: 在三个神经符号领域实验中，SmartLabel为98%的基准测试识别出真实程序，平均需要不到5轮用户交互，而先前技术最多只能为65%的基准测试收敛到真实程序

Conclusion: SmartLabel方法有效解决了神经符号程序合成中的主动学习挑战，显著优于现有技术，为神经符号程序合成的实际应用提供了可靠解决方案

Abstract: The goal of active learning for program synthesis is to synthesize the
desired program by asking targeted questions that minimize user interaction.
While prior work has explored active learning in the purely symbolic setting,
such techniques are inadequate for the increasingly popular paradigm of
neurosymbolic program synthesis, where the synthesized program incorporates
neural components. When applied to the neurosymbolic setting, such techniques
can -- and, in practice, do -- return an unintended program due to
mispredictions of neural components. This paper proposes a new active learning
technique that can handle the unique challenges posed by neural network
mispredictions. Our approach is based upon a new evaluation strategy called
constrained conformal evaluation (CCE), which accounts for neural
mispredictions while taking into account user-provided feedback. Our proposed
method iteratively makes CCE more precise until all remaining programs are
guaranteed to be observationally equivalent. We have implemented this method in
a tool called SmartLabel and experimentally evaluated it on three neurosymbolic
domains. Our results demonstrate that SmartLabel identifies the ground truth
program for 98% of the benchmarks, requiring under 5 rounds of user interaction
on average. In contrast, prior techniques for active learning are only able to
converge to the ground truth program for at most 65% of the benchmarks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [9] [Mini-Batch Robustness Verification of Deep Neural Networks](https://arxiv.org/abs/2508.15454)
*Saar Tzour-Shaday,Dana Drachsler Cohen*

Main category: cs.LG

TL;DR: 这篇论文提出了BaVerLy验证器，通过组合验证相似计算的输入球来提高神经网结构的本地稳健性验证效率，平均提速2.3倍


<details>
  <summary>Details</summary>
Motivation: 现有的本地稳健性验证器在分析大量输入时要么耗时过长，要么失去太多精度，需要更高效的验证方法

Method: 提出组合本地稳健性验证概念，通过动态构建和验证小批量来利用相似网结构计算的输入球，减少总体分析时间

Result: BaVerLy在MNIST和CIFAR-10数据集上对全连接和卷积网结构进行验证，平均提速2.3倍，最高达4.1倍，将总分析时间从24小时缩短到6小时

Conclusion: 组合验证方法能够显著提高神经网结构本地稳健性验证的效率，为安全关键应用提供了更强大的防御能力

Abstract: Neural network image classifiers are ubiquitous in many safety-critical
applications. However, they are susceptible to adversarial attacks. To
understand their robustness to attacks, many local robustness verifiers have
been proposed to analyze $\epsilon$-balls of inputs. Yet, existing verifiers
introduce a long analysis time or lose too much precision, making them less
effective for a large set of inputs. In this work, we propose a new approach to
local robustness: group local robustness verification. The key idea is to
leverage the similarity of the network computations of certain $\epsilon$-balls
to reduce the overall analysis time. We propose BaVerLy, a sound and complete
verifier that boosts the local robustness verification of a set of
$\epsilon$-balls by dynamically constructing and verifying mini-batches.
BaVerLy adaptively identifies successful mini-batch sizes, accordingly
constructs mini-batches of $\epsilon$-balls that have similar network
computations, and verifies them jointly. If a mini-batch is verified, all
$\epsilon$-balls are proven robust. Otherwise, one $\epsilon$-ball is suspected
as not being robust, guiding the refinement. In the latter case, BaVerLy
leverages the analysis results to expedite the analysis of that $\epsilon$-ball
as well as the other $\epsilon$-balls in the batch. We evaluate BaVerLy on
fully connected and convolutional networks for MNIST and CIFAR-10. Results show
that BaVerLy scales the common one by one verification by 2.3x on average and
up to 4.1x, in which case it reduces the total analysis time from 24 hours to 6
hours.

</details>
