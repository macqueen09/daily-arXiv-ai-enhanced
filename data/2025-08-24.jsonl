{"id": "2508.15109", "pdf": "https://arxiv.org/pdf/2508.15109", "abs": "https://arxiv.org/abs/2508.15109", "authors": ["Ziteng Wang", "Ruijie Fang", "Linus Zheng", "Dixin Tang", "Isil Dillig"], "title": "Homomorphism Calculus for User-Defined Aggregations", "categories": ["cs.PL", "D.3.0; F.3.1"], "comment": null, "summary": "Data processing frameworks like Apache Spark and Flink provide built-in\nsupport for user-defined aggregation functions (UDAFs), enabling the\nintegration of domain-specific logic. However, for these frameworks to support\n\\emph{efficient} UDAF execution, the function needs to satisfy a\n\\emph{homomorphism property}, which ensures that partial results from\nindependent computations can be merged correctly. Motivated by this problem,\nthis paper introduces a novel \\emph{homomorphism calculus} that can both verify\nand refute whether a UDAF is a dataframe homomorphism. If so, our calculus also\nenables the construction of a corresponding merge operator which can be used\nfor incremental computation and parallel execution. We have implemented an\nalgorithm based on our proposed calculus and evaluate it on real-world UDAFs,\ndemonstrating that our approach significantly outperforms two leading\nsynthesizers."}
{"id": "2508.15137", "pdf": "https://arxiv.org/pdf/2508.15137", "abs": "https://arxiv.org/abs/2508.15137", "authors": ["Ruijie Fang", "Zachary Kincaid", "Thomas Reps"], "title": "Software Model Checking via Summary-Guided Search (Extended Version)", "categories": ["cs.PL", "cs.SE"], "comment": "Preliminary manuscript of extended version of paper that will appear\n  in OOPSLA 2025. 36 pages", "summary": "In this work, we describe a new software model-checking algorithm called GPS.\nGPS treats the task of model checking a program as a directed search of the\nprogram states, guided by a compositional, summary-based static analysis. The\nsummaries produced by static analysis are used both to prune away infeasible\npaths and to drive test generation to reach new, unexplored program states. GPS\ncan find both proofs of safety and counter-examples to safety (i.e., inputs\nthat trigger bugs), and features a novel two-layered search strategy that\nrenders it particularly efficient at finding bugs in programs featuring long,\ninput-dependent error paths. To make GPS refutationally complete (in the sense\nthat it will find an error if one exists, if it is allotted enough time), we\nintroduce an instrumentation technique and show that it helps GPS achieve\nrefutation-completeness without sacrificing overall performance. We benchmarked\nGPS on a suite of benchmarks including both programs from the Software\nVerification Competition (SV-COMP) and from prior literature, and found that\nour implementation of GPS outperforms state-of-the-art software model checkers\n(including the top performers in SV-COMP ReachSafety-Loops category), both in\nterms of the number of benchmarks solved and in terms of running time."}
{"id": "2508.15157", "pdf": "https://arxiv.org/pdf/2508.15157", "abs": "https://arxiv.org/abs/2508.15157", "authors": ["David M Kahn", "Jan Hoffmann", "Runming Li"], "title": "Big-Stop Semantics: A Simple Way to Get the Benefits of Small-Step Semantics in a Big-Step Judgment", "categories": ["cs.PL"], "comment": "26 pages, 27 figures", "summary": "As evident in the programming language literature, many practitioners favor\nspecifying dynamic program behavior using big-step over small-step semantics.\nUnlike small-step semantics, which must dwell on every intermediate program\nstate, big-step semantics conveniently jump directly to the ever-important\nresult of the computation. Big-step semantics also typically involve fewer\ninference rules than their small-step counterparts. However, in exchange for\nergonomics, big-step semantics give up power: Small-step semantics describes\nprogram behaviors that are outside the grasp of big-step semantics, notably\ndivergence. This work presents a little-known extension of big-step semantics\nwith inductive definitions that captures diverging computations without\nintroducing error states. This big-stop semantics is illustrated for typed,\nuntyped, and effectful variants of PCF, as well as a while-loop-based\nimperative language. Big-stop semantics extends the standard big-step inference\nrules with a few additional rules to define an evaluation judgment that is\nequivalent to the reflexive-transitive closure of small-step transitions. This\nsimple extension contrasts with other solutions in the literature which\nsacrifice ergonomics by introducing many additional inference rules, global\nstate, and/or less-commonly-understood reasoning principles like coinduction."}
{"id": "2508.15166", "pdf": "https://arxiv.org/pdf/2508.15166", "abs": "https://arxiv.org/abs/2508.15166", "authors": ["Jingbo Wang", "Shashin Halalingaiah", "Weiyi Chen", "Chao Wang", "Isil Dillig"], "title": "Probabilistic Inference for Datalog with Correlated Inputs", "categories": ["cs.PL"], "comment": "Accepted for publication at OOPSLA 2025 (R2)", "summary": "Probabilistic extensions of logic programming languages, such as ProbLog,\nintegrate logical reasoning with probabilistic inference to evaluate\nprobabilities of output relations; however, prior work does not account for\npotential statistical correlations among input facts. This paper introduces\nPraline, a new extension to Datalog designed for precise probabilistic\ninference in the presence of (partially known) input correlations. We formulate\nthe inference task as a constrained optimization problem, where the solution\nyields sound and precise probability bounds for output facts. However, due to\nthe complexity of the resulting optimization problem, this approach alone often\ndoes not scale to large programs. To address scalability, we propose a more\nefficient $\\delta$-exact inference algorithm that leverages constraint solving,\nstatic analysis, and iterative refinement. Our empirical evaluation on\nchallenging real-world benchmarks, including side-channel analysis,\ndemonstrates that our method not only scales effectively but also delivers\ntight probability bounds."}
{"id": "2508.15264", "pdf": "https://arxiv.org/pdf/2508.15264", "abs": "https://arxiv.org/abs/2508.15264", "authors": ["Patrick Redmond", "Jonathan Castello", "José Manuel Calderón Trilla", "Lindsey Kuper"], "title": "Exploring the Theory and Practice of Concurrency in the Entity-Component-System Pattern", "categories": ["cs.PL"], "comment": "This is an extended version (with appendices) of the OOPSLA 2025\n  paper", "summary": "The Entity-Component-System (ECS) software design pattern, long used in game\ndevelopment, encourages a clean separation of identity (entities), data\nproperties (components), and computational behaviors (systems). Programs\nwritten using the ECS pattern are naturally concurrent, and the pattern offers\nmodularity, flexibility, and performance benefits that have led to a\nproliferation of ECS frameworks. Nevertheless, the ECS pattern is little-known\nand not well understood outside of a few domains. Existing explanations of the\nECS pattern tend to be mired in the concrete details of particular ECS\nframeworks, or they explain the pattern in terms of imperfect metaphors or in\nterms of what it is not. We seek a rigorous understanding of the ECS pattern\nvia the design of a formal model, Core ECS, that abstracts away the details of\nspecific implementations to reveal the essence of software using the ECS\npattern. We identify a class of Core ECS programs that behave deterministically\nregardless of scheduling, enabling use of the ECS pattern as a\ndeterministic-by-construction concurrent programming model. With Core ECS as a\npoint of comparison, we then survey several real-world ECS frameworks and find\nthat they all leave opportunities for deterministic concurrency unexploited.\nOur findings point out a space for new ECS implementation techniques that\nbetter leverage such opportunities."}
{"id": "2508.15333", "pdf": "https://arxiv.org/pdf/2508.15333", "abs": "https://arxiv.org/abs/2508.15333", "authors": ["Francesco Dagnino", "Paola Giannini", "Violet Ka I Pun", "Ulises Torrella"], "title": "Fair Termination for Resource-Aware Active Objects", "categories": ["cs.PL", "F.3.3"], "comment": "18 pages, 12 pages of appendix, 12 figures, APLAS 2025", "summary": "Active object systems are a model of distributed computation that has been\nadopted for modelling distributed systems and business process workflows. This\nfield of modelling is, in essence, concurrent and resource-aware, motivating\nthe development of resource-aware formalisations on the active object model.\nThe contributions of this work are the development of a core calculus for\nresource-aware active objects together with a type system ensuring that\nwell-typed programs are fairly terminating, i.e., they can always eventually\nterminate. To achieve this, we combine techniques from graded semantics and\ntype systems, which are quite well understood for sequential programs, with\nthose for fair termination, which have been developed for synchronous~sessions."}
{"id": "2508.15576", "pdf": "https://arxiv.org/pdf/2508.15576", "abs": "https://arxiv.org/abs/2508.15576", "authors": ["Andreas Lööw", "Seung Hoon Park", "Daniele Nantes-Sobrinho", "Sacha-Élie Ayoun", "Opale Sjöstedt", "Philippa Gardner"], "title": "Compositional Symbolic Execution for the Next 700 Memory Models (Extended Version)", "categories": ["cs.PL"], "comment": null, "summary": "Multiple successful compositional symbolic execution (CSE) tools and\nplatforms exploit separation logic (SL) for compositional verification and/or\nincorrectness separation logic (ISL) for compositional bug-finding, including\nVeriFast, Viper, Gillian, CN, and Infer-Pulse. Previous work on the Gillian\nplatform, the only CSE platform that is parametric on the memory model, meaning\nthat it can be instantiated to different memory models, suggests that the\nability to use custom memory models allows for more flexibility in supporting\nanalysis of a wide range of programming languages, for implementing custom\nautomation, and for improving performance. However, the literature lacks a\nsatisfactory formal foundation for memory-model-parametric CSE platforms.\n  In this paper, inspired by Gillian, we provide a new formal foundation for\nmemory-model-parametric CSE platforms. Our foundation advances the state of the\nart in four ways. First, we mechanise our foundation (in the interactive\ntheorem prover Rocq). Second, we validate our foundation by instantiating it to\na broad range of memory models, including models for C and CHERI. Third,\nwhereas previous memory-model-parametric work has only covered SL analyses, we\ncover both SL and ISL analyses. Fourth, our foundation is based on standard\ndefinitions of SL and ISL (including definitions of function specification\nvalidity, to ensure sound interoperation with other tools and platforms also\nbased on standard definitions)."}
{"id": "2508.15750", "pdf": "https://arxiv.org/pdf/2508.15750", "abs": "https://arxiv.org/abs/2508.15750", "authors": ["Celeste Barnaby", "Qiaochu Chen", "Ramya Ramalingam", "Osbert Bastani", "Isil Dillig"], "title": "Active Learning for Neurosymbolic Program Synthesis", "categories": ["cs.PL"], "comment": null, "summary": "The goal of active learning for program synthesis is to synthesize the\ndesired program by asking targeted questions that minimize user interaction.\nWhile prior work has explored active learning in the purely symbolic setting,\nsuch techniques are inadequate for the increasingly popular paradigm of\nneurosymbolic program synthesis, where the synthesized program incorporates\nneural components. When applied to the neurosymbolic setting, such techniques\ncan -- and, in practice, do -- return an unintended program due to\nmispredictions of neural components. This paper proposes a new active learning\ntechnique that can handle the unique challenges posed by neural network\nmispredictions. Our approach is based upon a new evaluation strategy called\nconstrained conformal evaluation (CCE), which accounts for neural\nmispredictions while taking into account user-provided feedback. Our proposed\nmethod iteratively makes CCE more precise until all remaining programs are\nguaranteed to be observationally equivalent. We have implemented this method in\na tool called SmartLabel and experimentally evaluated it on three neurosymbolic\ndomains. Our results demonstrate that SmartLabel identifies the ground truth\nprogram for 98% of the benchmarks, requiring under 5 rounds of user interaction\non average. In contrast, prior techniques for active learning are only able to\nconverge to the ground truth program for at most 65% of the benchmarks."}
{"id": "2508.15454", "pdf": "https://arxiv.org/pdf/2508.15454", "abs": "https://arxiv.org/abs/2508.15454", "authors": ["Saar Tzour-Shaday", "Dana Drachsler Cohen"], "title": "Mini-Batch Robustness Verification of Deep Neural Networks", "categories": ["cs.LG", "cs.LO", "cs.PL"], "comment": "30 pages, 12 figures, conference OOPSLA 2025", "summary": "Neural network image classifiers are ubiquitous in many safety-critical\napplications. However, they are susceptible to adversarial attacks. To\nunderstand their robustness to attacks, many local robustness verifiers have\nbeen proposed to analyze $\\epsilon$-balls of inputs. Yet, existing verifiers\nintroduce a long analysis time or lose too much precision, making them less\neffective for a large set of inputs. In this work, we propose a new approach to\nlocal robustness: group local robustness verification. The key idea is to\nleverage the similarity of the network computations of certain $\\epsilon$-balls\nto reduce the overall analysis time. We propose BaVerLy, a sound and complete\nverifier that boosts the local robustness verification of a set of\n$\\epsilon$-balls by dynamically constructing and verifying mini-batches.\nBaVerLy adaptively identifies successful mini-batch sizes, accordingly\nconstructs mini-batches of $\\epsilon$-balls that have similar network\ncomputations, and verifies them jointly. If a mini-batch is verified, all\n$\\epsilon$-balls are proven robust. Otherwise, one $\\epsilon$-ball is suspected\nas not being robust, guiding the refinement. In the latter case, BaVerLy\nleverages the analysis results to expedite the analysis of that $\\epsilon$-ball\nas well as the other $\\epsilon$-balls in the batch. We evaluate BaVerLy on\nfully connected and convolutional networks for MNIST and CIFAR-10. Results show\nthat BaVerLy scales the common one by one verification by 2.3x on average and\nup to 4.1x, in which case it reduces the total analysis time from 24 hours to 6\nhours."}
