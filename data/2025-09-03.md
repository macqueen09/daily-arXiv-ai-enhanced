<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 6]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [cs.DC](#cs.DC) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [ChopChop: a Programmable Framework for Semantically Constraining the Output of Language Models](https://arxiv.org/abs/2509.00360)
*Shaan Nagy,Timothy Zhou,Nadia Polikarpova,Loris D'Antoni*

Main category: cs.PL

TL;DR: ChopChop是一个可编程的语义约束解码框架，通过连接词元级生成和抽象程序结构推理，确保语言模型生成的代码满足丰富的语义属性。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型生成的代码无法保证正确性，经常违反类型安全、程序不变量或语义等价性。现有约束解码方法仅限于浅层语法约束或依赖脆弱的语义编码。

Method: 采用基于共归纳的形式化方法，将约束执行简化为正则共数据上的可实现性问题，连接词元级生成与抽象程序结构推理。

Result: 框架支持类型安全和程序等价性约束，将语义约束解码从小众技术转变为系统化、原则性的语言模型扩展，提高了成功率和解码效率。

Conclusion: ChopChop展示了形式化方法可以无缝集成到LM驱动的代码生成中，为语义约束解码提供了系统化的解决方案。

Abstract: Language models (LMs) can generate code, but cannot guarantee its
correctness--producing outputs that often violate type safety, program
invariants, or semantic equivalence. Constrained decoding offers a solution by
restricting generation to programs that satisfy desired properties. Yet,
existing methods are limited to shallow syntactic constraints or rely on
brittle, ad hoc encodings of semantics over token sequences.
  We present ChopChop, the first programmable framework for semantic
constrained decoding, enabling LMs to generate code that provably satisfies
rich semantic properties. ChopChop connects token-level generation with
reasoning over abstract program structures using a coinduction-based formalism
and reduces constraint enforcement to a realizability problem over regular
codata. We demonstrate ChopChop's generality through generation constrained by
type safety and program equivalence, showing how formal methods can be
seamlessly integrated into LM-driven code generation. ChopChop transforms
semantic constrained decoding from a niche technique into a systematic,
principled extension of LMs--improving success rates across models and tasks
while maintaining practical decoding latency.

</details>


### [2] [A Hoare Logic for Symmetry Properties](https://arxiv.org/abs/2509.00587)
*Vaibhav Mehta,Justin Hsu*

Main category: cs.PL

TL;DR: 本文提出了一个用于验证程序对称性属性的形式化方法，包括群动作语法设计和Hoare风格逻辑，并开发了原型工具SymVerif进行验证


<details>
  <summary>Details</summary>
Motivation: 现有形式化方法缺乏对程序对称性属性的支持，而许多自然程序正确性属性可以通过对称性来表达

Method: 设计群动作语法支持标准构造和蕴含关系，开发Hoare风格的逻辑来验证命令式程序的对称性属性，群动作替代传统的断言条件

Result: 开发了原型工具SymVerif，在一系列手工制作的基准测试中验证了对称性属性，并发现了McLachlan和Quispel(2002)描述的动力学系统模型中的错误

Conclusion: 该方法能够有效验证广泛的对称性属性，工具在实际案例中发现了现有模型中的错误，证明了方法的实用价值

Abstract: Many natural program correctness properties can be stated in terms of
  symmetries, but existing formal methods have little support for reasoning
  about such properties. We consider how to formally verify a broad class of
  symmetry properties expressed in terms of group actions. To specify these
  properties, we design a syntax for group actions, supporting standard
  constructions and a natural notion of entailment. Then, we develop a
  Hoare-style logic for verifying symmetry properties of imperative programs,
  where group actions take the place of the typical pre- and post-condition
  assertions. Finally, we develop a prototype tool $\mathsf{SymVerif}$, and use
  it to verify symmetry properties on a series of handcrafted benchmarks. Our
  tool uncovered an error in a model of a dynamical system described by
\citet{McLachlan_Quispel_2002}.

</details>


### [3] [Formalizing Linear Motion G-code for Invariant Checking and Differential Testing of Fabrication Tools](https://arxiv.org/abs/2509.00699)
*Yumeng He,Chandrakana Nandi,Sreepathi Pai*

Main category: cs.PL

TL;DR: 提出了一种将G代码转换为立方体集合的算法，并通过点云表示进行高效操作，以支持CAD模型错误定位、切片器比较和网格修复工具效果评估


<details>
  <summary>Details</summary>
Motivation: 传统编译器有成熟的程序不变量检查和差分测试技术，但3D打印制造流程缺乏类似方法。需要一种能够处理几何对象和机器代码的方法来支持该领域的可靠性验证

Method: 开发了一种将G代码程序升级为立方体集合的算法，并定义了近似点云表示来高效操作这些立方体。实现了原型工具GlitchFinder

Result: 在58个实际CAD模型上评估，结果显示GlitchFinder能有效识别小特征导致的切片问题，对比不同切片器的差异，以及发现网格修复工具在修复过程中引入的新错误

Conclusion: 该算法为3D打印制造流程提供了类似传统编译器验证的新方法，开启了错误定位、工具比较和质量评估等新的应用场景

Abstract: The computational fabrication pipeline for 3D printing is much like a
compiler - users design models in Computer Aided Design (CAD) tools that are
lowered to polygon meshes to be ultimately compiled to machine code by 3D
slicers. For traditional compilers and programming languages, techniques for
checking program invariants are well-established. Similarly, methods like
differential testing are often used to uncover bugs in compilers themselves,
which makes them more reliable. The fabrication pipeline would benefit from
similar techniques but traditional approaches do not directly apply to the
representations used in this domain. Unlike traditional programs, 3D models
exist both as geometric objects as well as machine code that ultimately runs on
the hardware. The machine code, like in traditional compiling, is affected by
many factors like the model, the slicer being used, and numerous
user-configurable parameters that control the slicing process. In this work, we
propose a new algorithm for lifting G-code (a common language used in
fabrication pipelines) by denoting a G-code program to a set of cuboids, and
then defining an approximate point cloud representation for efficiently
operating on these cuboids. Our algorithm opens up new opportunities: we show
three use cases that demonstrate how it enables error localization in CAD
models through invariant checking, quantitative comparisons between slicers,
and evaluating the efficacy of mesh repair tools. We present a prototype
implementation of our algorithm in a tool, GlitchFinder, and evaluate it on 58
real-world CAD models. Our results show that GlitchFinder is particularly
effective in identifying slicing issues due to small features, can highlight
differences in how popular slicers (Cura and PrusaSlicer) slice the same model,
and can identify cases where mesh repair tools (MeshLab and Meshmixer)
introduce new errors during repair.

</details>


### [4] [Decision Procedure for A Theory of String Sequences](https://arxiv.org/abs/2509.00948)
*Denghang Hu,Taolue Chen,Philipp Rümmer,Fu Song,Zhilin Wu*

Main category: cs.PL

TL;DR: 本文提出字符串序列理论，研究其可满足性问题，通过将字符串序列编码为字符串并限制为直线片段来恢复可判定性，实现了OSTRICH框架下的决策过程。


<details>
  <summary>Details</summary>
Motivation: 现有SMT求解器通常只支持通用序列理论，缺乏对字符串序列操作（如正则表达式匹配、字符串分割和连接）的直接支持，而这些操作在字符串处理程序中经常使用。

Method: 将每个字符串序列编码为字符串，将字符串序列操作映射为相应的字符串操作，通过自动机进行结果操作的前像计算，并将其集成到OSTRICH字符串约束求解框架中。

Result: 实现了工具ostrichseq，在真实JavaScript程序生成的基准约束、手工制作的模板和单元测试上进行了实验，验证了方法的有效性。

Conclusion: 虽然字符串序列理论在一般情况下是不可判定的，但通过限制到直线片段可以恢复可判定性，所提出的方法在实际应用中表现出良好的效能。

Abstract: The theory of sequences, supported by many SMT solvers, can model program
data types including bounded arrays and lists. Sequences are parameterized by
the element data type and provide operations such as accessing elements,
concatenation, forming sub-sequences and updating elements. Strings and
sequences are intimately related; many operations, e.g., matching a string
according to a regular expression, splitting strings, or joining strings in a
sequence, are frequently used in string-manipulating programs. Nevertheless,
these operations are typically not directly supported by existing SMT solvers,
which instead only consider the generic theory of sequences. In this paper, we
propose a theory of string sequences and study its satisfiability. We show
that, while it is undecidable in general, the decidability can be recovered by
restricting to the straight-line fragment. This is shown by encoding each
string sequence as a string, and each string sequence operation as a
corresponding string operation. We provide pre-image computation for the
resulting string operations with respect to automata, effectively casting it
into the generic OSTRICH string constraint solving framework. We implement the
new decision procedure as a tool $\ostrichseq$, and carry out experiments on
benchmark constraints generated from real-world JavaScript programs,
hand-crafted templates and unit tests. The experiments confirm the efficacy of
our approach.

</details>


### [5] [Type-Based Incorrectness Reasoning](https://arxiv.org/abs/2509.01511)
*Zhe Zhou,Benjamin Delaware,Suresh Jagannathan*

Main category: cs.PL

TL;DR: 覆盖类型将函数式语言中的精化类型推广到支持必须式下近似推理，与不正确性逻辑有深刻联系，为测试框架和程序分析提供新机会


<details>
  <summary>Details</summary>
Motivation: 传统的精化类型主要支持上近似推理，而属性测试框架需要验证测试生成器的完整性和安全性，这需要下近似推理能力。覆盖类型与不正确性逻辑之间的意外联系值得深入探索

Method: 提出将不正确性推理机制系统性地集成到表达性精化类型系统中，探索覆盖类型与不正确性逻辑的深层联系

Result: 建立了覆盖类型与不正确性逻辑的理论联系，展示了这种集成如何为函数式程序员、程序验证器和分析工具提供新的能力

Conclusion: 覆盖类型与不正确性逻辑的整合为程序验证和分析开辟了新途径，特别是在属性测试和程序正确性验证方面具有重要应用价值

Abstract: A coverage type generalizes refinement types found in many functional
languages with support for must-style underapproximate reasoning.
Property-based testing frameworks are one particularly useful domain where such
capabilities are useful as they allow us to verify the completeness, as well as
safety, of test generators. There is a surprising connection between the kind
of underapproximate reasoning coverage types offer and the style of reasoning
enabled by recently proposed Incorrectness Logic frameworks. In our
presentation, we propose to explore this connection more deeply, identifying
mechanisms that more systematically integrate incorrectness reasoning within an
expressive refinement type system and the opportunities that such integration
offers to functional programmers, program verifiers, and program analyzers and
related tools.

</details>


### [6] [From Traces to Program Incorrectness: A Type-Theoretic Approach](https://arxiv.org/abs/2509.02428)
*Yongwei Yuan,Zhe Zhou,Julia Belyakova,Benjamin Delaware,Suresh Jagannathan*

Main category: cs.PL

TL;DR: 提出了一个类型论框架，用于分析函数式程序中与有副作用的不透明库API交互时的错误行为，基于符号正则表达式和符号有限自动机进行轨迹追踪和错误规范验证。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏对函数式程序中与不透明库API交互时错误行为的系统性分析框架，需要一种能够跨函数边界形式化指定错误抽象数据类型行为的方法。

Method: 使用符号正则表达式(SREs)表示API调用轨迹，开发新型类型推断算法，基于符号有限自动机(SFAs)进行组合式轨迹推理，在指定错误属性下进行模运算。

Result: 算法成功时，推断的类型证明ADT实现能够表现出指定错误行为的某个子集，实现了基于轨迹的错误规范的下近似推理。

Conclusion: 这是首个针对基于轨迹的错误规范进行系统性下近似推理的方法，为轨迹引导的组合式分析开辟了新途径。

Abstract: We present a type-theoretic framework for reasoning about incorrectness in
functional programs that interact with effectful, opaque library APIs. Our
approach centers on traces -- temporally-ordered sequences of library API
invocations -- which naturally characterize both the preconditions of
individual APIs and their composite behavior. We represent these traces using
symbolic regular expressions (SREs), enabling formal specification of incorrect
abstract data type (ADT) behaviors across function boundaries. The core
contribution is a novel type inference algorithm that operates modulo specified
incorrectness properties and leverages the symbolic finite automata (SFAs)
representations of regexes for compositional reasoning of traces. When the
algorithm succeeds, the inferred types witness that an ADT implementation can
exhibit some subset of the specified incorrect behaviors. This represents the
first systematic approach to underapproximate reasoning against trace-based
incorrectness specifications, enabling a new form of trace-guided compositional
analysis.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [7] [Traq: Estimating the Quantum Cost of Classical Programs](https://arxiv.org/abs/2509.01508)
*Anurudh Peduri,Gilles Barthe,Michael Walter*

Main category: quant-ph

TL;DR: Traq是一个自动估算量子加速的框架，包含量子加速原语语言、成本分析和量子程序编译，能提供细粒度的非渐近复杂度上界


<details>
  <summary>Details</summary>
Motivation: 当前量子加速预测需要冗长的手动分析和数值模拟，且每次只能针对一个特定应用，缺乏自动化且可证明保证的方法

Method: 开发了包含量子加速原语的高级语言，设计成本分析算法来上界量子程序复杂度（包含非渐近信息和输入敏感性），并提供编译到低级量子程序的能力

Result: 提出了概念验证实现，并通过AND-OR树的案例研究验证了方法的有效性

Conclusion: Traq提供了首个具有可证明保证的全自动量子加速估算方法，能够细粒度地分析量子程序复杂度

Abstract: Predicting practical speedups offered by future quantum computers has become
a major focus of the quantum computing community. Typically, these predictions
are supported by lengthy manual analyses and numerical simulations and are
carried out for one specific application at a time. In this paper, we present
Traq, a principled approach towards estimating the quantum speedup of classical
programs fully automatically and with provable guarantees. It consists of a
classical language that includes high-level primitives amenable to quantum
speedups, a cost analysis, and a compilation to low-level quantum programs. Our
cost analysis upper bounds the complexity of the resulting quantum program in a
fine-grained way: it captures non-asymptotic information and is sensitive to
the input of the program (rather than providing worst-case costs). We also
provide a proof-of-concept implementation and a case study inspired by AND-OR
trees.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [8] [REFINESTAT: Efficient Exploration for Probabilistic Program Synthesis](https://arxiv.org/abs/2509.01082)
*Madhav Kanda,Shubham Ugare,Sasa Misailovic*

Main category: cs.LG

TL;DR: RefineStat是一个基于语言模型的框架，通过语义约束和诊断感知的细化来生成语法正确且统计可靠的概率程序，使小型语言模型在概率编程代码生成任务上达到或超越大型语言模型的表现。


<details>
  <summary>Details</summary>
Motivation: 概率编程中的统计模型发现需要在大搜索空间和严格领域约束下进行，小型语言模型生成的概率程序经常存在语法和语义错误（如错误的推理结构）。受概率程序员领域专业知识和调试策略的启发。

Method: 引入RefineStat框架：1）强制执行语义约束确保合成程序包含有效分布和良好参数；2）当可靠性检查失败时，应用诊断感知细化，重新采样先验或似然组件。

Result: 在多个概率编程代码生成任务上评估，使用小型语言模型时，RefineStat生成的程序既语法正确又统计可靠，经常匹配或超越闭源大型语言模型（如OpenAI o3）的表现。

Conclusion: RefineStat通过结合语义约束和诊断感知细化，有效解决了小型语言模型在概率编程中的错误问题，显著提升了生成程序的质量和可靠性。

Abstract: Probabilistic programming offers a powerful framework for modeling
uncertainty, yet statistical model discovery in this domain entails navigating
an immense search space under strict domain-specific constraints. When small
language models are tasked with generating probabilistic programs, they
frequently produce outputs that suffer from both syntactic and semantic errors,
such as flawed inference constructs. Motivated by probabilistic programmers'
domain expertise and debugging strategies, we introduce RefineStat, a language
model--driven framework that enforces semantic constraints ensuring synthesized
programs contain valid distributions and well-formed parameters, and then
applies diagnostic-aware refinement by resampling prior or likelihood
components whenever reliability checks fail. We evaluate RefineStat on multiple
probabilistic-programming code-generation tasks using smaller language models
(SLMs) and find that it produces programs that are both syntactically sound and
statistically reliable, often matching or surpassing those from closed-source
large language models (e.g., OpenAI o3).

</details>


### [9] [DaCe AD: Unifying High-Performance Automatic Differentiation for Machine Learning and Scientific Computing](https://arxiv.org/abs/2509.02197)
*Afif Boudaoud,Alexandru Calotoiu,Marcin Copik,Torsten Hoefler*

Main category: cs.LG

TL;DR: DaCe AD是一个通用的自动微分引擎，无需代码修改，通过ILP算法优化存储与重计算的权衡，在HPC基准测试中比JAX快92倍以上


<details>
  <summary>Details</summary>
Motivation: 现有AD框架存在编程语言支持有限、需要代码修改、科学计算性能不足、存储策略简单等限制，迫使领域科学家手动计算大型问题的梯度

Method: 使用基于整数线性规划(ILP)的新算法，在给定内存约束下优化存储与重计算的权衡，实现最大性能

Result: 在NPBench HPC基准测试套件上，平均比最先进的JAX框架快92倍以上，且无需任何代码修改

Conclusion: DaCe AD提供了一个通用高效的自动微分解决方案，解决了现有AD框架的主要限制，特别适用于科学计算和机器学习集成应用

Abstract: Automatic differentiation (AD) is a set of techniques that systematically
applies the chain rule to compute the gradients of functions without requiring
human intervention. Although the fundamentals of this technology were
established decades ago, it is experiencing a renaissance as it plays a key
role in efficiently computing gradients for backpropagation in machine learning
algorithms. AD is also crucial for many applications in scientific computing
domains, particularly emerging techniques that integrate machine learning
models within scientific simulations and schemes. Existing AD frameworks have
four main limitations: limited support of programming languages, requiring code
modifications for AD compatibility, limited performance on scientific computing
codes, and a naive store-all solution for forward-pass data required for
gradient calculations. These limitations force domain scientists to manually
compute the gradients for large problems. This work presents DaCe AD, a
general, efficient automatic differentiation engine that requires no code
modifications. DaCe AD uses a novel ILP-based algorithm to optimize the
trade-off between storing and recomputing to achieve maximum performance within
a given memory constraint. We showcase the generality of our method by applying
it to NPBench, a suite of HPC benchmarks with diverse scientific computing
patterns, where we outperform JAX, a Python framework with state-of-the-art
general AD capabilities, by more than 92 times on average without requiring any
code changes.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [10] [Safe Memory Reclamation Techniques](https://arxiv.org/abs/2509.02457)
*Ajay Singh*

Main category: cs.DC

TL;DR: 该论文研究安全内存回收算法，针对无垃圾收集编程语言中的乐观和无锁并发数据结构，探讨如何实现高速、可扩展、易用且内存占用小的解决方案。


<details>
  <summary>Details</summary>
Motivation: 在非垃圾收集编程语言中，安全内存回收对于乐观和无锁并发数据结构的内存安全至关重要。设计理想的安全内存回收算法面临多个挑战，包括实现高速和可扩展性、程序员易用性、广泛数据结构的适用性、管理延迟释放内存导致的大内存占用，以及避免数据结构操作的不对称开销。

Method: 通过融合硬件-软件栈各层的思路和工具，研究多种安全内存回收算法设计方法。这些解决方案跨越传统边界，利用不同层级暴露的特性。

Result: 论文提出了跨层级的安全内存回收解决方案，但具体结果需要阅读完整论文才能了解。

Conclusion: 安全内存回收算法的设计需要综合考虑性能、易用性和内存效率，通过硬件-软件栈的协同设计可以实现更好的解决方案。

Abstract: Safe memory reclamation is crucial to memory safety for optimistic and
lock-free concurrent data structures in non garbage collected programming
languages. However, several challenges arise in designing an ideal safe memory
reclamation algorithm, including achieving high speed and scalability, easy of
use for programmers, applicability to wide class of data structures, managing
the large memory footprint caused by delayed freeing of memory for safety and
performance, and avoiding asymmetric overhead on data structure operations.
Several approaches to designing safe memory reclamation algorithms are studied
by blending ideas and tools from across the hardware-software stack. These
solutions cross traditional boundaries and exploit features exposed at
different layers.

</details>
