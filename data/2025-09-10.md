<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 3]
- [cs.SE](#cs.SE) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [veScale: Consistent and Efficient Tensor Programming with Eager-Mode SPMD](https://arxiv.org/abs/2509.07003)
*Youjie Li,Cheng Wan,Zhiqi Lin,Hongyu Zhu,Jiacheng Yang,Ziang Song,Xinyi Di,Jiawei Wu,Huiyao Shu,Wenlei Bao,Yanghua Peng,Haibin Lin,Li-Wen Chang*

Main category: cs.PL

TL;DR: veScale是一个基于SPMD范式的分布式训练系统，通过创新的分布式随机数生成算法解决了结果一致性问题，并在性能和代码复杂度方面显著优于现有系统


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模和复杂度的快速增长，分布式训练需要更复杂的并行策略（如3D并行），这促使研究者寻求更简单、更易调试的编程范式，如SPMD。但SPMD在即时执行模式下存在结果一致性和性能扩展两大挑战

Method: veScale采用完全SPMD范式，引入创新的分布式随机数生成算法（兼容任意分片操作符），通过减少PyTorch原语开销和提升通信效率来优化性能

Result: 评估显示veScale相比最先进的训练系统（如TorchTitan）实现了最高2.2倍的加速，代码复杂度降低78.4%，同时保持与单设备等效的结果

Conclusion: veScale成功实现了SPMD范式的民主化分布式张量编程，在保证结果一致性的同时显著提升了训练性能和开发效率

Abstract: Large Language Models (LLMs) have scaled rapidly in size and complexity,
requiring increasingly intricate parallelism for distributed training, such as
3D parallelism. This sophistication motivates a shift toward simpler, more
debuggable programming paradigm like Single Program Multiple Data (SPMD).
However, SPMD in eager execution introduces two key challenges: ensuring
consistency with single-device execution and achieving high performance at
scale. In this paper, we introduce veScale, an eager-mode training system that
fully embraces SPMD paradigm to democratize distributed tensor programming.
veScale addresses the prevalent issue of inconsistent results in systems like
PyTorch by introducing a novel algorithm of distributed Random Number
Generation (RNG) compatible with arbitrary sharded operators. veScale also
significantly boosts training performance by reducing PyTorch primitive's
overhead and improving communication efficiency. Evaluations show that veScale
delivers up to 2.2x speedup over the state-of-the-art training systems, like
TorchTitan, and cuts code complexity by 78.4%, while preserving
single-device-equivalent results.

</details>


### [2] [Fast and Extensible Hybrid Embeddings with Micros](https://arxiv.org/abs/2509.07551)
*Sean Bocirnea,William J. Bowman*

Main category: cs.PL

TL;DR: 小型嵌入技术（micros）通过语法到中间表示的转换，提高了编译性能，实现了可扩展的静态类型语言嵌入


<details>
  <summary>Details</summary>
Motivation: 解决宏嵌入技术在编译时性能上的不足，虽然宏嵌入能够实现可扩展的类型语言，但编译速度较慢

Method: 采用micros技术（语法到IR转换器），先生成深度嵌入的中间表示，然后再浅嵌入回源代码语法，结合多种设计模式使IR及其函数可扩展

Result: 实现了可扩展的混合嵌入静态类型语言，编译时性能显著提高，较宏嵌入方法有显著改善

Conclusion: Micro嵌入技术通过优化编译过程，在保持语言扩展性的同时大幅提升了编译效率，为嵌入式语言开发提供了更高性能的解决方案

Abstract: Macro embedding is a popular approach to defining extensible shallow
embeddings of object languages in Scheme like host languages. While macro
embedding has even been shown to enable implementing extensible typed languages
in systems like Racket, it comes at a cost: compile-time performance. In this
paper, we revisit micros - syntax to intermediate representation (IR)
transformers, rather than source syntax to source syntax transformers (macros).
Micro embedding enables stopping at an IR, producing a deep embedding and
enabling high performance compile-time functions over an efficient IR, before
shallowly embedding the IR back into source syntax. Combining micros with
several design patterns to enable the IR and functions over it to be
extensible, we achieve extensible hybrid embedding of statically typed
languages with significantly improved compile-time compared to macro-embedding
approaches. We describe our design patterns and propose new abstractions
packaging these patterns.

</details>


### [3] [What's in the Box: Ergonomic and Expressive Capture Tracking over Generic Data Structures (Extended Version)](https://arxiv.org/abs/2509.07609)
*Yichen Xu,Oliver Bračevac,Cao Nguyen Pham,Martin Odersky*

Main category: cs.PL

TL;DR: System Capless为Scala捕获类型提供了新理论基础，通过reach capabilities机制解决泛型数据结构中能力命名问题，支持标准集合库的捕获检查


<details>
  <summary>Details</summary>
Motivation: 现有Scala捕获类型系统无法有效跟踪嵌入在泛型数据结构中的能力，限制了其在标准集合库中的应用，阻碍了更广泛的采用

Method: 开发System Capless演算，引入reach capabilities机制，将通用能力概念细化为存在和通用捕获集量化方案，并在Lean中形式化验证

Result: 基于System Capless在Scala 3中完全重新实现捕获检查，成功迁移整个Scala集合库和异步编程库，证明在实际代码中采用的最小改动和零标注开销

Conclusion: reach capabilities使得捕获检查能够在生产代码中以最小改动和几乎零标注开销的方式被采用，解决了泛型数据结构中能力跟踪的关键限制

Abstract: Capturing types in Scala unify static effect and resource tracking with
object capabilities, enabling lightweight effect polymorphism with minimal
notational overhead. However, their expressiveness has been insufficient for
tracking capabilities embedded in generic data structures, preventing them from
scaling to the standard collections library -- an essential prerequisite for
broader adoption. This limitation stems from the inability to name capabilities
within the system's notion of box types.
  This paper develops System Capless, a new foundation for capturing types that
provides the theoretical basis for reach capabilities (rcaps), a novel
mechanism for naming "what's in the box." The calculus refines the universal
capability notion into a new scheme with existential and universal capture set
quantification. Intuitively, rcaps witness existentially quantified capture
sets inside the boxes of generic types in a way that does not require exposing
existential capture types in the surface language. We have fully mechanized the
formal metatheory of System Capless in Lean, including proofs of type soundness
and scope safety. System Capless supports the same lightweight notation of
capturing types plus rcaps, as certified by a type-preserving translation, and
also enables fully optional explicit capture-set quantification to increase
expressiveness.
  Finally, we present a full reimplementation of capture checking in Scala 3
based on System Capless and migrate the entire Scala collections library and an
asynchronous programming library to evaluate its practicality and ergonomics.
Our results demonstrate that reach capabilities enable the adoption of capture
checking in production code with minimal changes and minimal-to-zero notational
overhead in a vast majority of cases.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [4] [What Were You Thinking? An LLM-Driven Large-Scale Study of Refactoring Motivations in Open-Source Projects](https://arxiv.org/abs/2509.07763)
*Mikel Robredo,Matteo Esposito,Fabio Palomba,Rafael Peñaloza,Valentina Lenarduzzi*

Main category: cs.SE

TL;DR: LLMs能有效识别80%的代码重构动机，但与传统文献动机仅47%一致，主要关注可读性和可维护性等实用目标。


<details>
  <summary>Details</summary>
Motivation: 理解开发者重构代码的动机以及哪些指标能捕捉这些动机，以支持更广泛和有效的重构实践。

Method: 大规模实证研究，利用大语言模型从版本控制数据中识别重构动机，并与文献中报告的动机进行比较。

Result: LLMs在80%的情况下与人类判断一致，但与文献动机仅47%一致；丰富了22%的动机细节；主要动机是简化代码和提高可维护性；开发者经验和代码可读性指标相关性较弱。

Conclusion: LLMs能有效捕捉表面层动机但难以处理架构推理，其价值在于提供局部解释，与软件指标结合可形成混合方法，系统化重构优先级并平衡短期改进与长期架构目标。

Abstract: Context. Code refactoring improves software quality without changing external
behavior. Despite its advantages, its benefits are hindered by the considerable
cost of time, resources, and continuous effort it demands. Aim. Understanding
why developers refactor, and which metrics capture these motivations, may
support wider and more effective use of refactoring in practice. Method. We
performed a large-scale empirical study to analyze developers refactoring
activity, leveraging Large Language Models (LLMs) to identify underlying
motivations from version control data, comparing our findings with previous
motivations reported in the literature. Results. LLMs matched human judgment in
80% of cases, but aligned with literature-based motivations in only 47%. They
enriched 22% of motivations with more detailed rationale, often highlighting
readability, clarity, and structural improvements. Most motivations were
pragmatic, focused on simplification and maintainability. While metrics related
to developer experience and code readability ranked highest, their correlation
with motivation categories was weak. Conclusions. We conclude that LLMs
effectively capture surface-level motivations but struggle with architectural
reasoning. Their value lies in providing localized explanations, which, when
combined with software metrics, can form hybrid approaches. Such integration
offers a promising path toward prioritizing refactoring more systematically and
balancing short-term improvements with long-term architectural goals.

</details>
