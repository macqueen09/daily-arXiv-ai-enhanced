{"id": "2509.14496", "pdf": "https://arxiv.org/pdf/2509.14496", "abs": "https://arxiv.org/abs/2509.14496", "authors": ["Wyatt Petula", "Anushcka Joshi", "Peggy Tu", "Amrutha Somasundar", "Suman Saha"], "title": "DeliverC: Teaching Pointers through GenAI-Powered Game-Based Learning", "categories": ["cs.PL"], "comment": "The paper before Camera-ready paper. The paper has been accepted by\n  SIGCSE 2026", "summary": "While game-based learning is widely used in programming education, few tools\noffer adaptive, real-time support for complex topics, such as C pointers. We\npresent DeliverC, a GenAI-enhanced game that integrates GPT-4-mini to provide\npersonalized hints and generate pointer-related challenges on the fly. In a\npilot study involving 25 undergraduate students, we investigated the impact of\nthe system on learning through gameplay data and a 15-item survey that covered\nconstructs such as motivation, self-efficacy, metacognition, and feedback\nquality. Results show that most students felt more confident and reflective\nafter using the tool, and error rates decreased as students progressed through\nscaffolded levels. However, participation decreased with task difficulty, and\nsome students reported receiving unclear or vague feedback. These findings\nsuggest that DeliverC can enhance engagement and understanding in systems\nprogramming, although refinement in AI-generated feedback is still needed. Our\nstudy highlights the potential of combining GenAI with game-based learning to\nsupport personalized and interactive practice in traditionally challenging\nprogramming domains."}
{"id": "2509.15005", "pdf": "https://arxiv.org/pdf/2509.15005", "abs": "https://arxiv.org/abs/2509.15005", "authors": ["Facundo Dom√≠nguez", "Arnaud Spiwack"], "title": "Refinement-Types Driven Development: A study", "categories": ["cs.PL"], "comment": "11 pages, 3 figures, artifacts\n  https://github.com/tweag/ifl2025-liquidhaskell", "summary": "This paper advocates for the broader application of SMT solvers in everyday\nprogramming, challenging the conventional wisdom that these tools are solely\nfor formal methods and verification. We claim that SMT solvers, when seamlessly\nintegrated into a compiler's static checks, significantly enhance the\ncapabilities of ordinary type checkers in program composition. Specifically, we\nargue that refinement types, as embodied by Liquid Haskell, enable the use of\nSMT solvers in mundane programming tasks. Through a case study on handling\nbinder scopes in compilers, we envision a future where ordinary programming is\nmade simpler and more enjoyable with the aid of refinement types and SMT\nsolvers. As a secondary contribution, we present a prototype implementation of\na theory of finite maps for Liquid Haskell's solver, developed to support our\ncase study."}
{"id": "2509.14404", "pdf": "https://arxiv.org/pdf/2509.14404", "abs": "https://arxiv.org/abs/2509.14404", "authors": ["Haoye Tian", "Chong Wang", "BoYang Yang", "Lyuye Zhang", "Yang Liu"], "title": "A Taxonomy of Prompt Defects in LLM Systems", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.PL"], "comment": null, "summary": "Large Language Models (LLMs) have become key components of modern software,\nwith prompts acting as their de-facto programming interface. However, prompt\ndesign remains largely empirical and small mistakes can cascade into\nunreliable, insecure, or inefficient behavior. This paper presents the first\nsystematic survey and taxonomy of prompt defects, recurring ways that prompts\nfail to elicit their intended behavior from LLMs. We organize defects along six\ndimensions: (1) Specification and Intent, (2) Input and Content, (3) Structure\nand Formatting, (4) Context and Memory, (5) Performance and Efficiency, and (6)\nMaintainability and Engineering. Each dimension is refined into fine-grained\nsubtypes, illustrated with concrete examples and root cause analysis. Grounded\nin software engineering principles, we show how these defects surface in real\ndevelopment workflows and examine their downstream effects. For every subtype,\nwe distill mitigation strategies that span emerging prompt engineering\npatterns, automated guardrails, testing harnesses, and evaluation frameworks.\nWe then summarize these strategies in a master taxonomy that links defect,\nimpact, and remedy. We conclude with open research challenges and a call for\nrigorous engineering-oriented methodologies to ensure that LLM-driven systems\nare dependable by design."}
{"id": "2509.14623", "pdf": "https://arxiv.org/pdf/2509.14623", "abs": "https://arxiv.org/abs/2509.14623", "authors": ["Hanlong Wan", "Xing Lu", "Yan Chen", "Karthik Devaprasad", "Laura Hinkle"], "title": "Automating Modelica Module Generation Using Large Language Models: A Case Study on Building Control Description Language", "categories": ["cs.SE", "cs.AI", "cs.PL", "cs.SY", "eess.SY"], "comment": "This is the pre-peer-review version of a journal paper; the repo is\n  available at: https://github.com/pnnl/prompt2control", "summary": "Dynamic energy systems and controls require advanced modeling frameworks to\ndesign and test supervisory and fault tolerant strategies. Modelica is a widely\nused equation based language, but developing control modules is labor intensive\nand requires specialized expertise. This paper examines the use of large\nlanguage models (LLMs) to automate the generation of Control Description\nLanguage modules in the Building Modelica Library as a case study. We developed\na structured workflow that combines standardized prompt scaffolds, library\naware grounding, automated compilation with OpenModelica, and human in the loop\nevaluation. Experiments were carried out on four basic logic tasks (And, Or,\nNot, and Switch) and five control modules (chiller enable/disable, bypass valve\ncontrol, cooling tower fan speed, plant requests, and relief damper control).\nThe results showed that GPT 4o failed to produce executable Modelica code in\nzero shot mode, while Claude Sonnet 4 achieved up to full success for basic\nlogic blocks with carefully engineered prompts. For control modules, success\nrates reached 83 percent, and failed outputs required medium level human repair\n(estimated one to eight hours). Retrieval augmented generation often produced\nmismatches in module selection (for example, And retrieved as Or), while a\ndeterministic hard rule search strategy avoided these errors. Human evaluation\nalso outperformed AI evaluation, since current LLMs cannot assess simulation\nresults or validate behavioral correctness. Despite these limitations, the LLM\nassisted workflow reduced the average development time from 10 to 20 hours down\nto 4 to 6 hours per module, corresponding to 40 to 60 percent time savings.\nThese results highlight both the potential and current limitations of LLM\nassisted Modelica generation, and point to future research in pre simulation\nvalidation, stronger grounding, and closed loop evaluation."}
{"id": "2509.14635", "pdf": "https://arxiv.org/pdf/2509.14635", "abs": "https://arxiv.org/abs/2509.14635", "authors": ["Weihan Peng", "Yuling Shi", "Yuhang Wang", "Xinyun Zhang", "Beijun Shen", "Xiaodong Gu"], "title": "SWE-QA: Can Language Models Answer Repository-level Code Questions?", "categories": ["cs.CL", "cs.PL", "cs.SE"], "comment": "Code and data available at\n  https://github.com/peng-weihan/SWE-QA-Bench", "summary": "Understanding and reasoning about entire software repositories is an\nessential capability for intelligent software engineering tools. While existing\nbenchmarks such as CoSQA and CodeQA have advanced the field, they predominantly\nfocus on small, self-contained code snippets. These setups fail to capture the\ncomplexity of real-world repositories, where effective understanding and\nreasoning often require navigating multiple files, understanding software\narchitecture, and grounding answers in long-range code dependencies. In this\npaper, we present SWE-QA, a repository-level code question answering (QA)\nbenchmark designed to facilitate research on automated QA systems in realistic\ncode environments. SWE-QA involves 576 high-quality question-answer pairs\nspanning diverse categories, including intention understanding, cross-file\nreasoning, and multi-hop dependency analysis. To construct SWE-QA, we first\ncrawled 77,100 GitHub issues from 11 popular repositories. Based on an analysis\nof naturally occurring developer questions extracted from these issues, we\ndeveloped a two-level taxonomy of repository-level questions and constructed a\nset of seed questions for each category. For each category, we manually curated\nand validated questions and collected their corresponding answers. As a\nprototype application, we further develop SWE-QA-Agent, an agentic framework in\nwhich LLM agents reason and act to find answers automatically. We evaluate six\nadvanced LLMs on SWE-QA under various context augmentation strategies.\nExperimental results highlight the promise of LLMs, particularly our\nSWE-QA-Agent framework, in addressing repository-level QA, while also revealing\nopen challenges and pointing to future research directions."}
{"id": "2509.14646", "pdf": "https://arxiv.org/pdf/2509.14646", "abs": "https://arxiv.org/abs/2509.14646", "authors": ["Yongpan Wang", "Xin Xu", "Xiaojie Zhu", "Xiaodong Gu", "Beijun Shen"], "title": "SALT4Decompile: Inferring Source-level Abstract Logic Tree for LLM-Based Binary Decompilation", "categories": ["cs.SE", "cs.PL"], "comment": "13 pages, 7 figures", "summary": "Decompilation is widely used in reverse engineering to recover high-level\nlanguage code from binary executables. While recent approaches leveraging Large\nLanguage Models (LLMs) have shown promising progress, they typically treat\nassembly code as a linear sequence of instructions, overlooking arbitrary jump\npatterns and isolated data segments inherent to binary files. This limitation\nsignificantly hinders their ability to correctly infer source code semantics\nfrom assembly code. To address this limitation, we propose \\saltm, a novel\nbinary decompilation method that abstracts stable logical features shared\nbetween binary and source code. The core idea of \\saltm is to abstract selected\nbinary-level operations, such as specific jumps, into a high-level logic\nframework that better guides LLMs in semantic recovery. Given a binary\nfunction, \\saltm constructs a Source-level Abstract Logic Tree (\\salt) from\nassembly code to approximate the logic structure of high-level language. It\nthen fine-tunes an LLM using the reconstructed \\salt to generate decompiled\ncode. Finally, the output is refined through error correction and symbol\nrecovery to improve readability and correctness. We compare \\saltm to three\ncategories of baselines (general-purpose LLMs, commercial decompilers, and\ndecompilation methods) using three well-known datasets (Decompile-Eval, MBPP,\nExebench). Our experimental results demonstrate that \\saltm is highly effective\nin recovering the logic of the source code, significantly outperforming\nstate-of-the-art methods (e.g., 70.4\\% TCP rate on Decompile-Eval with a 10.6\\%\nimprovement). The results further validate its robustness against four commonly\nused obfuscation techniques. Additionally, analyses of real-world software and\na user study confirm that our decompiled output offers superior assistance to\nhuman analysts in comprehending binary functions."}
{"id": "2509.15015", "pdf": "https://arxiv.org/pdf/2509.15015", "abs": "https://arxiv.org/abs/2509.15015", "authors": ["Harrison Oates", "Hyeonggeun Yun", "Nikhila Gurusinghe"], "title": "Theorem Provers: One Size Fits All?", "categories": ["cs.LO", "cs.PL"], "comment": null, "summary": "Theorem provers are important tools for people working in formal\nverification. There are a myriad of interactive systems available today, with\nvarying features and approaches motivating their development. These design\nchoices impact their usability, alongside the problem domain in which they are\nemployed. We test-drive two such provers, Coq and Idris2, by proving the\ncorrectness of insertion sort, before providing a qualitative evaluation of\ntheir performance. We then compare their community and library support. This\nwork helps users to make an informed choice of system, and highlight approaches\nin other systems that developers might find useful."}
{"id": "2509.15074", "pdf": "https://arxiv.org/pdf/2509.15074", "abs": "https://arxiv.org/abs/2509.15074", "authors": ["Dominik Gei√üler", "Tobias Winkler"], "title": "Weighted Automata for Exact Inference in Discrete Probabilistic Programs", "categories": ["cs.FL", "cs.PL"], "comment": null, "summary": "In probabilistic programming, the inference problem asks to determine a\nprogram's posterior distribution conditioned on its \"observe\" instructions.\nInference is challenging, especially when exact rather than approximate results\nare required. Inspired by recent work on probability generating functions\n(PGFs), we propose encoding distributions on $\\mathbb{N}^k$ as weighted\nautomata over a commutative alphabet with $k$ symbols. Based on this, we map\nthe semantics of various imperative programming statements to\nautomata-theoretic constructions. For a rich class of programs, this results in\nan effective translation from prior to posterior distribution, both encoded as\nautomata. We prove that our approach is sound with respect to a standard\noperational program semantics."}
{"id": "2509.15150", "pdf": "https://arxiv.org/pdf/2509.15150", "abs": "https://arxiv.org/abs/2509.15150", "authors": ["Federico Bruzzone", "Walter Cazzola", "Luca Favalli"], "title": "Code Less to Code More: Streamlining Language Server Protocol and Type System Development for Language Families", "categories": ["cs.SE", "cs.PL", "D.2.6; D.3.3; D.2.3; D.2.5; D.2.13"], "comment": "34 pages, 10 figures, Journal of Systems and Software, June 2025, for\n  the replication package, see https://doi.org/10.5281/zenodo.15276991", "summary": "Developing editing support for $L$ languages in $E$ editors is complex and\ntime-consuming. Some languages do not provide dedicated editors, while others\noffer a single native editor. The $\\textit{language server protocol}$ (LSP)\nreduces the language-editor combinations $L \\times E$ to $L + E$, where a\nsingle language server communicates with editors via LSP plugins. However,\noverlapping implementations of linguistic components remain an issue. Existing\nlanguage workbenches struggle with modularity, reusability, and leveraging type\nsystems for language server generation. In this work, we propose: (i) Typelang,\na family of domain-specific languages for modular, composable, and reusable\ntype system implementation, (ii) a modular language server generation process,\nproducing servers for languages built in a modular workbench, (iii) the\nvariant-oriented programming paradigm and a cross-artifact coordination layer\nto manage interdependent software variants, and (iv) an LSP plugin generator,\nreducing $E$ to $1$ by automating plugin creation for multiple editors. To\nsimplify editing support for language families, each language artifact\nintegrates its own Typelang variant, used to generate language servers. This\nreduces combinations to $T \\times 1$, where $T = L$ represents the number of\ntype systems. Further reuse of language artifacts across languages lowers this\nto $N \\times 1$, where $N << T$, representing unique type systems. We implement\nTypelang in Neverlang, generating language servers for each artifact and LSP\nplugins for three editors. Empirical evaluation shows a 93.48% reduction in\ncharacters needed for type system implementation and 100% automation of LSP\nplugin generation, significantly lowering effort for editing support in\nlanguage families, especially when artifacts are reused."}
