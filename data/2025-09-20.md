<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.SE](#cs.SE) [Total: 4]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.CL](#cs.CL) [Total: 1]
- [cs.FL](#cs.FL) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [DeliverC: Teaching Pointers through GenAI-Powered Game-Based Learning](https://arxiv.org/abs/2509.14496)
*Wyatt Petula,Anushcka Joshi,Peggy Tu,Amrutha Somasundar,Suman Saha*

Main category: cs.PL

TL;DR: DeliverC是一个集成GPT-4-mini的GenAI增强游戏，为C语言指针学习提供实时个性化提示和挑战生成。研究表明它能提升学生信心和反思能力，但AI反馈质量仍需改进。


<details>
  <summary>Details</summary>
Motivation: 解决编程教育中复杂主题（如C指针）缺乏实时自适应支持工具的问题，探索GenAI与游戏化学习结合在传统挑战性编程领域的潜力。

Method: 开发DeliverC游戏系统，集成GPT-4-mini提供个性化提示和动态生成指针相关挑战。通过25名本科生的试点研究，收集游戏数据和15项问卷调查（涵盖动机、自我效能、元认知和反馈质量等维度）。

Result: 大多数学生使用后感到更自信和善于反思，错误率随脚手架式关卡进展而下降。但参与度随任务难度增加而降低，部分学生反馈AI生成的提示不够清晰明确。

Conclusion: DeliverC能够增强系统编程学习的参与度和理解力，证明了GenAI与游戏化学习结合支持个性化交互式练习的潜力，但AI生成反馈的质量仍需进一步优化。

Abstract: While game-based learning is widely used in programming education, few tools
offer adaptive, real-time support for complex topics, such as C pointers. We
present DeliverC, a GenAI-enhanced game that integrates GPT-4-mini to provide
personalized hints and generate pointer-related challenges on the fly. In a
pilot study involving 25 undergraduate students, we investigated the impact of
the system on learning through gameplay data and a 15-item survey that covered
constructs such as motivation, self-efficacy, metacognition, and feedback
quality. Results show that most students felt more confident and reflective
after using the tool, and error rates decreased as students progressed through
scaffolded levels. However, participation decreased with task difficulty, and
some students reported receiving unclear or vague feedback. These findings
suggest that DeliverC can enhance engagement and understanding in systems
programming, although refinement in AI-generated feedback is still needed. Our
study highlights the potential of combining GenAI with game-based learning to
support personalized and interactive practice in traditionally challenging
programming domains.

</details>


### [2] [Refinement-Types Driven Development: A study](https://arxiv.org/abs/2509.15005)
*Facundo Domínguez,Arnaud Spiwack*

Main category: cs.PL

TL;DR: 这篇论文推广SMT求解器在普通编程中的应用，通过精细类型和编译器集成来提升程序组合能力，并为Liquid Haskell开发了有限映射理论原型。


<details>
  <summary>Details</summary>
Motivation: 诱导SMT求解器超越传统形式方法的限制，应用于普通编程任务，让编程更简单和愉快。

Method: 将SMT求解器集成到编译器静态检查中，利用精细类型（如Liquid Haskell）来增强类型检查器的能力，并通过处理绑定作用域的案例研究进行验证。

Result: 证明了SMT求解器在普通编程任务中的可行性，并为Liquid Haskell开发了支持有限映射的原型实现。

Conclusion: 精细类型和SMT求解器的结合有助于简化普通编程，为更广泛的应用推广了技术基础。

Abstract: This paper advocates for the broader application of SMT solvers in everyday
programming, challenging the conventional wisdom that these tools are solely
for formal methods and verification. We claim that SMT solvers, when seamlessly
integrated into a compiler's static checks, significantly enhance the
capabilities of ordinary type checkers in program composition. Specifically, we
argue that refinement types, as embodied by Liquid Haskell, enable the use of
SMT solvers in mundane programming tasks. Through a case study on handling
binder scopes in compilers, we envision a future where ordinary programming is
made simpler and more enjoyable with the aid of refinement types and SMT
solvers. As a secondary contribution, we present a prototype implementation of
a theory of finite maps for Liquid Haskell's solver, developed to support our
case study.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [3] [A Taxonomy of Prompt Defects in LLM Systems](https://arxiv.org/abs/2509.14404)
*Haoye Tian,Chong Wang,BoYang Yang,Lyuye Zhang,Yang Liu*

Main category: cs.SE

TL;DR: 这篇论文系统性调查了大语言模型提示缺陷，提出了六维度分类法，并针对每种缺陷提供了减轻策略。


<details>
  <summary>Details</summary>
Motivation: 提示设计目前主要靠经验，小错误可能导致不可靠、不安全或效率低下的行为，需要系统性的工程化方法来确保LLM驱动系统的可靠性。

Method: 采用软件工程原理，将提示缺陷沿六个维度进行系统分类：规范与意图、输入与内容、结构与格式、上下文与记忆、性能与效率、可维护性与工程化，并针对每种缺陷提供具体例子和根因分析。

Result: 构建了一个完整的提示缺陷分类法，涵盖了各种常见的提示失效方式，并为每种缺陷提炼了减轻策略，包括提示工程模式、自动匙杆、测试框架和评估体系。

Conclusion: 提出了主要的提示缺陷分类法和应对策略，并呼吁更严谨的工程化方法来确保LLM驱动系统的可靠性，持续研究挑战仍需解决。

Abstract: Large Language Models (LLMs) have become key components of modern software,
with prompts acting as their de-facto programming interface. However, prompt
design remains largely empirical and small mistakes can cascade into
unreliable, insecure, or inefficient behavior. This paper presents the first
systematic survey and taxonomy of prompt defects, recurring ways that prompts
fail to elicit their intended behavior from LLMs. We organize defects along six
dimensions: (1) Specification and Intent, (2) Input and Content, (3) Structure
and Formatting, (4) Context and Memory, (5) Performance and Efficiency, and (6)
Maintainability and Engineering. Each dimension is refined into fine-grained
subtypes, illustrated with concrete examples and root cause analysis. Grounded
in software engineering principles, we show how these defects surface in real
development workflows and examine their downstream effects. For every subtype,
we distill mitigation strategies that span emerging prompt engineering
patterns, automated guardrails, testing harnesses, and evaluation frameworks.
We then summarize these strategies in a master taxonomy that links defect,
impact, and remedy. We conclude with open research challenges and a call for
rigorous engineering-oriented methodologies to ensure that LLM-driven systems
are dependable by design.

</details>


### [4] [Automating Modelica Module Generation Using Large Language Models: A Case Study on Building Control Description Language](https://arxiv.org/abs/2509.14623)
*Hanlong Wan,Xing Lu,Yan Chen,Karthik Devaprasad,Laura Hinkle*

Main category: cs.SE

TL;DR: 本文研究使用大语言模型自动生成Modelica控制模块，GPT 4o零械模式失败，Claude Sonnet 4在精心设计提示下达到83%成功率，平均开发时间缩短40-60%。


<details>
  <summary>Details</summary>
Motivation: Modelica控制模块开发苦糕且需专业知识，需要自动化工具来提高效率。

Method: 开发结构化工作流，结合标准化提示架构、库知识基础、OpenModelica自动编译和人在循环评估。

Result: Claude Sonnet 4在控制模块上达到83%成功率，失败输出需中等人工修复，平均开发时间从10-20小时降至4-6小时。

Conclusion: LLM辅助Modelica生成有潜力但有限制，需要进一步研究事前验证、更强的基础和闭环评估。

Abstract: Dynamic energy systems and controls require advanced modeling frameworks to
design and test supervisory and fault tolerant strategies. Modelica is a widely
used equation based language, but developing control modules is labor intensive
and requires specialized expertise. This paper examines the use of large
language models (LLMs) to automate the generation of Control Description
Language modules in the Building Modelica Library as a case study. We developed
a structured workflow that combines standardized prompt scaffolds, library
aware grounding, automated compilation with OpenModelica, and human in the loop
evaluation. Experiments were carried out on four basic logic tasks (And, Or,
Not, and Switch) and five control modules (chiller enable/disable, bypass valve
control, cooling tower fan speed, plant requests, and relief damper control).
The results showed that GPT 4o failed to produce executable Modelica code in
zero shot mode, while Claude Sonnet 4 achieved up to full success for basic
logic blocks with carefully engineered prompts. For control modules, success
rates reached 83 percent, and failed outputs required medium level human repair
(estimated one to eight hours). Retrieval augmented generation often produced
mismatches in module selection (for example, And retrieved as Or), while a
deterministic hard rule search strategy avoided these errors. Human evaluation
also outperformed AI evaluation, since current LLMs cannot assess simulation
results or validate behavioral correctness. Despite these limitations, the LLM
assisted workflow reduced the average development time from 10 to 20 hours down
to 4 to 6 hours per module, corresponding to 40 to 60 percent time savings.
These results highlight both the potential and current limitations of LLM
assisted Modelica generation, and point to future research in pre simulation
validation, stronger grounding, and closed loop evaluation.

</details>


### [5] [SALT4Decompile: Inferring Source-level Abstract Logic Tree for LLM-Based Binary Decompilation](https://arxiv.org/abs/2509.14646)
*Yongpan Wang,Xin Xu,Xiaojie Zhu,Xiaodong Gu,Beijun Shen*

Main category: cs.SE

TL;DR: SALT提出了一种新颖的二进制反编译方法，通过构建源级抽象逻辑树(SALT)来抽象二进制和源代码之间的稳定逻辑特征，显著提升了LLM在语义恢复方面的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的反编译方法将汇编代码视为线性指令序列，忽略了二进制文件固有的任意跳转模式和孤立数据段，这严重阻碍了从汇编代码正确推断源代码语义的能力。

Method: SALT方法首先从汇编代码构建源级抽象逻辑树(SALT)来近似高级语言的逻辑结构，然后使用重建的SALT微调LLM生成反编译代码，最后通过错误校正和符号恢复来改进输出的可读性和正确性。

Result: 在三个知名数据集上的实验表明，SALT在恢复源代码逻辑方面非常有效，显著优于最先进的方法（如在Decompile-Eval上达到70.4%的TCP率，提升10.6%），并且对四种常用混淆技术具有鲁棒性。

Conclusion: SALT通过抽象二进制级操作到高级逻辑框架的方法，有效提升了二进制反编译的准确性和可读性，为人类分析人员理解二进制函数提供了更好的辅助。

Abstract: Decompilation is widely used in reverse engineering to recover high-level
language code from binary executables. While recent approaches leveraging Large
Language Models (LLMs) have shown promising progress, they typically treat
assembly code as a linear sequence of instructions, overlooking arbitrary jump
patterns and isolated data segments inherent to binary files. This limitation
significantly hinders their ability to correctly infer source code semantics
from assembly code. To address this limitation, we propose \saltm, a novel
binary decompilation method that abstracts stable logical features shared
between binary and source code. The core idea of \saltm is to abstract selected
binary-level operations, such as specific jumps, into a high-level logic
framework that better guides LLMs in semantic recovery. Given a binary
function, \saltm constructs a Source-level Abstract Logic Tree (\salt) from
assembly code to approximate the logic structure of high-level language. It
then fine-tunes an LLM using the reconstructed \salt to generate decompiled
code. Finally, the output is refined through error correction and symbol
recovery to improve readability and correctness. We compare \saltm to three
categories of baselines (general-purpose LLMs, commercial decompilers, and
decompilation methods) using three well-known datasets (Decompile-Eval, MBPP,
Exebench). Our experimental results demonstrate that \saltm is highly effective
in recovering the logic of the source code, significantly outperforming
state-of-the-art methods (e.g., 70.4\% TCP rate on Decompile-Eval with a 10.6\%
improvement). The results further validate its robustness against four commonly
used obfuscation techniques. Additionally, analyses of real-world software and
a user study confirm that our decompiled output offers superior assistance to
human analysts in comprehending binary functions.

</details>


### [6] [Code Less to Code More: Streamlining Language Server Protocol and Type System Development for Language Families](https://arxiv.org/abs/2509.15150)
*Federico Bruzzone,Walter Cazzola,Luca Favalli*

Main category: cs.SE

TL;DR: 提出Typelang语言家族和模块化语言服务器生成方法，显著减少语言编辑器支持开发工作量，实现93.48%的类型系统实现代码减少和100%的LSP插件自动化生成


<details>
  <summary>Details</summary>
Motivation: 解决多语言多编辑器支持开发复杂度高的问题，现有语言工作台在模块化、可重用性和利用类型系统生成语言服务器方面存在不足

Method: 开发Typelang DSL家族用于模块化类型系统实现；提出模块化语言服务器生成过程；引入变体导向编程范式和跨工件协调层；开发LSP插件生成器

Result: 在Neverlang中实现Typelang，为三个编辑器生成LSP插件，类型系统实现代码减少93.48%，LSP插件生成100%自动化

Conclusion: 该方法显著降低了语言家族编辑支持的工作量，特别是在工件重用的情况下，将语言编辑器组合从L×E减少到N×1（N远小于语言数量T）

Abstract: Developing editing support for $L$ languages in $E$ editors is complex and
time-consuming. Some languages do not provide dedicated editors, while others
offer a single native editor. The $\textit{language server protocol}$ (LSP)
reduces the language-editor combinations $L \times E$ to $L + E$, where a
single language server communicates with editors via LSP plugins. However,
overlapping implementations of linguistic components remain an issue. Existing
language workbenches struggle with modularity, reusability, and leveraging type
systems for language server generation. In this work, we propose: (i) Typelang,
a family of domain-specific languages for modular, composable, and reusable
type system implementation, (ii) a modular language server generation process,
producing servers for languages built in a modular workbench, (iii) the
variant-oriented programming paradigm and a cross-artifact coordination layer
to manage interdependent software variants, and (iv) an LSP plugin generator,
reducing $E$ to $1$ by automating plugin creation for multiple editors. To
simplify editing support for language families, each language artifact
integrates its own Typelang variant, used to generate language servers. This
reduces combinations to $T \times 1$, where $T = L$ represents the number of
type systems. Further reuse of language artifacts across languages lowers this
to $N \times 1$, where $N << T$, representing unique type systems. We implement
Typelang in Neverlang, generating language servers for each artifact and LSP
plugins for three editors. Empirical evaluation shows a 93.48% reduction in
characters needed for type system implementation and 100% automation of LSP
plugin generation, significantly lowering effort for editing support in
language families, especially when artifacts are reused.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [7] [Theorem Provers: One Size Fits All?](https://arxiv.org/abs/2509.15015)
*Harrison Oates,Hyeonggeun Yun,Nikhila Gurusinghe*

Main category: cs.LO

TL;DR: 通过在Coq和Idris2中证明插入排序的正确性，对比了这两种定理证明器的性能、社区支持和库支持，以帮助用户做出信息化的系统选择


<details>
  <summary>Details</summary>
Motivation: 定理证明器在形式验证中致关重要，但不同系统的设计选择影响其易用性。需要通过实际案例来评估和比较不同定理证明器的表现

Method: 使用Coq和Idris2两种定理证明器实际证明插入排序算法的正确性，然后进行定性评估和社区库支持对比

Result: 获得了两种定理证明器在实际证明任务中的表现数据，包括开发效率、语言特性、库支持等方面的详细对比信息

Conclusion: 研究结果能够帮助用户根据具体需求选择适合的定理证明器，同时为开发者提供了可以借鉴的设计思路和方法

Abstract: Theorem provers are important tools for people working in formal
verification. There are a myriad of interactive systems available today, with
varying features and approaches motivating their development. These design
choices impact their usability, alongside the problem domain in which they are
employed. We test-drive two such provers, Coq and Idris2, by proving the
correctness of insertion sort, before providing a qualitative evaluation of
their performance. We then compare their community and library support. This
work helps users to make an informed choice of system, and highlight approaches
in other systems that developers might find useful.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [8] [SWE-QA: Can Language Models Answer Repository-level Code Questions?](https://arxiv.org/abs/2509.14635)
*Weihan Peng,Yuling Shi,Yuhang Wang,Xinyun Zhang,Beijun Shen,Xiaodong Gu*

Main category: cs.CL

TL;DR: SWE-QA是一个仓库级代码问答基准数据集，包含576个高质量问答对，涵盖意图理解、跨文件推理和多跳依赖分析等类别，旨在促进真实代码环境中的自动化问答系统研究。


<details>
  <summary>Details</summary>
Motivation: 现有基准如CoSQA和CodeQA主要关注小型自包含代码片段，无法捕捉真实软件仓库的复杂性，需要理解多文件、软件架构和长距离代码依赖关系。

Method: 从11个流行仓库的77,100个GitHub问题中提取开发者自然问题，开发两级分类法，手动策划和验证问题并收集答案，构建SWE-QA数据集和SWE-QA-Agent代理框架。

Result: 实验评估了6个先进LLM在不同上下文增强策略下的表现，结果显示LLM特别是SWE-QA-Agent框架在仓库级问答方面具有潜力。

Conclusion: SWE-QA基准为仓库级代码问答研究提供了重要资源，同时揭示了现有挑战并指明了未来研究方向。

Abstract: Understanding and reasoning about entire software repositories is an
essential capability for intelligent software engineering tools. While existing
benchmarks such as CoSQA and CodeQA have advanced the field, they predominantly
focus on small, self-contained code snippets. These setups fail to capture the
complexity of real-world repositories, where effective understanding and
reasoning often require navigating multiple files, understanding software
architecture, and grounding answers in long-range code dependencies. In this
paper, we present SWE-QA, a repository-level code question answering (QA)
benchmark designed to facilitate research on automated QA systems in realistic
code environments. SWE-QA involves 576 high-quality question-answer pairs
spanning diverse categories, including intention understanding, cross-file
reasoning, and multi-hop dependency analysis. To construct SWE-QA, we first
crawled 77,100 GitHub issues from 11 popular repositories. Based on an analysis
of naturally occurring developer questions extracted from these issues, we
developed a two-level taxonomy of repository-level questions and constructed a
set of seed questions for each category. For each category, we manually curated
and validated questions and collected their corresponding answers. As a
prototype application, we further develop SWE-QA-Agent, an agentic framework in
which LLM agents reason and act to find answers automatically. We evaluate six
advanced LLMs on SWE-QA under various context augmentation strategies.
Experimental results highlight the promise of LLMs, particularly our
SWE-QA-Agent framework, in addressing repository-level QA, while also revealing
open challenges and pointing to future research directions.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [9] [Weighted Automata for Exact Inference in Discrete Probabilistic Programs](https://arxiv.org/abs/2509.15074)
*Dominik Geißler,Tobias Winkler*

Main category: cs.FL

TL;DR: 将概率程序推理问题转化为加权自动机构造，通过自动机理论方法实现从先验分布到后验分布的有效转换


<details>
  <summary>Details</summary>
Motivation: 概率编程中的推理问题需要确定程序在观测指令条件下的后验分布，精确推理尤其具有挑战性

Method: 将N^k上的分布编码为k符号交换字母表上的加权自动机，将各种命令式编程语句的语义映射到自动机构造

Result: 为丰富程序类实现了从先验到后验分布的有效翻译，两者都编码为自动机

Conclusion: 该方法相对于标准操作程序语义是可靠的

Abstract: In probabilistic programming, the inference problem asks to determine a
program's posterior distribution conditioned on its "observe" instructions.
Inference is challenging, especially when exact rather than approximate results
are required. Inspired by recent work on probability generating functions
(PGFs), we propose encoding distributions on $\mathbb{N}^k$ as weighted
automata over a commutative alphabet with $k$ symbols. Based on this, we map
the semantics of various imperative programming statements to
automata-theoretic constructions. For a rich class of programs, this results in
an effective translation from prior to posterior distribution, both encoded as
automata. We prove that our approach is sound with respect to a standard
operational program semantics.

</details>
