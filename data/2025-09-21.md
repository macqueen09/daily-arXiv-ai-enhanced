<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.SE](#cs.SE) [Total: 4]
- [cs.CL](#cs.CL) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.FL](#cs.FL) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [DeliverC: Teaching Pointers through GenAI-Powered Game-Based Learning](https://arxiv.org/abs/2509.14496)
*Wyatt Petula,Anushcka Joshi,Peggy Tu,Amrutha Somasundar,Suman Saha*

Main category: cs.PL

TL;DR: DeliverC是一个集成GPT-4-mini的GenAI增强游戏，为C语言指针学习提供实时个性化提示和挑战生成。研究表明能提升学生信心和反思能力，但AI反馈质量仍需改进。


<details>
  <summary>Details</summary>
Motivation: 传统游戏化编程教育缺乏对复杂主题（如C指针）的实时自适应支持，需要开发能够提供个性化学习体验的工具。

Method: 开发DeliverC游戏，集成GPT-4-mini生成个性化提示和指针相关挑战。通过25名本科生的试点研究，收集游戏数据和15项问卷调查（动机、自我效能、元认知、反馈质量等）。

Result: 大多数学生使用后信心和反思能力提升，错误率随脚手架式关卡进展而下降。但参与度随任务难度增加而降低，部分学生反馈AI生成的提示不够清晰。

Conclusion: DeliverC能增强系统编程学习的参与度和理解，但需要改进AI生成反馈的质量。研究展示了GenAI与游戏化学习结合在传统挑战性编程领域的个性化互动实践潜力。

Abstract: While game-based learning is widely used in programming education, few tools
offer adaptive, real-time support for complex topics, such as C pointers. We
present DeliverC, a GenAI-enhanced game that integrates GPT-4-mini to provide
personalized hints and generate pointer-related challenges on the fly. In a
pilot study involving 25 undergraduate students, we investigated the impact of
the system on learning through gameplay data and a 15-item survey that covered
constructs such as motivation, self-efficacy, metacognition, and feedback
quality. Results show that most students felt more confident and reflective
after using the tool, and error rates decreased as students progressed through
scaffolded levels. However, participation decreased with task difficulty, and
some students reported receiving unclear or vague feedback. These findings
suggest that DeliverC can enhance engagement and understanding in systems
programming, although refinement in AI-generated feedback is still needed. Our
study highlights the potential of combining GenAI with game-based learning to
support personalized and interactive practice in traditionally challenging
programming domains.

</details>


### [2] [Refinement-Types Driven Development: A study](https://arxiv.org/abs/2509.15005)
*Facundo Domínguez,Arnaud Spiwack*

Main category: cs.PL

TL;DR: 推广SMT求解器在普通编程中的应用，通过精细类型与编译器静态检查整合，提升程序组合能力


<details>
  <summary>Details</summary>
Motivation: 质疑SMT求解器仅用于形式方法和验证的传统观念，推动其在普通编程任务中的应用

Method: 使用Liquid Haskell体现的精细类型，将SMT求解器与编译器静态检查无缝集成，并通过处理绑定子作用域的案例研究进行验证

Result: 展现了精细类型和SMT求解器能够简化普通编程任务，并为Liquid Haskell开发了有限映射理论的原型实现

Conclusion: 精细类型和SMT求解器有潜力让普通编程更简单、更愉快，为日常编程带来新可能性

Abstract: This paper advocates for the broader application of SMT solvers in everyday
programming, challenging the conventional wisdom that these tools are solely
for formal methods and verification. We claim that SMT solvers, when seamlessly
integrated into a compiler's static checks, significantly enhance the
capabilities of ordinary type checkers in program composition. Specifically, we
argue that refinement types, as embodied by Liquid Haskell, enable the use of
SMT solvers in mundane programming tasks. Through a case study on handling
binder scopes in compilers, we envision a future where ordinary programming is
made simpler and more enjoyable with the aid of refinement types and SMT
solvers. As a secondary contribution, we present a prototype implementation of
a theory of finite maps for Liquid Haskell's solver, developed to support our
case study.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [3] [A Taxonomy of Prompt Defects in LLM Systems](https://arxiv.org/abs/2509.14404)
*Haoye Tian,Chong Wang,BoYang Yang,Lyuye Zhang,Yang Liu*

Main category: cs.SE

TL;DR: 本文首次系统性地调查和分类了提示缺陷，提出了包含6个维度的缺陷分类框架，并为每种缺陷类型提供了具体的缓解策略和工程方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型已成为现代软件的关键组件，但提示设计仍主要依赖经验，小错误可能导致不可靠、不安全或低效的行为，需要系统化的缺陷分类和缓解方法。

Method: 通过系统调查和分类法，将提示缺陷组织为6个维度：规范和意图、输入和内容、结构和格式、上下文和记忆、性能和效率、可维护性和工程化，每个维度细分为具体子类型。

Result: 建立了全面的提示缺陷分类框架，提供了具体的缺陷示例、根本原因分析和缓解策略，包括提示工程模式、自动化防护、测试框架和评估方法。

Conclusion: 需要以工程为导向的严谨方法来确保LLM驱动系统的可靠性，提出了开放研究挑战和面向设计的可靠性保障方法。

Abstract: Large Language Models (LLMs) have become key components of modern software,
with prompts acting as their de-facto programming interface. However, prompt
design remains largely empirical and small mistakes can cascade into
unreliable, insecure, or inefficient behavior. This paper presents the first
systematic survey and taxonomy of prompt defects, recurring ways that prompts
fail to elicit their intended behavior from LLMs. We organize defects along six
dimensions: (1) Specification and Intent, (2) Input and Content, (3) Structure
and Formatting, (4) Context and Memory, (5) Performance and Efficiency, and (6)
Maintainability and Engineering. Each dimension is refined into fine-grained
subtypes, illustrated with concrete examples and root cause analysis. Grounded
in software engineering principles, we show how these defects surface in real
development workflows and examine their downstream effects. For every subtype,
we distill mitigation strategies that span emerging prompt engineering
patterns, automated guardrails, testing harnesses, and evaluation frameworks.
We then summarize these strategies in a master taxonomy that links defect,
impact, and remedy. We conclude with open research challenges and a call for
rigorous engineering-oriented methodologies to ensure that LLM-driven systems
are dependable by design.

</details>


### [4] [Automating Modelica Module Generation Using Large Language Models: A Case Study on Building Control Description Language](https://arxiv.org/abs/2509.14623)
*Hanlong Wan,Xing Lu,Yan Chen,Karthik Devaprasad,Laura Hinkle*

Main category: cs.SE

TL;DR: 本文研究了大语言模型在自动生成Modelica控制模块中的应用，通过结构化工作流提高了开发效率，节省40-60%时间，但仍需要人工修复和验证。


<details>
  <summary>Details</summary>
Motivation: 解决Modelica控制模块开发劳动密集且需要专业知识的问题，探索大语言模型在自动化代码生成中的应用潜力。

Method: 开发结构化工作流，包括标准化提示架构、库知识基础、OpenModelica自动编译和人工评估，对四种基础逻辑任务和五种控制模块进行实验。

Result: GPT 4o零检查失败，Claude Sonnet 4通过精心设计的提示在基础逻辑块中达到完全成功，控制模块成功率达83%，平均开发时间从10-20小时降至4-6小时。

Conclusion: LLM辅助工作流显示了显著的效率提升潜力，但当前仍需要人工干预和验证，未来需要在预模拟验证、更强的基础和闭环评估方面进一步研究。

Abstract: Dynamic energy systems and controls require advanced modeling frameworks to
design and test supervisory and fault tolerant strategies. Modelica is a widely
used equation based language, but developing control modules is labor intensive
and requires specialized expertise. This paper examines the use of large
language models (LLMs) to automate the generation of Control Description
Language modules in the Building Modelica Library as a case study. We developed
a structured workflow that combines standardized prompt scaffolds, library
aware grounding, automated compilation with OpenModelica, and human in the loop
evaluation. Experiments were carried out on four basic logic tasks (And, Or,
Not, and Switch) and five control modules (chiller enable/disable, bypass valve
control, cooling tower fan speed, plant requests, and relief damper control).
The results showed that GPT 4o failed to produce executable Modelica code in
zero shot mode, while Claude Sonnet 4 achieved up to full success for basic
logic blocks with carefully engineered prompts. For control modules, success
rates reached 83 percent, and failed outputs required medium level human repair
(estimated one to eight hours). Retrieval augmented generation often produced
mismatches in module selection (for example, And retrieved as Or), while a
deterministic hard rule search strategy avoided these errors. Human evaluation
also outperformed AI evaluation, since current LLMs cannot assess simulation
results or validate behavioral correctness. Despite these limitations, the LLM
assisted workflow reduced the average development time from 10 to 20 hours down
to 4 to 6 hours per module, corresponding to 40 to 60 percent time savings.
These results highlight both the potential and current limitations of LLM
assisted Modelica generation, and point to future research in pre simulation
validation, stronger grounding, and closed loop evaluation.

</details>


### [5] [SALT4Decompile: Inferring Source-level Abstract Logic Tree for LLM-Based Binary Decompilation](https://arxiv.org/abs/2509.14646)
*Yongpan Wang,Xin Xu,Xiaojie Zhu,Xiaodong Gu,Beijun Shen*

Main category: cs.SE

TL;DR: SALT提出了一种新颖的二进制反编译方法，通过构建源级抽象逻辑树(SALT)来抽象二进制和源代码之间的稳定逻辑特征，显著提升了LLM在语义恢复方面的性能


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的反编译方法将汇编代码视为线性指令序列，忽略了二进制文件固有的任意跳转模式和孤立数据段，这严重阻碍了从汇编代码正确推断源代码语义的能力

Method: SALT方法首先从汇编代码构建源级抽象逻辑树(SALT)来近似高级语言的逻辑结构，然后使用重构的SALT微调LLM生成反编译代码，最后通过错误校正和符号恢复来改进输出的可读性和正确性

Result: 在三个知名数据集(Decompile-Eval、MBPP、Exebench)上的实验表明，SALT在恢复源代码逻辑方面非常有效，显著优于最先进的方法(如在Decompile-Eval上达到70.4%的TCP率，提升10.6%)，并对四种常用混淆技术表现出鲁棒性

Conclusion: SALT通过抽象二进制级操作到高级逻辑框架的方法，有效解决了现有LLM反编译方法的局限性，为人类分析人员理解二进制函数提供了优越的帮助

Abstract: Decompilation is widely used in reverse engineering to recover high-level
language code from binary executables. While recent approaches leveraging Large
Language Models (LLMs) have shown promising progress, they typically treat
assembly code as a linear sequence of instructions, overlooking arbitrary jump
patterns and isolated data segments inherent to binary files. This limitation
significantly hinders their ability to correctly infer source code semantics
from assembly code. To address this limitation, we propose \saltm, a novel
binary decompilation method that abstracts stable logical features shared
between binary and source code. The core idea of \saltm is to abstract selected
binary-level operations, such as specific jumps, into a high-level logic
framework that better guides LLMs in semantic recovery. Given a binary
function, \saltm constructs a Source-level Abstract Logic Tree (\salt) from
assembly code to approximate the logic structure of high-level language. It
then fine-tunes an LLM using the reconstructed \salt to generate decompiled
code. Finally, the output is refined through error correction and symbol
recovery to improve readability and correctness. We compare \saltm to three
categories of baselines (general-purpose LLMs, commercial decompilers, and
decompilation methods) using three well-known datasets (Decompile-Eval, MBPP,
Exebench). Our experimental results demonstrate that \saltm is highly effective
in recovering the logic of the source code, significantly outperforming
state-of-the-art methods (e.g., 70.4\% TCP rate on Decompile-Eval with a 10.6\%
improvement). The results further validate its robustness against four commonly
used obfuscation techniques. Additionally, analyses of real-world software and
a user study confirm that our decompiled output offers superior assistance to
human analysts in comprehending binary functions.

</details>


### [6] [Code Less to Code More: Streamlining Language Server Protocol and Type System Development for Language Families](https://arxiv.org/abs/2509.15150)
*Federico Bruzzone,Walter Cazzola,Luca Favalli*

Main category: cs.SE

TL;DR: 通过Typelang语言家族和模块化语言服务器生成技术，大幅减少编辑支持开发复杂度，实现了LSP插件全自动生成和类型系统实现代码量的93.48%减少


<details>
  <summary>Details</summary>
Motivation: 解决多语言多编辑器支持开发的复杂性问题，LSP协议虽减少了组合数量，但语言组件的重复实现仍是挑战，现有语言工作台在模块化、重用性和类型系统利用方面存在不足

Method: 提出Typelang基于域特定语言家族，支持模块化、组合和重用的类型系统实现；模块化语言服务器生成流程；变体导向编程范式和跨产品协调层；LSP插件生成器自动为多编辑器创建插件

Result: 在Neverlang中实现Typelang，为每个语言产品生成语言服务器，为三个编辑器生成LSP插件。类型系统实现需要的字符数减少93.48%，LSP插件生成完全自动化(100%)，显著降低了语言家族编辑支持的开发苦工

Conclusion: 该方法通过模块化设计和自动化生成，有效解决了多语言多编辑器支持的复杂性问题，特别是在语言产品重用时效果更为显著，为语言工程领域提供了可扩展的解决方案

Abstract: Developing editing support for $L$ languages in $E$ editors is complex and
time-consuming. Some languages do not provide dedicated editors, while others
offer a single native editor. The $\textit{language server protocol}$ (LSP)
reduces the language-editor combinations $L \times E$ to $L + E$, where a
single language server communicates with editors via LSP plugins. However,
overlapping implementations of linguistic components remain an issue. Existing
language workbenches struggle with modularity, reusability, and leveraging type
systems for language server generation. In this work, we propose: (i) Typelang,
a family of domain-specific languages for modular, composable, and reusable
type system implementation, (ii) a modular language server generation process,
producing servers for languages built in a modular workbench, (iii) the
variant-oriented programming paradigm and a cross-artifact coordination layer
to manage interdependent software variants, and (iv) an LSP plugin generator,
reducing $E$ to $1$ by automating plugin creation for multiple editors. To
simplify editing support for language families, each language artifact
integrates its own Typelang variant, used to generate language servers. This
reduces combinations to $T \times 1$, where $T = L$ represents the number of
type systems. Further reuse of language artifacts across languages lowers this
to $N \times 1$, where $N << T$, representing unique type systems. We implement
Typelang in Neverlang, generating language servers for each artifact and LSP
plugins for three editors. Empirical evaluation shows a 93.48% reduction in
characters needed for type system implementation and 100% automation of LSP
plugin generation, significantly lowering effort for editing support in
language families, especially when artifacts are reused.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [7] [SWE-QA: Can Language Models Answer Repository-level Code Questions?](https://arxiv.org/abs/2509.14635)
*Weihan Peng,Yuling Shi,Yuhang Wang,Xinyun Zhang,Beijun Shen,Xiaodong Gu*

Main category: cs.CL

TL;DR: SWE-QA是一个仓库级代码问答基准，包含576个高质量问答对，涵盖跨文件推理和多跳依赖分析等复杂场景，旨在推动真实代码环境中的自动化问答系统研究。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注小型自包含代码片段，无法捕捉真实软件仓库的复杂性，需要理解多文件、软件架构和长距离代码依赖关系。

Method: 从11个热门仓库的77,100个GitHub问题中提取开发者问题，开发两级分类法，手动构建和验证问答对，并开发SWE-QA-Agent代理框架进行自动推理。

Result: 实验评估了6个先进LLM在不同上下文增强策略下的表现，结果显示LLM特别是SWE-QA-Agent框架在仓库级问答方面具有潜力。

Conclusion: SWE-QA为仓库级代码问答研究提供了重要基准，揭示了现有挑战并为未来研究方向指明了道路。

Abstract: Understanding and reasoning about entire software repositories is an
essential capability for intelligent software engineering tools. While existing
benchmarks such as CoSQA and CodeQA have advanced the field, they predominantly
focus on small, self-contained code snippets. These setups fail to capture the
complexity of real-world repositories, where effective understanding and
reasoning often require navigating multiple files, understanding software
architecture, and grounding answers in long-range code dependencies. In this
paper, we present SWE-QA, a repository-level code question answering (QA)
benchmark designed to facilitate research on automated QA systems in realistic
code environments. SWE-QA involves 576 high-quality question-answer pairs
spanning diverse categories, including intention understanding, cross-file
reasoning, and multi-hop dependency analysis. To construct SWE-QA, we first
crawled 77,100 GitHub issues from 11 popular repositories. Based on an analysis
of naturally occurring developer questions extracted from these issues, we
developed a two-level taxonomy of repository-level questions and constructed a
set of seed questions for each category. For each category, we manually curated
and validated questions and collected their corresponding answers. As a
prototype application, we further develop SWE-QA-Agent, an agentic framework in
which LLM agents reason and act to find answers automatically. We evaluate six
advanced LLMs on SWE-QA under various context augmentation strategies.
Experimental results highlight the promise of LLMs, particularly our
SWE-QA-Agent framework, in addressing repository-level QA, while also revealing
open challenges and pointing to future research directions.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [8] [Theorem Provers: One Size Fits All?](https://arxiv.org/abs/2509.15015)
*Harrison Oates,Hyeonggeun Yun,Nikhila Gurusinghe*

Main category: cs.LO

TL;DR: 通过在Coq和Idris2中实现插入排序的正确性证明，对比了这两种定理证明器的性能、社区支持和库支持，以帮助用户做出信息化的选择。


<details>
  <summary>Details</summary>
Motivation: 定理证明器在形式验证中具有重要作用，但不同系统的设计选择影响其易用性。需要通过实际案例对比不同工具的表现、社区和库支持情况。

Method: 使用Coq和Idris2两种定理证明器实现插入排序算法的正确性证明，并进行定性性能评估，同时比较它们的社区活跃度和库支持情况。

Result: 获得了两种定理证明器在实际证明任务中的表现数据，以及它们社区和库支持方面的对比分析结果。

Conclusion: 该研究为用户选择适合的定理证明器提供了实践基础，同时为开发者突出了可以借鉴的设计方法，有助于推动形式验证工具的发展。

Abstract: Theorem provers are important tools for people working in formal
verification. There are a myriad of interactive systems available today, with
varying features and approaches motivating their development. These design
choices impact their usability, alongside the problem domain in which they are
employed. We test-drive two such provers, Coq and Idris2, by proving the
correctness of insertion sort, before providing a qualitative evaluation of
their performance. We then compare their community and library support. This
work helps users to make an informed choice of system, and highlight approaches
in other systems that developers might find useful.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [9] [Weighted Automata for Exact Inference in Discrete Probabilistic Programs](https://arxiv.org/abs/2509.15074)
*Dominik Geißler,Tobias Winkler*

Main category: cs.FL

TL;DR: 将概率程序中的分布编码为加权自动机，通过自动机理论构造实现从先验分布到后验分布的有效转换


<details>
  <summary>Details</summary>
Motivation: 概率编程中的推理问题需要确定程序在观察指令条件下的后验分布，精确推理具有挑战性

Method: 将N^k上的分布编码为具有k个符号的交换字母表上的加权自动机，将各种命令式编程语句的语义映射到自动机理论构造

Result: 对于丰富的程序类，实现了从先验分布到后验分布的有效转换，两者都编码为自动机

Conclusion: 该方法相对于标准操作程序语义是可靠的

Abstract: In probabilistic programming, the inference problem asks to determine a
program's posterior distribution conditioned on its "observe" instructions.
Inference is challenging, especially when exact rather than approximate results
are required. Inspired by recent work on probability generating functions
(PGFs), we propose encoding distributions on $\mathbb{N}^k$ as weighted
automata over a commutative alphabet with $k$ symbols. Based on this, we map
the semantics of various imperative programming statements to
automata-theoretic constructions. For a rich class of programs, this results in
an effective translation from prior to posterior distribution, both encoded as
automata. We prove that our approach is sound with respect to a standard
operational program semantics.

</details>
