<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 3]
- [cs.ET](#cs.ET) [Total: 2]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]
- [cs.CL](#cs.CL) [Total: 1]
- [physics.app-ph](#physics.app-ph) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [VerilogMonkey: Exploring Parallel Scaling for Automated Verilog Code Generation with LLMs](https://arxiv.org/abs/2509.16246)
*Juxin Niu,Yuxin Du,Dan Niu,Xi Wang,Zhe Jiang,Nan Guan*

Main category: cs.PL

TL;DR: VerilogMonkey是一个关于Verilog自动生成任务中并行扩展的实证研究，发现通过并行采样大量输出可以显著提升LLM性能，且成本效益高


<details>
  <summary>Details</summary>
Motivation: 研究并行扩展在自动化Verilog生成这一未被充分探索任务中的应用效果

Method: 在多个基准测试和主流LLM上，通过并行采样数百个输出来评估性能提升，分析输出随机性对效果的影响

Result: 并行扩展到数百个样本在时间和金钱上都具有成本效益，即使没有额外增强方法也能超越之前基于LLM的Verilog生成结果

Conclusion: 并行扩展是提升LLM在Verilog生成任务中性能的有效策略，输出随机性影响其有效性

Abstract: We present VerilogMonkey, an empirical study of parallel scaling for the
under-explored task of automated Verilog generation. Parallel scaling improves
LLM performance by sampling many outputs in parallel. Across multiple
benchmarks and mainstream LLMs, we find that scaling to hundreds of samples is
cost-effective in both time and money and, even without any additional
enhancements such as post-training or agentic methods, surpasses prior results
on LLM-based Verilog generation. We further dissect why parallel scaling
delivers these gains and show how output randomness in LLMs affects its
effectiveness.

</details>


### [2] [GraphMend: Code Transformations for Fixing Graph Breaks in PyTorch 2](https://arxiv.org/abs/2509.16248)
*Savini Kashmira,Jayanaka Dantanarayana,Thamirawaran Sathiyalogeswaran,Yichao Yuan,Nishil Talati,Krisztian Flautner,Lingjia Tang,Jason Mars*

Main category: cs.PL

TL;DR: GraphMend是一个高级编译器，通过代码转换消除PyTorch 2程序中的FX图断点，提升编译效率和性能


<details>
  <summary>Details</summary>
Motivation: PyTorch 2的TorchDynamo和TorchInductor虽然支持即时图编译，但动态控制流和不支持的Python构造会导致模型被分割成多个FX图，造成频繁回退到eager模式、CPU-GPU同步开销和优化机会减少

Method: 基于Jac编译框架，GraphMend引入两种代码转换技术：消除动态控制流导致的图断点和消除Python I/O函数导致的图断点，在代码执行前进行分析和转换

Result: 在8个Hugging Face模型上评估，GraphMend消除了所有可修复的图断点，6个模型断点降为0，另一个从5降到2。在RTX 3090和A40 GPU上实现最高75%延迟降低和8%端到端吞吐量提升

Conclusion: 高级代码转换是PyTorch动态JIT编译管道的有效补充，显著提升了可用性和性能

Abstract: This paper presents GraphMend, a high-level compiler that eliminates FX graph
breaks in PyTorch 2 programs. Although PyTorch 2 introduced TorchDynamo and
TorchInductor to enable just-in-time graph compilation, unresolved dynamic
control flow and unsupported Python constructs often fragment models into
multiple FX graphs. These fragments force frequent fallbacks to eager mode,
incur costly CPU-to-GPU synchronizations, and reduce optimization
opportunities. GraphMend addresses this limitation by analyzing and
transforming source code before execution. Built on the Jac compilation
framework, GraphMend introduces two code transformations that remove graph
breaks due to dynamic control flow and Python I/O functions. This design allows
PyTorch's compilation pipeline to capture larger, uninterrupted FX graphs
without requiring manual refactoring by developers. Evaluation across eight
Hugging Face models shows that GraphMend removes all fixable graph breaks due
to dynamic control flow and Python I/O functions, driving the break count to 0
in 6 models and reducing it from 5 to 2 in another model. On NVIDIA RTX 3090
and A40 GPUs, GraphMend achieves up to 75% latency reductions and up to 8%
higher end-to-end throughput. These results demonstrate that high-level code
transformation is an effective complement to PyTorch's dynamic JIT compilation
pipeline, substantially improving both usability and performance.

</details>


### [3] [Efficient Linearizability Monitoring](https://arxiv.org/abs/2509.17795)
*Parosh Aziz Abdulla,Samuel Grahn,Bengt Jonsson,Shankaranarayanan Krishna,Om Swostik Mishra*

Main category: cs.PL

TL;DR: 本文重新审视了监控并发栈、队列、集合和多集的线性化问题，提出了更高效的监控算法，并发现了现有工具的证明错误。


<details>
  <summary>Details</summary>
Motivation: 现有的线性化监控算法存在时间复杂度高（立方级）和正确性问题（缺乏证明、证明错误或算法错误），需要开发更高效且正确性可证明的算法。

Method: 针对栈、队列和（多）集合分别设计了时间复杂度为O(n²)、O(n log n)和O(n)的监控算法，其中n是操作数量。对于栈和队列，算法基于数据独立性假设。

Result: 实现了栈和队列算法的工具LiMo，实验评估表明LiMo在效率和可扩展性方面优于现有工具Violin。

Conclusion: 本文提出的新算法在保证正确性的同时显著提高了线性化监控的效率，解决了现有方法中的正确性问题。

Abstract: This paper revisits the fundamental problem of monitoring the linearizability
of concurrent stacks, queues, sets, and multisets. Given a history of a library
implementing one of these abstract data types, the monitoring problem is to
answer whether the given history is linearizable. For stacks, queues, and
(multi)sets, we present monitoring algorithms with complexities
$\mathcal{O}(n^2)$, $\mathcal{O}(n\; log\, n)$, and $\mathcal{O}{(n)}$,
respectively, where $n$ is the number of operations in the input history. For
stacks and queues, our results hold under the standard assumption of {\it
data-independence}, i.e., the behavior of the library is not sensitive to the
actual values stored in the data structure. Past works to solve the same
problems have cubic time complexity and (more seriously) have correctness
issues: they either (i) lack correctness proofs or (ii) the suggested
correctness proofs are erroneous (we present counter-examples), or (iii) have
incorrect algorithms. Our improved complexity results rely on substantially
different algorithms for which we provide detailed proofs of correctness. We
have implemented our stack and queue algorithms in LiMo (Linearizability
Monitor). We evaluate LiMo and compare it with the state-of-the-art tool Violin
-- whose correctness proofs we have found errors in -- which checks for
linearizability violations. Our experimental evaluation confirms that LiMo
outperforms Violin regarding both efficiency and scalability.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [4] [A 200-Line Python Micro-Benchmark Suite for NISQ Circuit Compilers](https://arxiv.org/abs/2509.16205)
*Juhani Merilehto*

Main category: cs.ET

TL;DR: microbench.py是一个约200行的Python脚本，用于自动化收集多个开源量子电路转换器的关键编译器指标，包括门深度、双量子比特门计数、编译时间和内存占用。


<details>
  <summary>Details</summary>
Motivation: 为NISQ编译器研究提供一个快速启动的回归测试工具，自动化评估不同量子编译器的性能指标。

Method: 开发了一个紧凑的Python脚本，包含6个教学电路（3-8量子比特），支持Qiskit、tket、Cirq和Qiskit-Braket提供程序，自动收集关键编译器指标并生成CSV文件和可发布图表。

Result: 整个运行在笔记本电脑上不到3分钟完成，生成了单一CSV文件和可发布图表，能够通过一条命令重现论文中的图表。

Conclusion: 该工具作为NISQ编译器研究的快速启动回归测试工具发布，采用MIT许可证。

Abstract: We present microbench.py, a compact (approx. 200 lines) Python script that
automates the collection of key compiler metrics, i.e., gate depth,
two-qubit-gate count, wall-clock compilation time, and memory footprint, across
multiple open-source quantum circuit transpilers. The suite ships with six
didactic circuits (3 to 8 qubits) implementing fundamental quantum algorithms
and supports Qiskit, tket, Cirq, and the Qiskit-Braket provider; in this paper
we showcase results for Qiskit 0.46 and Braket 1.16. The entire run completes
in under three minutes on a laptop, emits a single CSV plus publisheable plot,
and reproduces the figure here with one command. We release the code under the
MIT licence to serve as a quick-start regression harness for NISQ compiler
research.

</details>


### [5] [PrediPrune: Reducing Verification Overhead in Souper with Machine Learning Driven Pruning](https://arxiv.org/abs/2509.16497)
*Ange-Thierry Ishimwe,Raghuveer Shivakumar,Heewoo Kim,Tamara Lehman,Joseph Izraelevitz*

Main category: cs.ET

TL;DR: PrediPrune是一种随机候选剪枝策略，通过机器学习预测LLVM IR优化候选的有效性，减少SMT求解器的验证负担，显著降低编译时间。


<details>
  <summary>Details</summary>
Motivation: Souper超级优化器的验证过程依赖计算昂贵的SMT求解器，需要探索大量搜索空间，增加了编译工具集成负担。

Method: 使用机器学习技术基于代码特征预测候选有效性，早期剪枝不太可能的候选，结合Dataflow方法形成混合策略。

Result: 与基线相比减少51%编译时间，与仅使用Dataflow相比减少12%编译时间，展示了ML与非ML方法结合的有效性。

Conclusion: PrediPrune提供灵活接口在编译时间和优化机会之间权衡，为编译器优化提供了高效的验证加速方案。

Abstract: Souper is a powerful enumerative superoptimizer that enhances the runtime
performance of programs by optimizing LLVM intermediate representation (IR)
code. However, its verification process, which relies on a computationally
expensive SMT solver to validate optimization candidates, must explore a large
search space. This large search space makes the verification process
particularly expensive, increasing the burden to incorporate Souper into
compilation tools. We propose PrediPrune, a stochastic candidate pruning
strategy that effectively reduces the number of invalid candidates passed to
the SMT solver. By utilizing machine learning techniques to predict the
validity of candidates based on features extracted from the code, PrediPrune
prunes unlikely candidates early, decreasing the verification workload. When
combined with the state-of-the-art approach (Dataflow), PrediPrune decreases
compilation time by 51% compared to the Baseline and by 12% compared to using
only Dataflow, emphasizing the effectiveness of the combined approach that
integrates a purely ML-based method (PrediPrune) with a purely non-ML based
(Dataflow) method. Additionally, PrediPrune offers a flexible interface to
trade-off compilation time and optimization opportunities, allowing end users
to adjust the balance according to their needs.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [6] [Quantum Simulation Programming via Typing](https://arxiv.org/abs/2509.17343)
*Liyi Li,Federico Zahariev,Chandeepa Dissanayake,Jae Swanepoel,Amr Sabry,Mark S. Gordon*

Main category: quant-ph

TL;DR: QBLUE是第一个用于以二次量子化哈密顿量描述量子系统行为的编程语言，通过新颖的类型系统明确定义不同量子系统状态，将量子计算机视为特定类型的量子粒子系统。


<details>
  <summary>Details</summary>
Motivation: 现有量子模拟编译器大多使用Pauli字符串或数字量子电路表示量子粒子系统，这使得物理、化学和生物学领域的用户难以有效编程量子模拟。

Method: 提出QBLUE编程语言，包含新型类型系统来定义量子系统状态，将量子计算机视为量子粒子系统，支持数字和模拟量子计算机的编译。

Result: QBLUE允许用户指定所需的量子粒子系统并在量子计算机上建模系统，类型系统与量子模拟编译兼容。

Conclusion: QBLUE为解决量子模拟编程难题提供了有效解决方案，使非专业用户能够更轻松地进行量子系统建模。

Abstract: Quantum simulations are designed to model quantum systems, and many
compilation frameworks have been developed for executing such simulations on
quantum computers. Most compilers leverage the capabilities of digital and
analog quantum computers by representing quantum particle systems with Pauli
strings or digital quantum circuits, making it challenging for users in
physics, chemistry, and biology to program simulations effectively. QBLUE is
proposed as the first programming language for describing the behaviors of
quantum systems in terms of second quantization Hamiltonians. Within QBLUE, a
novel type system is proposed to clearly define states across different quantum
systems and treat quantum computers as quantum particle systems of specific
types. The type system is compatible with the compilation of quantum
simulations expressed in QBLUE for digital and analog quantum computers. With
QBLUE, users can specify the desired quantum particle system and model the
system on quantum computers.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [7] [Discovering Software Parallelization Points Using Deep Neural Networks](https://arxiv.org/abs/2509.16215)
*Izavan dos S. Correia,Henrique C. T. Santos,Tiago A. E. Ferreira*

Main category: cs.LG

TL;DR: 本研究提出了一种基于深度学习的方法，用于发现编程代码中具有并行化潜力的循环结构。通过遗传算法生成两种类型的代码（可并行化的独立循环和依赖关系不明确的模糊循环），并使用DNN和CNN模型进行分类。


<details>
  <summary>Details</summary>
Motivation: 自动化识别代码中可并行化的循环结构，为软件优化和性能提升提供工具支持。

Method: 开发两种遗传算法代码生成器生成训练数据，使用DNN和CNN深度学习模型进行分类，并进行30次独立运行的统计分析。

Result: CNN模型表现出略高的平均性能，两种模型具有相似的变异性。数据多样性对模型性能至关重要。

Conclusion: 深度学习可用于自动化识别代码中的并行化结构，为软件优化提供了有前景的工具。

Abstract: This study proposes a deep learning-based approach for discovering loops in
programming code according to their potential for parallelization. Two genetic
algorithm-based code generators were developed to produce two distinct types of
code: (i) independent loops, which are parallelizable, and (ii) ambiguous
loops, whose dependencies are unclear, making them impossible to define if the
loop is parallelizable or not. The generated code snippets were tokenized and
preprocessed to ensure a robust dataset. Two deep learning models - a Deep
Neural Network (DNN) and a Convolutional Neural Network (CNN) - were
implemented to perform the classification. Based on 30 independent runs, a
robust statistical analysis was employed to verify the expected performance of
both models, DNN and CNN. The CNN showed a slightly higher mean performance,
but the two models had a similar variability. Experiments with varying dataset
sizes highlighted the importance of data diversity for model performance. These
results demonstrate the feasibility of using deep learning to automate the
identification of parallelizable structures in code, offering a promising tool
for software optimization and performance improvement.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [8] [REAMS: Reasoning Enhanced Algorithm for Maths Solving](https://arxiv.org/abs/2509.16241)
*Eishkaran Singh,Tanav Singh Bajaj,Siddharth Nayak*

Main category: cs.CL

TL;DR: 本文提出了一种基于语言的方法，结合零样本学习和数学推理，用于解决大学级别的复杂数学问题，在MIT、哥伦比亚大学课程和MATH数据集上取得了90.15%的准确率，显著超越了之前81%的基准。


<details>
  <summary>Details</summary>
Motivation: 解决大学级别复杂数学问题（特别是MIT、哥伦比亚大学课程和MATH数据集中的问题）一直是人工智能领域的重大挑战，传统方法在此领域表现不佳，需要更先进的解决方案。

Method: 采用基于语言的方法，结合零样本学习和数学推理，通过集成程序合成技术来减少对大规模训练数据的依赖，同时提高问题解决的准确性。

Result: 该方法在复杂数学问题解决上达到了90.15%的准确率，相比之前81%的基准有显著提升，为自动化数学问题解决设立了新的标准。

Conclusion: 研究结果表明，先进的AI方法在应对复杂数学课程和数据集挑战方面具有巨大潜力，能够有效解决、解释和生成高级数学问题的解决方案。

Abstract: The challenges of solving complex university-level mathematics problems,
particularly those from MIT, and Columbia University courses, and selected
tasks from the MATH dataset, remain a significant obstacle in the field of
artificial intelligence. Conventional methods have consistently fallen short in
this domain, highlighting the need for more advanced approaches. In this paper,
we introduce a language-based solution that leverages zero-shot learning and
mathematical reasoning to effectively solve, explain, and generate solutions
for these advanced math problems. By integrating program synthesis, our method
reduces reliance on large-scale training data while significantly improving
problem-solving accuracy. Our approach achieves an accuracy of 90.15%,
representing a substantial improvement over the previous benchmark of 81% and
setting a new standard in automated mathematical problem-solving. These
findings highlight the significant potential of advanced AI methodologies to
address and overcome the challenges presented by some of the most complex
mathematical courses and datasets.

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [9] [LightCode: Compiling LLM Inference for Photonic-Electronic Systems](https://arxiv.org/abs/2509.16443)
*Ryan Tomich,Zhizhen Zhong,Dirk Englund*

Main category: physics.app-ph

TL;DR: LightCode是一个针对混合光电系统的LLM推理编译器框架和模拟器，通过堆叠图中间表示和约束子图选择优化，在GPT-2和Llama-7B上实现了最高50%的能耗降低和10倍以上的延迟改进。


<details>
  <summary>Details</summary>
Motivation: 随着对低延迟、高能效LLM推理需求的增长，需要结合GPU和新兴光电张量单元(PTU)等专用加速器的异构架构，但现有GPU难以与光电加速器集成，因此需要混合编译策略。

Method: 提出LightCode编译器框架，引入堆叠图中间表示编码张量操作的多种硬件特定实现，将硬件分配建模为约束子图选择问题，基于参数化成本模型优化延迟或能耗。

Result: 在GPT-2和Llama-7B预填充阶段评估显示：光电硬件在最大序列长度下可降低50%能耗；复用和分配策略带来超过10倍的延迟改进；延迟和能耗优化产生不同的硬件映射。

Conclusion: LightCode为将LLM编译到新兴光电加速器提供了一个模块化、基础性的框架和模拟器。

Abstract: The growing demand for low-latency, energy-efficient inference in large
language models (LLMs) has catalyzed interest in heterogeneous architectures.
While GPUs remain dominant, they are poorly suited for integration with
emerging domain-specific accelerators like the Photonic Tensor Units (PTUs),
which offer low-power, high-throughput linear computation. This motivates
hybrid compilation strategies that combine photonic and electronic resources.
We present LightCode, a compiler framework and simulator for mapping LLM
inference workloads across hybrid photonic-electronic systems. LightCode
introduces the Stacked Graph, an intermediate representation that encodes
multiple hardware-specific realizations of each tensor operation. Hardware
assignment is formulated as a constrained subgraph selection problem optimized
for latency or energy under parametric cost models. We evaluate LightCode on
the prefill stage of GPT-2 and Llama-7B showing that under our workload and
hardware assumptions, (i) Photonic hardware reduced energy by up to 50% in our
simulated workloads at maximum sequence length; (ii) multiplexing and
assignment strategy yielded latency improvements exceeding 10x; and (iii)
Optimizing for latency or energy resulted in distinct hardware mappings in our
simulations. LightCode offers a module, foundational framework and simulator
for compiling LLMs to emerging photonic accelerators.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [10] [Gödel Mirror: A Formal System For Contradiction-Driven Recursion](https://arxiv.org/abs/2509.16239)
*Jhet Chan*

Main category: cs.LO

TL;DR: Gödel Mirror是一个在Lean 4中定义的形式系统，将矛盾作为递归结构演化的控制信号，利用自指悖论作为确定性转换，实现悖论的封装和解析。


<details>
  <summary>Details</summary>
Motivation: 受哥德尔自指启发，旨在构建能够将矛盾转化为结构的形式系统，为能够解决内部不一致性的智能体提供形式基础。

Method: 在Lean 4中实现的操作语义，将符号悖论编码为确定性转换，通过受控的非终止循环作为生产性特征。

Result: 证明自指悖论可以被确定性封装并解析为新结构，不会导致逻辑爆炸，形成准一致性推理循环：悖论->封装->重新进入->节点。

Conclusion: 该演算开启了一类新的符号系统，其中矛盾被代谢为结构，为能够解决内部不一致性的智能体提供了形式基础。

Abstract: We introduce the G\"odel Mirror, a formal system defined in Lean 4 that
treats contradiction as a control signal for recursive structural evolution.
  Inspired by G\"odelian self-reference, our system's operational semantics
encode symbolic paradoxes as deterministic transitions. Unlike systems designed
to guarantee normalization, the G\"odel Mirror is a minimal and verifiable
architecture that leverages a controlled, non-terminating loop as a productive
feature.
  Our Lean 4 mechanization proves that self-referential paradoxes are
deterministically encapsulated and resolved into new structures without leading
to logical explosion, yielding a paraconsistent inference loop: Paradox ->
Encapsulate -> Reenter -> Node
  We argue that this calculus opens a new class of symbolic systems in which
contradiction is metabolized into structure, providing a formal basis for
agents capable of resolving internal inconsistencies.

</details>
