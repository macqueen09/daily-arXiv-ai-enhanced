{"id": "2509.20426", "pdf": "https://arxiv.org/pdf/2509.20426", "abs": "https://arxiv.org/abs/2509.20426", "authors": ["Mahmoud Samir Fayed"], "title": "Dual-Language General-Purpose Self-Hosted Visual Language and new Textual Programming Language for Applications", "categories": ["cs.PL", "cs.SE"], "comment": "PhD thesis", "summary": "Most visual programming languages (VPLs) are domain-specific, with few\ngeneral-purpose VPLs like Programming Without Coding Technology (PWCT). These\ngeneral-purpose VPLs are developed using textual programming languages and\nimproving them requires textual programming. In this thesis, we designed and\ndeveloped PWCT2, a dual-language (Arabic/English), general-purpose,\nself-hosting visual programming language. Before doing so, we specifically\ndesigned a textual programming language called Ring for its development. Ring\nis a dynamically typed language with a lightweight implementation, offering\nsyntax customization features. It permits the creation of domain-specific\nlanguages through new features that extend object-oriented programming,\nallowing for specialized languages resembling Cascading Style Sheets (CSS) or\nSupernova language. The Ring Compiler and Virtual Machine are designed using\nthe PWCT visual programming language where the visual implementation is\ncomposed of 18,945 components that generate 24,743 lines of C code, which\nincreases the abstraction level and hides unnecessary details. Using PWCT to\ndevelop Ring allowed us to realize several issues in PWCT, which led to the\ndevelopment of the PWCT2 visual programming language using the Ring textual\nprogramming language. PWCT2 provides approximately 36 times faster code\ngeneration and requires 20 times less storage for visual source files. It also\nallows for the conversion of Ring code into visual code, enabling the creation\nof a self-hosting VPL that can be developed using itself. PWCT2 consists of\napproximately 92,000 lines of Ring code and comes with 394 visual components.\nPWCT2 is distributed to many users through the Steam platform and has received\npositive feedback, On Steam, 1772 users have launched the software, and the\ntotal recorded usage time exceeds 17,000 hours, encouraging further research\nand development.", "AI": {"tldr": "\u5f00\u53d1\u4e86PWCT2\uff0c\u8fd9\u662f\u4e00\u4e2a\u53cc\u8bed\u8a00\uff08\u963f\u62c9\u4f2f\u8bed/\u82f1\u8bed\uff09\u3001\u901a\u7528\u3001\u81ea\u6258\u7ba1\u7684\u53ef\u89c6\u5316\u7f16\u7a0b\u8bed\u8a00\uff0c\u901a\u8fc7\u4f7f\u7528Ring\u6587\u672c\u7f16\u7a0b\u8bed\u8a00\u6784\u5efa\uff0c\u5b9e\u73b0\u4e86\u6bd4\u524d\u4ee3PWCT\u5feb36\u500d\u7684\u4ee3\u7801\u751f\u6210\u901f\u5ea6\u548c20\u500d\u7684\u5b58\u50a8\u6548\u7387\u63d0\u5347\u3002", "motivation": "\u5927\u591a\u6570\u53ef\u89c6\u5316\u7f16\u7a0b\u8bed\u8a00\u90fd\u662f\u9886\u57df\u7279\u5b9a\u7684\uff0c\u800c\u901a\u7528VPL\u5982PWCT\u9700\u8981\u6587\u672c\u7f16\u7a0b\u6765\u6539\u8fdb\u3002\u4f5c\u8005\u5e0c\u671b\u521b\u5efa\u4e00\u4e2a\u81ea\u6258\u7ba1\u7684\u901a\u7528VPL\uff0c\u80fd\u591f\u7528\u81ea\u8eab\u6765\u5f00\u53d1\u81ea\u8eab\u3002", "method": "\u9996\u5148\u8bbe\u8ba1Ring\u6587\u672c\u7f16\u7a0b\u8bed\u8a00\uff0c\u7136\u540e\u7528PWCT\u5f00\u53d1Ring\u7f16\u8bd1\u5668\uff0c\u518d\u57fa\u4e8eRing\u5f00\u53d1PWCT2\u3002PWCT2\u5305\u542b92,000\u884cRing\u4ee3\u7801\u548c394\u4e2a\u53ef\u89c6\u5316\u7ec4\u4ef6\uff0c\u652f\u6301\u5c06Ring\u4ee3\u7801\u8f6c\u6362\u4e3a\u53ef\u89c6\u5316\u4ee3\u7801\u3002", "result": "PWCT2\u4ee3\u7801\u751f\u6210\u901f\u5ea6\u5feb36\u500d\uff0c\u5b58\u50a8\u9700\u6c42\u51cf\u5c1120\u500d\u3002\u5728Steam\u5e73\u53f0\u4e0a\u67091772\u7528\u6237\u542f\u52a8\uff0c\u603b\u4f7f\u7528\u65f6\u95f4\u8d85\u8fc717,000\u5c0f\u65f6\uff0c\u83b7\u5f97\u79ef\u6781\u53cd\u9988\u3002", "conclusion": "\u6210\u529f\u521b\u5efa\u4e86\u81ea\u6258\u7ba1\u7684\u901a\u7528\u53ef\u89c6\u5316\u7f16\u7a0b\u8bed\u8a00PWCT2\uff0c\u8bc1\u660e\u4e86\u7528\u53ef\u89c6\u5316\u8bed\u8a00\u5f00\u53d1\u81ea\u8eab\u662f\u53ef\u884c\u7684\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5f00\u53d1\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.20534", "pdf": "https://arxiv.org/pdf/2509.20534", "abs": "https://arxiv.org/abs/2509.20534", "authors": ["Bowen Zhu", "Aayush Sabharwal", "Songchen Tan", "Yingbo Ma", "Alan Edelman", "Christopher Rackauckas"], "title": "Efficient Symbolic Computation vis Hash Consing", "categories": ["cs.PL", "cs.SC", "68W30", "I.1.1; G.4"], "comment": null, "summary": "Symbolic computation systems suffer from memory inefficiencies due to\nredundant storage of structurally identical subexpressions, commonly known as\nexpression swell, which degrades performance in both classical computer algebra\nand emerging AI-driven mathematical reasoning tools. In this paper, we present\nthe first integration of hash consing into JuliaSymbolics, a high-performance\nsymbolic toolkit in Julia, by employing a global weak-reference hash table that\ncanonicalizes expressions and eliminates duplication. This approach reduces\nmemory consumption and accelerates key operations such as differentiation,\nsimplification, and code generation, while seamlessly integrating with Julia's\nmetaprogramming and just-in-time compilation infrastructure. Benchmark\nevaluations across different computational domains reveal substantial\nimprovements: symbolic computations are accelerated by up to 3.2 times, memory\nusage is reduced by up to 2 times, code generation is up to 5 times faster,\nfunction compilation up to 10 times faster, and numerical evaluation up to 100\ntimes faster for larger models. While certain workloads with fewer duplicate\nunknown-variable expressions show more modest gains or even slight overhead in\ninitial computation stages, downstream processing consistently benefits\nsignificantly. These findings underscore the importance of hash consing in\nscaling symbolic computation and pave the way for future work integrating hash\nconsing with e-graphs for enhanced equivalence-aware expression sharing in\nAI-driven pipelines.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c06hash consing\u6280\u672f\u96c6\u6210\u5230JuliaSymbolics\u4e2d\uff0c\u901a\u8fc7\u5168\u5c40\u5f31\u5f15\u7528\u54c8\u5e0c\u8868\u6d88\u9664\u91cd\u590d\u8868\u8fbe\u5f0f\u5b58\u50a8\uff0c\u663e\u8457\u63d0\u5347\u7b26\u53f7\u8ba1\u7b97\u6027\u80fd\u548c\u5185\u5b58\u6548\u7387\u3002", "motivation": "\u7b26\u53f7\u8ba1\u7b97\u7cfb\u7edf\u5b58\u5728\u5185\u5b58\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u7ed3\u6784\u76f8\u540c\u7684\u5b50\u8868\u8fbe\u5f0f\u88ab\u91cd\u590d\u5b58\u50a8\uff08\u8868\u8fbe\u5f0f\u81a8\u80c0\uff09\uff0c\u8fd9\u964d\u4f4e\u4e86\u7ecf\u5178\u8ba1\u7b97\u673a\u4ee3\u6570\u548c\u65b0\u5174AI\u9a71\u52a8\u6570\u5b66\u63a8\u7406\u5de5\u5177\u7684\u6027\u80fd\u3002", "method": "\u5728JuliaSymbolics\u4e2d\u96c6\u6210hash consing\u6280\u672f\uff0c\u4f7f\u7528\u5168\u5c40\u5f31\u5f15\u7528\u54c8\u5e0c\u8868\u5bf9\u8868\u8fbe\u5f0f\u8fdb\u884c\u89c4\u8303\u5316\u5904\u7406\uff0c\u6d88\u9664\u91cd\u590d\u5b58\u50a8\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\u663e\u8457\u6539\u8fdb\uff1a\u7b26\u53f7\u8ba1\u7b97\u52a0\u901f\u8fbe3.2\u500d\uff0c\u5185\u5b58\u4f7f\u7528\u51cf\u5c11\u8fbe2\u500d\uff0c\u4ee3\u7801\u751f\u6210\u52a0\u901f\u8fbe5\u500d\uff0c\u51fd\u6570\u7f16\u8bd1\u52a0\u901f\u8fbe10\u500d\uff0c\u5927\u578b\u6a21\u578b\u7684\u6570\u503c\u8bc4\u4f30\u52a0\u901f\u8fbe100\u500d\u3002", "conclusion": "hash consing\u5bf9\u4e8e\u6269\u5c55\u7b26\u53f7\u8ba1\u7b97\u81f3\u5173\u91cd\u8981\uff0c\u4e3a\u672a\u6765\u5c06hash consing\u4e0ee-graphs\u96c6\u6210\u4ee5\u5b9e\u73b0AI\u9a71\u52a8\u7ba1\u9053\u4e2d\u589e\u5f3a\u7684\u7b49\u4ef7\u611f\u77e5\u8868\u8fbe\u5f0f\u5171\u4eab\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2509.20380", "pdf": "https://arxiv.org/pdf/2509.20380", "abs": "https://arxiv.org/abs/2509.20380", "authors": ["Samyak Jhaveri", "Vanessa Klotzmann", "Crista Lopes"], "title": "ACCeLLiuM: Supervised Fine-Tuning for Automated OpenACC Pragma Generation", "categories": ["cs.SE", "cs.AI", "cs.PL"], "comment": null, "summary": "The increasing ubiquity of GPUs is accompanied by the increasing complexity\nof their hardware and parallel programming frameworks. Directive-based parallel\nprogramming standards like OpenACC simplify GPU programming to some extent by\nabstracting away low-level complexities, but a fair amount of expertise is\nstill required in order to use those directives effectively.\n  We introduce ACCeLLiuM, two open weights Large Language Models specifically\nfine-tuned for generating expert OpenACC directives for data-parallel loops,\nalong with the supervised fine-tuning dataset that was used to train them. The\nACCeLLiuM SFT dataset contains 4,033 OpenACC pragma-loop pairs mined from\npublic GitHub C/C++ repositories, with 3,223 pairs for training and 810 for\ntesting. Experimental evaluations show a pronounced performance gap in\ngenerating correct OpenACC pragmas between base LLMs and our fine-tuned\nversions. On the held-out test set, base LLMs fail to consistently generate\nvalid pragmas, whereas LLMs fine-tuned on the ACCeLLiuM dataset generate valid\npragmas with the correct directive type for $87\\%$ of the data-parallel loops,\nand exact pragmas--including directives, clauses, clause order, and clause\nvariables--for $50\\%$ of the cases. Even when not exact, generated pragmas\nfrequently incorporate the correct clauses in a different order than the\nground-truth label, or include additional clauses that enable finer control\nover parallel execution, data movement, and concurrency, offering practical\nvalue beyond strict string-matching. By publicly releasing the code, models,\nand dataset as ACCeLLiuM we hope to establish a reproducible benchmark for\nLLM-powered OpenACC pragma generation, and lower the barrier to automated GPU\noffloading of serially written programs.", "AI": {"tldr": "ACCeLLiuM\u662f\u4e13\u95e8\u4e3a\u6570\u636e\u5e76\u884c\u5faa\u73af\u751f\u6210OpenACC\u6307\u4ee4\u800c\u5fae\u8c03\u7684\u4e24\u4e2a\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86OpenACC\u6307\u4ee4\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u968f\u7740GPU\u7684\u666e\u53ca\uff0c\u5176\u786c\u4ef6\u548c\u5e76\u884c\u7f16\u7a0b\u6846\u67b6\u65e5\u76ca\u590d\u6742\u3002\u867d\u7136\u57fa\u4e8e\u6307\u4ee4\u7684\u5e76\u884c\u7f16\u7a0b\u6807\u51c6\u5982OpenACC\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u7b80\u5316\u4e86GPU\u7f16\u7a0b\uff0c\u4f46\u6709\u6548\u4f7f\u7528\u8fd9\u4e9b\u6307\u4ee4\u4ecd\u9700\u8981\u76f8\u5f53\u7684\u4e13\u4e1a\u77e5\u8bc6\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b4,033\u4e2aOpenACC pragma-loop\u5bf9\u7684\u76d1\u7763\u5fae\u8c03\u6570\u636e\u96c6\uff0c\u5176\u4e2d3,223\u5bf9\u7528\u4e8e\u8bad\u7ec3\uff0c810\u5bf9\u7528\u4e8e\u6d4b\u8bd5\u3002\u57fa\u4e8e\u6b64\u6570\u636e\u96c6\u5bf9LLM\u8fdb\u884c\u4e13\u95e8\u5fae\u8c03\u3002", "result": "\u5728\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u57fa\u7840LLM\u65e0\u6cd5\u4e00\u81f4\u751f\u6210\u6709\u6548\u7684pragma\uff0c\u800c\u5fae\u8c03\u540e\u7684LLM\u80fd\u591f\u4e3a87%\u7684\u6570\u636e\u5e76\u884c\u5faa\u73af\u751f\u6210\u5177\u6709\u6b63\u786e\u6307\u4ee4\u7c7b\u578b\u7684\u6709\u6548pragma\uff0c\u5176\u4e2d50%\u7684\u6848\u4f8b\u80fd\u591f\u751f\u6210\u5b8c\u5168\u51c6\u786e\u7684pragma\u3002", "conclusion": "ACCeLLiuM\u663e\u8457\u964d\u4f4e\u4e86\u81ea\u52a8\u5316GPU\u5378\u8f7d\u7684\u969c\u788d\uff0c\u4e3aLLM\u9a71\u52a8\u7684OpenACC pragma\u751f\u6210\u5efa\u7acb\u4e86\u53ef\u590d\u73b0\u7684\u57fa\u51c6\uff0c\u5373\u4f7f\u5728\u4e0d\u5b8c\u5168\u51c6\u786e\u7684\u60c5\u51b5\u4e0b\uff0c\u751f\u6210\u7684pragma\u4e5f\u7ecf\u5e38\u5305\u542b\u6709\u7528\u7684\u989d\u5916\u5b50\u53e5\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.20384", "pdf": "https://arxiv.org/pdf/2509.20384", "abs": "https://arxiv.org/abs/2509.20384", "authors": ["Jiayi Lin", "Liangcai Su", "Junzhe Li", "Chenxiong Qian"], "title": "R1-Fuzz: Specializing Language Models for Textual Fuzzing via Reinforcement Learning", "categories": ["cs.CR", "cs.AI", "cs.PL", "cs.SE"], "comment": null, "summary": "Fuzzing is effective for vulnerability discovery but struggles with complex\ntargets such as compilers, interpreters, and database engines, which accept\ntextual input that must satisfy intricate syntactic and semantic constraints.\nAlthough language models (LMs) have attracted interest for this task due to\ntheir vast latent knowledge and reasoning potential, their practical adoption\nhas been limited. The major challenges stem from insufficient exploration of\ndeep program logic among real-world codebases, and the high cost of leveraging\nlarger models. To overcome these challenges, we propose R1-Fuzz, the first\nframework that leverages reinforcement learning (RL) to specialize\ncost-efficient LMs and integrate them for complex textual fuzzing input\ngeneration. R1-Fuzz introduces two key designs: coverage-slicing-based question\nconstruction and a distance-based reward calculation. Through RL-based\npost-training of a model with our constructed dataset, R1-Fuzz designs a\nfuzzing workflow that tightly integrates LMs to reason deep program semantics\nduring fuzzing. Evaluations on diverse real-world targets show that our design\nenables a small model, named R1-Fuzz-7B, to rival or even outperform much\nlarger models in real-world fuzzing. Notably, R1-Fuzz achieves up to 75\\%\nhigher coverage than state-of-the-art fuzzers and discovers 29 previously\nunknown vulnerabilities, demonstrating its practicality.", "AI": {"tldr": "R1-Fuzz\u662f\u4e00\u4e2a\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u4e13\u95e8\u5316\u6210\u672c\u6548\u76ca\u9ad8\u7684\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u590d\u6742\u6587\u672c\u6a21\u7cca\u6d4b\u8bd5\u8f93\u5165\u751f\u6210\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8986\u76d6\u5207\u7247\u5f0f\u95ee\u9898\u6784\u5efa\u548c\u57fa\u4e8e\u8ddd\u79bb\u7684\u5956\u52b1\u8ba1\u7b97\uff0c\u4f7f\u5c0f\u578b\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u6a21\u7cca\u6d4b\u8bd5\u4e2d\u5ab2\u7f8e\u751a\u81f3\u8d85\u8d8a\u66f4\u5927\u6a21\u578b\u3002", "motivation": "\u9488\u5bf9\u7f16\u8bd1\u5668\u3001\u89e3\u91ca\u5668\u548c\u6570\u636e\u5e93\u5f15\u64ce\u7b49\u590d\u6742\u76ee\u6807\u7684\u6a21\u7cca\u6d4b\u8bd5\u9762\u4e34\u8f93\u5165\u9700\u6ee1\u8db3\u590d\u6742\u8bed\u6cd5\u8bed\u4e49\u7ea6\u675f\u7684\u6311\u6218\uff0c\u73b0\u6709\u8bed\u8a00\u6a21\u578b\u65b9\u6cd5\u5b58\u5728\u5bf9\u6df1\u5c42\u7a0b\u5e8f\u903b\u8f91\u63a2\u7d22\u4e0d\u8db3\u548c\u5927\u578b\u6a21\u578b\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faR1-Fuzz\u6846\u67b6\uff0c\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u6210\u672c\u6548\u76ca\u9ad8\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u5305\u542b\u8986\u76d6\u5207\u7247\u5f0f\u95ee\u9898\u6784\u5efa\u548c\u57fa\u4e8e\u8ddd\u79bb\u7684\u5956\u52b1\u8ba1\u7b97\u4e24\u4e2a\u5173\u952e\u8bbe\u8ba1\uff0c\u5c06\u8bed\u8a00\u6a21\u578b\u7d27\u5bc6\u96c6\u6210\u5230\u6a21\u7cca\u6d4b\u8bd5\u5de5\u4f5c\u6d41\u4e2d\u4ee5\u63a8\u7406\u6df1\u5c42\u7a0b\u5e8f\u8bed\u4e49\u3002", "result": "\u5728\u591a\u6837\u5316\u771f\u5b9e\u4e16\u754c\u76ee\u6807\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cR1-Fuzz-7B\u5c0f\u578b\u6a21\u578b\u80fd\u591f\u5ab2\u7f8e\u751a\u81f3\u8d85\u8d8a\u66f4\u5927\u6a21\u578b\uff0c\u6bd4\u6700\u5148\u8fdb\u7684\u6a21\u7cca\u6d4b\u8bd5\u5de5\u5177\u5b9e\u73b0\u9ad8\u8fbe75%\u7684\u8986\u76d6\u7387\u63d0\u5347\uff0c\u5e76\u53d1\u73b0\u4e8629\u4e2a\u5148\u524d\u672a\u77e5\u7684\u6f0f\u6d1e\u3002", "conclusion": "R1-Fuzz\u8bc1\u660e\u4e86\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4e13\u95e8\u5316\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u6587\u672c\u6a21\u7cca\u6d4b\u8bd5\u4e2d\u7684\u5b9e\u7528\u6027\uff0c\u4e3a\u6210\u672c\u6548\u76ca\u9ad8\u7684\u6f0f\u6d1e\u53d1\u73b0\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.20518", "pdf": "https://arxiv.org/pdf/2509.20518", "abs": "https://arxiv.org/abs/2509.20518", "authors": ["Sayed Mahbub Hasan Amiri", "Md Mainul Islam"], "title": "Enhancing Python Programming Education with an AI-Powered Code Helper: Design, Implementation, and Impact", "categories": ["cs.SE", "cs.PL", "D.2.3"], "comment": "20 pages, 16 figures", "summary": "This is the study that presents an AI-Python-based chatbot that helps\nstudents to learn programming by demonstrating solutions to such problems as\ndebugging errors, solving syntax problems or converting abstract theoretical\nconcepts to practical implementations. Traditional coding tools like Integrated\nDevelopment Environments (IDEs) and static analyzers do not give robotic help\nwhile AI-driven code assistants such as GitHub Copilot focus on getting things\ndone. To close this gap, our chatbot combines static code analysis, dynamic\nexecution tracing, and large language models (LLMs) to provide the students\nwith relevant and practical advice, hence promoting the learning process. The\nchatbots hybrid architecture employs CodeLlama for code embedding, GPT-4 for\nnatural language interactions, and Docker-based sandboxing for secure\nexecution. Evaluated through a mixed-methods approach involving 1,500 student\nsubmissions, the system demonstrated an 85% error resolution success rate,\noutperforming standalone tools like pylint (62%) and GPT-4 (73%). Quantitative\nresults revealed a 59.3% reduction in debugging time among users, with pre- and\npost-test assessments showing a 34% improvement in coding proficiency,\nparticularly in recursion and exception handling. Qualitative feedback from 120\nstudents highlighted the chatbots clarity, accessibility, and\nconfidence-building impact, though critiques included occasional latency and\nrestrictive code sanitization. By balancing technical innovation with\npedagogical empathy, this research provides a blueprint for AI tools that\nprioritize educational equity and long-term skill retention over mere code\ncompletion. The chatbot exemplifies how AI can augment human instruction,\nfostering deeper conceptual understanding in programming education.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eAI-Python\u7684\u804a\u5929\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u7ed3\u5408\u9759\u6001\u4ee3\u7801\u5206\u6790\u3001\u52a8\u6001\u6267\u884c\u8ffd\u8e2a\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5e2e\u52a9\u5b66\u751f\u5b66\u4e60\u7f16\u7a0b\uff0c\u5728\u9519\u8bef\u89e3\u51b3\u3001\u8c03\u8bd5\u65f6\u95f4\u548c\u7f16\u7a0b\u80fd\u529b\u63d0\u5347\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4f20\u7edfIDE\u548c\u9759\u6001\u5206\u6790\u5de5\u5177\u7f3a\u4e4f\u673a\u5668\u4eba\u8f85\u52a9\uff0c\u800cAI\u4ee3\u7801\u52a9\u624b\u5982GitHub Copilot\u4e3b\u8981\u5173\u6ce8\u4ee3\u7801\u5b8c\u6210\u800c\u975e\u6559\u80b2\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u5f00\u53d1\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u7f16\u7a0b\u6559\u80b2\u7684AI\u52a9\u624b\u3002", "method": "\u91c7\u7528\u6df7\u5408\u67b6\u6784\uff1a\u4f7f\u7528CodeLlama\u8fdb\u884c\u4ee3\u7801\u5d4c\u5165\uff0cGPT-4\u5904\u7406\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\uff0cDocker\u6c99\u7bb1\u786e\u4fdd\u5b89\u5168\u6267\u884c\u3002\u7ed3\u5408\u9759\u6001\u4ee3\u7801\u5206\u6790\u548c\u52a8\u6001\u6267\u884c\u8ffd\u8e2a\u6280\u672f\u3002", "result": "\u57281,500\u4efd\u5b66\u751f\u63d0\u4ea4\u7684\u8bc4\u4f30\u4e2d\uff0c\u7cfb\u7edf\u9519\u8bef\u89e3\u51b3\u6210\u529f\u7387\u8fbe85%\uff0c\u4f18\u4e8epylint\uff0862%\uff09\u548cGPT-4\uff0873%\uff09\u3002\u8c03\u8bd5\u65f6\u95f4\u51cf\u5c1159.3%\uff0c\u7f16\u7a0b\u80fd\u529b\u63d0\u534734%\uff0c\u7279\u522b\u662f\u5728\u9012\u5f52\u548c\u5f02\u5e38\u5904\u7406\u65b9\u9762\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86AI\u5982\u4f55\u589e\u5f3a\u4eba\u7c7b\u6559\u5b66\uff0c\u5728\u7f16\u7a0b\u6559\u80b2\u4e2d\u4fc3\u8fdb\u66f4\u6df1\u5c42\u6b21\u7684\u6982\u5ff5\u7406\u89e3\uff0c\u4e3a\u4f18\u5148\u8003\u8651\u6559\u80b2\u516c\u5e73\u548c\u957f\u671f\u6280\u80fd\u4fdd\u7559\u7684AI\u5de5\u5177\u63d0\u4f9b\u4e86\u84dd\u56fe\u3002"}}
{"id": "2509.21039", "pdf": "https://arxiv.org/pdf/2509.21039", "abs": "https://arxiv.org/abs/2509.21039", "authors": ["William F. Godoy", "Tatiana Melnichenko", "Pedro Valero-Lara", "Wael Elwasif", "Philip Fackler", "Rafael Ferreira Da Silva", "Keita Teranishi", "Jeffrey S. Vetter"], "title": "Mojo: MLIR-Based Performance-Portable HPC Science Kernels on GPUs for the Python Ecosystem", "categories": ["cs.DC", "cs.CE", "cs.ET", "cs.PL"], "comment": "Accepted at the IEEE/ACM SC25 Conference WACCPD Workshop. The\n  International Conference for High Performance Computing, Networking, Storage,\n  and Analysis, St. Louis, MO, Nov 16-21, 2025. 15 pages, 7 figures. WFG and TM\n  contributed equally", "summary": "We explore the performance and portability of the novel Mojo language for\nscientific computing workloads on GPUs. As the first language based on the\nLLVM's Multi-Level Intermediate Representation (MLIR) compiler infrastructure,\nMojo aims to close performance and productivity gaps by combining Python's\ninteroperability and CUDA-like syntax for compile-time portable GPU\nprogramming. We target four scientific workloads: a seven-point stencil\n(memory-bound), BabelStream (memory-bound), miniBUDE (compute-bound), and\nHartree-Fock (compute-bound with atomic operations); and compare their\nperformance against vendor baselines on NVIDIA H100 and AMD MI300A GPUs. We\nshow that Mojo's performance is competitive with CUDA and HIP for memory-bound\nkernels, whereas gaps exist on AMD GPUs for atomic operations and for fast-math\ncompute-bound kernels on both AMD and NVIDIA GPUs. Although the learning curve\nand programming requirements are still fairly low-level, Mojo can close\nsignificant gaps in the fragmented Python ecosystem in the convergence of\nscientific computing and AI.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86Mojo\u8bed\u8a00\u5728GPU\u4e0a\u79d1\u5b66\u8ba1\u7b97\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6027\u80fd\u548c\u53ef\u79fb\u690d\u6027\uff0c\u6bd4\u8f83\u4e86\u5728NVIDIA H100\u548cAMD MI300A GPU\u4e0a\u4e0eCUDA\u548cHIP\u7684\u6027\u80fd\u8868\u73b0", "motivation": "Mojo\u4f5c\u4e3a\u9996\u4e2a\u57fa\u4e8eLLVM MLIR\u7f16\u8bd1\u5668\u57fa\u7840\u8bbe\u65bd\u7684\u8bed\u8a00\uff0c\u65e8\u5728\u901a\u8fc7\u7ed3\u5408Python\u7684\u4e92\u64cd\u4f5c\u6027\u548c\u7c7b\u4f3cCUDA\u7684\u8bed\u6cd5\u6765\u7f29\u5c0f\u6027\u80fd\u548c\u751f\u4ea7\u529b\u5dee\u8ddd\uff0c\u5b9e\u73b0\u7f16\u8bd1\u65f6\u53ef\u79fb\u690d\u7684GPU\u7f16\u7a0b", "method": "\u9488\u5bf9\u56db\u79cd\u79d1\u5b66\u8ba1\u7b97\u5de5\u4f5c\u8d1f\u8f7d\u8fdb\u884c\u6d4b\u8bd5\uff1a\u4e03\u70b9\u6a21\u677f\uff08\u5185\u5b58\u53d7\u9650\uff09\u3001BabelStream\uff08\u5185\u5b58\u53d7\u9650\uff09\u3001miniBUDE\uff08\u8ba1\u7b97\u53d7\u9650\uff09\u548cHartree-Fock\uff08\u5177\u6709\u539f\u5b50\u64cd\u4f5c\u7684\u8ba1\u7b97\u53d7\u9650\uff09\uff0c\u5e76\u4e0e\u4f9b\u5e94\u5546\u57fa\u7ebf\u8fdb\u884c\u6bd4\u8f83", "result": "Mojo\u5728\u5185\u5b58\u53d7\u9650\u5185\u6838\u4e0a\u7684\u6027\u80fd\u4e0eCUDA\u548cHIP\u76f8\u5f53\uff0c\u4f46\u5728AMD GPU\u4e0a\u7684\u539f\u5b50\u64cd\u4f5c\u4ee5\u53caAMD\u548cNVIDIA GPU\u4e0a\u7684\u5feb\u901f\u6570\u5b66\u8ba1\u7b97\u53d7\u9650\u5185\u6838\u5b58\u5728\u6027\u80fd\u5dee\u8ddd", "conclusion": "\u5c3d\u7ba1\u5b66\u4e60\u66f2\u7ebf\u548c\u7f16\u7a0b\u8981\u6c42\u4ecd\u7136\u76f8\u5f53\u5e95\u5c42\uff0c\u4f46Mojo\u53ef\u4ee5\u5728\u79d1\u5b66\u8ba1\u7b97\u548cAI\u878d\u5408\u7684\u788e\u7247\u5316Python\u751f\u6001\u7cfb\u7edf\u4e2d\u7f29\u5c0f\u663e\u8457\u5dee\u8ddd"}}
