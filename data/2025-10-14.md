<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 12]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Herb.jl: A Unifying Program Synthesis Library](https://arxiv.org/abs/2510.09726)
*Tilman Hinnerichs,Reuben Gardos Reid,Jaap de Jong,Bart Swinkels,Pamela Wochner,Nicolae Filat,Tudor Magurescu,Issa Hanou,Sebastijan Dumancic*

Main category: cs.PL

TL;DR: Herb.jl是一个用Julia编程语言编写的统一程序合成库，旨在模块化合成算法，便于重用和扩展现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的程序合成工具虽然众多，但重用和混合先前开发的方法既繁琐又耗时。需要一种统一的方法来简化合成算法的重用和扩展。

Method: 将底层合成算法模块化为可通信和完全可扩展的子组件，允许直接重用这些模块。在Julia中实现统一的程序合成库Herb.jl。

Result: 展示了三个常见用例：实现简单问题和语法并求解；用几行代码实现先前开发的合成器；在基准测试上运行合成器。

Conclusion: Herb.jl提供了一个有效的统一框架，简化了程序合成方法的重用和扩展，提高了开发效率。

Abstract: Program synthesis -- the automatic generation of code given a specification
-- is one of the most fundamental tasks in artificial intelligence (AI) and
many programmers' dream. Numerous synthesizers have been developed to tackle
program synthesis, manifesting different ideas to approach the exponentially
growing program space. While numerous smart program synthesis tools exist,
reusing and remixing previously developed methods is tedious and
time-consuming. We propose Herb.jl, a unifying program synthesis library
written in the Julia programming language, to address these issues. Since
current methods rely on similar building blocks, we aim to modularize the
underlying synthesis algorithm into communicating and fully extendable
sub-compartments, allowing for straightforward reapplication of these modules.
To demonstrate the benefits of using Herb.jl, we show three common use cases:
1. how to implement a simple problem and grammar, and how to solve it, 2. how
to implement a previously developed synthesizer with just a few lines of code,
and 3. how to run a synthesizer against a benchmark.

</details>


### [2] [ACT: Automatically Generating Compiler Backends from Tensor Accelerator ISA Descriptions](https://arxiv.org/abs/2510.09932)
*Devansh Jain,Akash Pardeshi,Marco Frigo,Krut Patel,Kaustubh Khulbe,Jai Arora,Charith Mendis*

Main category: cs.PL

TL;DR: ACT是一个自动生成张量加速器编译器后端的工具，只需输入指令集架构描述即可生成高性能的编译器后端，解决了新型张量加速器缺乏编译器支持的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的张量加速器缺乏编译器后端支持，且硬件设计迭代快速，手动开发编译器后端成本高、周期长，阻碍了新型加速器的采用和软件开发。

Method: ACT通过形式化定义编译器后端生成问题，支持用户可编程内存和复杂参数化指令，采用参数化等式饱和的指令选择方法和基于约束规划的内存分配策略。

Result: 为三个工业和学术界的加速器平台生成了编译器后端，性能达到或超过手动优化的内核库代码，同时保持较低的编译开销。

Conclusion: ACT能够自动生成正确且完整的编译器后端，显著降低了为新型张量加速器开发编译器软件的成本和周期。

Abstract: Tensor compilers play a key role in enabling high-performance implementations
of deep learning workloads. These compilers rely on existing CPU and GPU code
generation backends to generate device-specific code. Recently, many tensor
accelerators (neural processing units) have been proposed to further accelerate
these workloads. Compared to commodity hardware, however, most of the proposed
tensor accelerators do not have compiler backends with code generation support.
Moreover, the accelerator designs are subject to fast iteration cycles, making
it difficult to manually develop compiler backends similar to commodity
hardware platforms. Therefore, to increase adoption and enable faster software
development cycles for novel tensor accelerator designs, we need to make the
compiler backend construction process more agile.
  To address this gap, we introduce ACT, a compiler backend generator that
automatically generates compiler backends for tensor accelerators, given just
the instruction set architecture (ISA) descriptions. We first formally specify
the compiler backend generation problem that introduces a novel specification
for describing tensor accelerator ISAs. Next, we design ACT such that it
supports user-programmable memories and complex parameterized instructions that
are prevalent in tensor accelerators. ACT uses a novel parameterized equality
saturation-based instruction selection phase and a constraint programming-based
memory allocation phase. We prove that compiler backends generated by ACT are
sound and complete. Finally, we generate compiler backends for three
accelerator platforms from industry and academia, and show that they match or
outperform code written using hand-optimized kernel libraries while maintaining
low compilation overheads.

</details>


### [3] [End-to-end Compositional Verification of Program Safety through Verified and Verifying Compilation](https://arxiv.org/abs/2510.10015)
*Jinhua Wu,Yuting Wang,Liukun Yu,Linglong Meng*

Main category: cs.PL

TL;DR: 提出了基于开放标记转移系统的模块化安全定义（开放安全），支持异构模块的安全验证和组合，并通过所有权语言编译器验证了该框架。


<details>
  <summary>Details</summary>
Motivation: 现代安全编程语言（如Rust）需要混合安全和不安全模块，传统端到端安全验证方法无法处理模块化场景，需要新的模块化安全定义和组合方法。

Method: 基于开放标记转移系统定义开放安全概念，支持模块边界组合，并通过所有权语言Owlang的验证编译器进行实证。

Result: 开发了模块化安全验证框架，能够组合异构模块（如Owlang和C）的安全结果，实现了验证编译和验证编译的协同工作。

Conclusion: 开放安全为异构模块的安全验证提供了理论基础，支持模块化验证和组合，是现代安全编程语言端到端安全验证的有效解决方案。

Abstract: Program safety (i.e., absence of undefined behaviors) is critical for correct
operation of computer systems. It is usually verified at the source level
(e.g., by separation logics) and preserved to the target by verified compilers
(e.g., CompCert), thereby achieving end-to-end verification of safety. However,
modern safe programming languages like Rust pose new problems in achieving
end-to-end safety. Because not all functionalities can be implemented in the
safe language, mixing safe and unsafe modules is needed. Therefore, verified
compilation must preserve a modular notion of safety which can be composed at
the target level. Furthermore, certain classes of errors (e.g., memory errors)
are automatically excluded by verifying compilation (e.g., borrow checking) for
modules written in safe languages. As a result, verified compilation needs to
cooperate with verifying compilation to ensure end-to-end safety.
  To address the above problems, we propose a modular and generic definition of
safety called open safety based on program semantics described as open labeled
transition systems (LTS). Open safety is composable at the boundary of modules
and can be modularly preserved by verified compositional compilation. Those
properties enable separate verification of safety for heterogeneous modules and
composition of the safety results at the target level. Open safety can be
generalized to partial safety (i.e., only a certain class of errors can occur).
By this we formalized the correctness of verifying compilation as derivation of
total safety from partial safety. We demonstrate how our framework can combine
verified and verifying compilation by developing a verified compiler for an
ownership language (called Owlang) inspired by Rust. We evaluate our approach
on the compositional safety verification using a hash map implemented by Owlang
and C.

</details>


### [4] [LOOPerSet: A Large-Scale Dataset for Data-Driven Polyhedral Compiler Optimization](https://arxiv.org/abs/2510.10209)
*Massinissa Merouani,Afif Boudaoud,Riyadh Baghdadi*

Main category: cs.PL

TL;DR: LOOPerSet是一个包含2800万个标记数据点的大规模公共数据集，用于机器学习驱动的编译器优化研究，特别是多面体模型中的性能预测。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习在编译器优化中的应用受到大规模公共性能数据集稀缺的限制，研究人员需要进行昂贵的数据生成活动，这减缓了创新速度并阻碍了可重复研究。

Method: 从22万个独特的合成生成的多面体程序中提取数据点，每个数据点将程序及其语义保持转换序列（如融合、倾斜、分块和并行化）映射到真实性能测量（执行时间）。

Result: 创建了一个包含2800万个标记数据点的大规模多样化数据集，为训练和评估学习成本模型、基准测试新模型架构以及探索自动化多面体调度前沿提供了宝贵资源。

Conclusion: LOOPerSet通过提供大规模公共数据集，降低了数据驱动编译器优化的入门门槛，促进了可重复研究，推动了多面体模型中的机器学习优化发展。

Abstract: The advancement of machine learning for compiler optimization, particularly
within the polyhedral model, is constrained by the scarcity of large-scale,
public performance datasets. This data bottleneck forces researchers to
undertake costly data generation campaigns, slowing down innovation and
hindering reproducible research learned code optimization. To address this gap,
we introduce LOOPerSet, a new public dataset containing 28 million labeled data
points derived from 220,000 unique, synthetically generated polyhedral
programs. Each data point maps a program and a complex sequence of
semantics-preserving transformations (such as fusion, skewing, tiling, and
parallelism)to a ground truth performance measurement (execution time). The
scale and diversity of LOOPerSet make it a valuable resource for training and
evaluating learned cost models, benchmarking new model architectures, and
exploring the frontiers of automated polyhedral scheduling. The dataset is
released under a permissive license to foster reproducible research and lower
the barrier to entry for data-driven compiler optimization.

</details>


### [5] [Learning to Guarantee Type Correctness in Code Generation through Type-Guided Program Synthesis](https://arxiv.org/abs/2510.10216)
*Zhechong Huang,Zhao Zhang,Ruyi Ji,Tingxuan Xia,Qihao Zhu,Qinxiang Cao,Zeyu Sun,Yingfei Xiong*

Main category: cs.PL

TL;DR: TyFlow是一个将类型推理内化到代码生成中的新系统，通过类型引导的程序合成方法，在类型推导树和合成推导树之间保持同构关系，从而消除类型错误并提高功能正确性。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型在代码生成方面表现出色，但确保类型正确性仍然是一个挑战。传统方法如约束解码只能外部拒绝不可类型化的代码，但模型本身没有有效学习类型推理，这限制了整体性能。

Method: TyFlow采用类型引导的程序合成系统，维持类型推导树和合成推导树之间的同构关系，使用基于合成决策序列的新代码表示而非传统的基于文本的标记序列，将类型系统学习的复杂性卸载到表示本身。

Result: 评估显示TyFlow不仅消除了类型错误，还显著提高了功能正确性，证明了将语言模型与类型系统内部对齐的重要性。

Conclusion: 将类型推理内化到代码生成中对于提高语言模型的代码生成质量至关重要，TyFlow通过新颖的表示方法成功实现了这一目标。

Abstract: Language models have shown remarkable proficiency in code generation;
nevertheless, ensuring type correctness remains a challenge. Although
traditional methods, such as constrained decoding, alleviate this problem by
externally rejecting untypable code, the model itself does not effectively
learn type reasoning internally, which ultimately limits its overall
performance. This paper introduces TyFlow, a novel system that internalizes
type reasoning within code generation to guide the model to learn the type
system. The core of our approach is a novel type-guided program synthesis
system that maintains an isomorphism between type derivation trees and
synthesis derivation trees, enabling a new code representation based on
synthesis decision sequences rather than traditional text-based token
sequences. By offloading the complexity of type system learning to the
representation itself, models can redirect their computational resources toward
higher-level program semantics. Our evaluation shows that TyFlow not only
eliminates type errors but also significantly improves functional correctness,
highlighting the importance of aligning LMs with type systems internally.

</details>


### [6] [Old is Gold: Optimizing Single-threaded Applications with Exgen-Malloc](https://arxiv.org/abs/2510.10219)
*Ruihao Li,Lizy K. John,Neeraja J. Yadwadkar*

Main category: cs.PL

TL;DR: Exgen-Malloc是一个专为单线程应用设计的内存分配器，通过消除不必要的元数据、简化控制流来减少开销，相比传统分配器在性能和内存使用上都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现代内存分配器针对多线程环境优化，但在单线程场景下会引入不必要的复杂性和开销。在超大规模数据中心，即使1%的效率提升也能节省数百万美元和能源消耗。

Method: 采用集中式堆、单一空闲块列表、平衡的内存提交和重定位策略，借鉴现代多线程分配器的设计原则，但专门针对单线程执行进行优化。

Result: 在两个Intel Xeon平台上测试，相比dlmalloc在SPEC CPU2017、redis-benchmark和mimalloc-bench上分别获得1.17x、1.10x和1.93x的速度提升，相比mimalloc分别节省6.2%、0.1%和25.2%的内存。

Conclusion: 专门为单线程应用设计的分配器能够显著提升性能和内存效率，证明了在特定场景下简化设计的价值。

Abstract: Memory allocators hide beneath nearly every application stack, yet their
performance footprint extends far beyond their code size. Even small
inefficiencies in the allocators ripple through caches and the rest of the
memory hierarchy, collectively imposing what operators often call a "datacenter
tax". At hyperscale, even a 1% improvement in allocator efficiency can unlock
millions of dollars in savings and measurable reductions in datacenter energy
consumption. Modern memory allocators are designed to optimize allocation speed
and memory fragmentation in multi-threaded environments, relying on complex
metadata and control logic to achieve high performance. However, the overhead
introduced by this complexity prompts a reevaluation of allocator design.
Notably, such overhead can be avoided in single-threaded scenarios, which
continue to be widely used across diverse application domains.
  In this paper, we introduce Exgen-Malloc, a memory allocator purpose-built
for single-threaded applications. By specializing for single-threaded
execution, Exgen-Malloc eliminates unnecessary metadata, simplifies the control
flow, thereby reducing overhead and improving allocation efficiency. Its core
design features include a centralized heap, a single free-block list, and a
balanced strategy for memory commitment and relocation. Additionally,
Exgen-Malloc incorporates design principles in modern multi-threaded
allocators, which do not exist in legacy single-threaded allocators such as
dlmalloc. We evaluate Exgen-Malloc on two Intel Xeon platforms. Across both
systems, Exgen-Malloc achieves a speedup of 1.17x, 1.10x, and 1.93x over
dlmalloc on SPEC CPU2017, redis-benchmark, and mimalloc-bench, respectively. In
addition to performance, Exgen-Malloc achieves 6.2%, 0.1%, and 25.2% memory
savings over mimalloc on SPEC CPU2017, redis-benchmark, and mimalloc-bench,
respectively.

</details>


### [7] [A Trace-based Approach for Code Safety Analysis](https://arxiv.org/abs/2510.10410)
*Hui Xu*

Main category: cs.PL

TL;DR: 本文系统分析了Rust语言中的不安全代码和未定义行为问题，建立了理解框架并总结了Rust代码的正确性标准，为安全封装提供了实践指导。


<details>
  <summary>Details</summary>
Motivation: Rust作为内存安全编程语言虽然禁止未定义行为，但不安全代码仍然是关键问题。通过审查Rust的安全设计和分析实际项目，需要建立系统框架来理解不安全代码和未定义行为。

Method: 通过回顾Rust的安全设计，分析真实世界的Rust项目，建立系统框架来理解不安全代码和未定义行为，并总结Rust代码的正确性标准。

Result: 建立了理解不安全代码和未定义行为的系统框架，总结了Rust代码的正确性标准，并推导出实现安全封装的可操作指导。

Conclusion: 该研究为Rust开发者提供了系统的方法来理解和处理不安全代码，为实现安全封装提供了具体指导，有助于提高Rust代码的整体安全性。

Abstract: Rust is a memory-safe programming language that disallows undefined behavior.
Its safety guarantees have been extensively examined by the community through
empirical studies, which has led to its remarkable success. However, unsafe
code remains a critical concern in Rust. By reviewing the safety design of Rust
and analyzing real-world Rust projects, this paper establishes a systematic
framework for understanding unsafe code and undefined behavior, and summarizes
the soundness criteria for Rust code. It further derives actionable guidance
for achieving sound encapsulation.

</details>


### [8] [ECO: Enhanced Code Optimization via Performance-Aware Prompting for Code-LLMs](https://arxiv.org/abs/2510.10517)
*Su-Hyeon Kim,Joonghyuk Hahn,Sooyoung Cha,Yo-Sub Han*

Main category: cs.PL

TL;DR: ECO是一个性能感知的代码优化提示框架，通过提取运行时优化指令和符号化瓶颈诊断，为代码LLMs提供优化指导，无需微调即可显著提升代码效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于慢-快代码对的优化方法往往导致表面模式模仿而非真正的性能推理，无法揭示性能提升的因果因素。

Method: ECO从参考代码对中提取运行时优化指令(ROIs)，并行使用符号化顾问进行瓶颈诊断和ROI检索器，将两者组合成性能感知提示。

Result: ECO显著提升了代码LLMs生成高效代码的能力，实现了最高7.81倍的速度提升，同时最小化正确性损失。

Conclusion: ECO框架提供了一种模型无关、无需微调的代码优化方法，通过性能感知提示有效指导代码LLMs进行性能推理。

Abstract: Code runtime optimization-the task of rewriting a given code to a faster
one-remains challenging, as it requires reasoning about performance trade-offs
involving algorithmic and structural choices. Recent approaches employ
code-LLMs with slow-fast code pairs provided as optimization guidance, but such
pair-based methods obscure the causal factors of performance gains and often
lead to superficial pattern imitation rather than genuine performance
reasoning. We introduce ECO, a performance-aware prompting framework for code
optimization. ECO first distills runtime optimization instructions (ROIs) from
reference slow-fast code pairs; Each ROI describes root causes of inefficiency
and the rationales that drive performance improvements. For a given input code,
ECO in parallel employs (i) a symbolic advisor to produce a bottleneck
diagnosis tailored to the code, and (ii) an ROI retriever to return related
ROIs. These two outputs are then composed into a performance-aware prompt,
providing actionable guidance for code-LLMs. ECO's prompts are model-agnostic,
require no fine-tuning, and can be easily prepended to any code-LLM prompt. Our
empirical studies highlight that ECO prompting significantly improves
code-LLMs' ability to generate efficient code, achieving speedups of up to
7.81x while minimizing correctness loss.

</details>


### [9] [A Verified High-Performance Composable Object Library for Remote Direct Memory Access (Extended Version)](https://arxiv.org/abs/2510.10531)
*Guillaume Ambal,George Hodgkins,Mark Madler,Gregory Chockler,Brijesh Dongol,Joseph Izraelevitz,Azalea Raad,Viktor Vafeiadis*

Main category: cs.PL

TL;DR: LOCO是一个经过形式化验证的RDMA多节点对象库，填补了共享内存和分布式系统编程之间的空白，提供高性能且易于验证的编程模型。


<details>
  <summary>Details</summary>
Motivation: RDMA虽然提供低延迟高吞吐量，但其弱内存模型难以使用且最近才被形式化，需要简化编程模型并支持形式化验证。

Method: 开发LOCO库构建多节点对象，利用RDMA的强局部性和弱一致性特性，并创建Mowgli模块化声明式验证框架来验证正确性。

Result: LOCO对象性能与定制RDMA系统相当，但编程模型更简单，适合形式化正确性证明。

Conclusion: LOCO成功填补了RDMA编程的空白，提供了高性能且可验证的多节点对象库，Mowgli框架为多节点对象验证提供了灵活解决方案。

Abstract: Remote Direct Memory Access (RDMA) is a memory technology that allows remote
devices to directly write to and read from each other's memory, bypassing
components such as the CPU and operating system. This enables low-latency
high-throughput networking, as required for many modern data centres, HPC
applications and AI/ML workloads. However, baseline RDMA comprises a highly
permissive weak memory model that is difficult to use in practice and has only
recently been formalised. In this paper, we introduce the Library of Composable
Objects (LOCO), a formally verified library for building multi-node objects on
RDMA, filling the gap between shared memory and distributed system programming.
LOCO objects are well-encapsulated and take advantage of the strong locality
and the weak consistency characteristics of RDMA. They have performance
comparable to custom RDMA systems (e.g. distributed maps), but with a far
simpler programming model amenable to formal proofs of correctness. To support
verification, we develop a novel modular declarative verification framework,
called Mowgli, that is flexible enough to model multinode objects and is
independent of a memory consistency model. We instantiate Mowgli with the RDMA
memory model, and use it to verify correctness of LOCO libraries.

</details>


### [10] [Abstract String Domain Defined with Word Equations as a Reduced Product (Extended Version)](https://arxiv.org/abs/2510.11007)
*Antonina Nepeivoda,Ilya Afanasyev*

Main category: cs.PL

TL;DR: 提出了一种基于字符串区间的抽象域，使用字方程和字不等式来表征字符串边界，构建了字符串对象的格结构，并定义了高效的抽象字符串操作，用于分析JavaScript字符串处理程序。


<details>
  <summary>Details</summary>
Motivation: 需要一种能够精确分析字符串操作程序的抽象域，特别是针对JavaScript等动态语言中的字符串处理，传统方法在处理复杂字符串操作时存在精度不足的问题。

Method: 定义字符串区间抽象域，使用字方程表示下界、字不等式表示上界；构建基于长度非递增态射的字符串属性半格；设计多种约简策略形成格结构；定义高效的抽象字符串操作。

Result: 建立了字符串对象域的格结构，设计了计算开销较小的抽象操作，能够有效分析JavaScript字符串处理程序的属性。

Conclusion: 该字符串区间抽象域为程序分析提供了精确且高效的字符串抽象方法，特别适用于JavaScript等语言的字符串操作分析。

Abstract: We introduce a string-interval abstract domain, where string intervals are
characterized by systems of word equations (encoding lower bounds on string
values) and word disequalities (encoding upper bounds). Building upon the
lattice structure of string intervals, we define an abstract string object as a
reduced product on a string property semilattice, determined by
length-non-increasing morphisms. We consider several reduction strategies for
abstract string objects and show that upon these strategies the string object
domain forms a lattice. We define basic abstract string operations on the
domain, aiming to minimize computational overheads on the reduction, and show
how the domain can be used to analyse properties of JavaScript string
manipulating programs.

</details>


### [11] [HUGR: A Quantum-Classical Intermediate Representation](https://arxiv.org/abs/2510.11420)
*Mark Koch,Agustín Borgna,Seyon Sivarajah,Alan Lawrence,Alec Edgington,Douglas Wilson,Craig Roy,Luca Mondada,Lukas Heidemann,Ross Duncan*

Main category: cs.PL

TL;DR: HUGR是一种新颖的基于图的中间表示，用于混合量子-经典程序，具有高表达性和可扩展性，支持多级抽象编译和安全性保证。


<details>
  <summary>Details</summary>
Motivation: 为了捕捉近端和未来量子计算设备的能力，以及新兴量子编程范式中的抽象概念，需要一种机器友好且支持强大编译技术的中间表示。

Method: 设计基于图的层次化统一图表示（HUGR），受MLIR启发，支持多级抽象编译，包含严格的静态类型和线性量子类型等安全保证。

Result: 开发了完整的HUGR规范和开源参考实现，支持模式匹配编译技术，能够平滑地在不同抽象级别之间进行程序转换。

Conclusion: HUGR为量子编译工具提供了安全、可扩展的中间表示，有助于快速开发编译工具而不用担心程序失效。

Abstract: We introduce the Hierarchical Unified Graph Representation (HUGR): a novel
graph based intermediate representation for mixed quantum-classical programs.
HUGR's design features high expressivity and extensibility to capture the
capabilities of near-term and forthcoming quantum computing devices, as well as
new and evolving abstractions from novel quantum programming paradigms. The
graph based structure is machine-friendly and supports powerful pattern
matching based compilation techniques. Inspired by MLIR, HUGR's extensibility
further allows compilation tooling to reason about programs at multiple levels
of abstraction, lowering smoothly between them. Safety guarantees in the
structure including strict, static typing and linear quantum types allow rapid
development of compilation tooling without fear of program invalidation. A full
specification of HUGR and reference implementation are open-source and
available online.

</details>


### [12] [(Dis)Proving Spectre Security with Speculation-Passing Style](https://arxiv.org/abs/2510.11573)
*Santiago Arranz-Olmos,Gilles Barthe,Lionel Blatter,Xingyu Xie,Zhiyuan Zhang*

Main category: cs.PL

TL;DR: 本文提出了一种称为推测传递风格(SPS)的程序转换方法，将推测常数时间(SCT)验证简化为常数时间(CT)验证，从而可以利用现有CT验证工具来检测Spectre漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有的推测常数时间(SCT)验证工具通常是从常数时间(CT)工具提升而来，但这些提升缺乏精确定义和形式化分析。本文旨在填补这一空白，为这些提升建立形式化基础。

Method: 引入推测传递风格(SPS)程序转换，通过为程序添加对应攻击者控制预测的新输入，并修改程序以遵循这些预测，从而将SCT验证问题转化为CT验证问题。

Result: SPS转换是完备的，即程序是SCT当且仅当其SPS转换是CT。通过将SPS与EasyCrypt、BINSEC和ctgrind三种CT验证工具结合，在Spectre-v1基准测试中进行了评估。

Conclusion: SPS方法为SCT验证提供了形式化基础，能够有效利用现有CT验证工具来检测Spectre漏洞，并可扩展到其他Spectre变种和泄漏模型。

Abstract: Constant-time (CT) verification tools are commonly used for detecting
potential side-channel vulnerabilities in cryptographic libraries. Recently, a
new class of tools, called speculative constant-time (SCT) tools, has also been
used for detecting potential Spectre vulnerabilities. In many cases, these SCT
tools have emerged as liftings of CT tools. However, these liftings are seldom
defined precisely and are almost never analyzed formally. The goal of this
paper is to address this gap, by developing formal foundations for these
liftings, and to demonstrate that these foundations can yield practical
benefits.
  Concretely, we introduce a program transformation, coined Speculation-Passing
Style (SPS), for reducing SCT verification to CT verification. Essentially, the
transformation instruments the program with a new input that corresponds to
attacker-controlled predictions and modifies the program to follow them. This
approach is sound and complete, in the sense that a program is SCT if and only
if its SPS transform is CT. Thus, we can leverage existing CT verification
tools to prove SCT; we illustrate this by combining SPS with three standard
methodologies for CT verification, namely reducing it to non-interference,
assertion safety and dynamic taint analysis. We realize these combinations with
three existing tools, EasyCrypt, BINSEC, and ctgrind, and we evaluate them on
Kocher's benchmarks for Spectre-v1. Our results focus on Spectre-v1 in the
standard CT leakage model; however, we also discuss applications of our method
to other variants of Spectre and other leakage models.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [13] [Hound: Relation-First Knowledge Graphs for Complex-System Reasoning in Security Audits](https://arxiv.org/abs/2510.09633)
*Bernhard Mueller*

Main category: cs.CR

TL;DR: Hound是一个关系优先的图引擎，通过分析师定义的视图和持久信念系统，在复杂代码库中实现精确检索和系统级推理，显著提高了漏洞检测的召回率和F1分数。


<details>
  <summary>Details</summary>
Motivation: 解决复杂代码库中系统级推理的挑战，传统方法难以跨组件分析抽象关系（如价值流、权限角色等），需要更灵活的图表示和持续的证据积累机制。

Method: 采用关系优先的图引擎，支持分析师定义视图和紧凑标注；建立持久信念系统跟踪漏洞假设；使用覆盖vs直觉规划和QA最终确认机制。

Result: 在ScaBench的五个项目上，相比基线LLM分析器，微召回率从8.3%提升到31.2%，F1分数从9.8%提升到14.2%，但精度略有下降。

Conclusion: 关系优先的图表示和假设中心循环有效扩展了模型理解能力，支持跨组件分析抽象方面，显著改善了系统级漏洞检测性能。

Abstract: Hound introduces a relation-first graph engine that improves system-level
reasoning across interrelated components in complex codebases. The agent
designs flexible, analyst-defined views with compact annotations (e.g.,
monetary/value flows, authentication/authorization roles, call graphs, protocol
invariants) and uses them to anchor exact retrieval: for any question, it loads
precisely the code that matters (often across components) so it can zoom out to
system structure and zoom in to the decisive lines. A second contribution is a
persistent belief system: long-lived vulnerability hypotheses whose confidence
is updated as evidence accrues. The agent employs coverage-versus-intuition
planning and a QA finalizer to confirm or reject hypotheses. On a five-project
subset of ScaBench[1], Hound improves recall and F1 over a baseline LLM
analyzer (micro recall 31.2% vs. 8.3%; F1 14.2% vs. 9.8%) with a modest
precision trade-off. We attribute these gains to flexible, relation-first
graphs that extend model understanding beyond call/dataflow to abstract
aspects, plus the hypothesis-centric loop; code and artifacts are released to
support reproduction.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [14] [OBsmith: Testing JavaScript Obfuscator using LLM-powered sketching](https://arxiv.org/abs/2510.10066)
*Shan Jiang,Chenguang Zhu,Sarfraz Khurshid*

Main category: cs.SE

TL;DR: OBsmith是一个使用大语言模型系统测试JavaScript混淆器正确性的框架，发现了11个未知的正确性bug，比现有模糊测试工具更有效


<details>
  <summary>Details</summary>
Motivation: JavaScript混淆器广泛用于保护知识产权和抵抗逆向工程，但其正确性一直被忽视。不正确的转换可能静默改变程序功能，破坏可靠性和安全性，违背混淆的根本目的

Method: OBsmith利用LLM生成程序草图（抽象模板），实例化为可执行程序，在不同配置下进行混淆测试。同时从真实程序中自动提取草图，支持项目特定功能的针对性测试

Result: 发现了11个先前未知的正确性bug。在相同程序预算下，五个最先进的JavaScript模糊测试工具都无法检测到这些问题。消融实验表明除通用元态关系外，所有组件都对至少一个bug类有贡献

Conclusion: OBsmith是自动化测试混淆器和其他语义保持工具链质量保证的重要一步，结果还引发了关于如何平衡混淆预设和性能成本的讨论

Abstract: JavaScript obfuscators are widely deployed to protect intellectual property
and resist reverse engineering, yet their correctness has been largely
overlooked compared to performance and resilience. Existing evaluations
typically measure resistance to deobfuscation, leaving the critical question of
whether obfuscators preserve program semantics unanswered. Incorrect
transformations can silently alter functionality, compromise reliability, and
erode security-undermining the very purpose of obfuscation. To address this
gap, we present OBsmith, a novel framework to systematically test JavaScript
obfuscators using large language models (LLMs). OBsmith leverages LLMs to
generate program sketches abstract templates capturing diverse language
constructs, idioms, and corner cases-which are instantiated into executable
programs and subjected to obfuscation under different configurations. Besides
LLM-powered sketching, OBsmith also employs a second source: automatic
extraction of sketches from real programs. This extraction path enables more
focused testing of project specific features and lets developers inject domain
knowledge into the resulting test cases. OBsmith uncovers 11 previously unknown
correctness bugs. Under an equal program budget, five general purpose
state-of-the-art JavaScript fuzzers (FuzzJIT, Jsfunfuzz, Superion, DIE,
Fuzzilli) failed to detect these issues, highlighting OBsmith's complementary
focus on obfuscation induced misbehavior. An ablation shows that all components
except our generic MRs contribute to at least one bug class; the negative MR
result suggests the need for obfuscator-specific metamorphic relations. Our
results also seed discussion on how to balance obfuscation presets and
performance cost. We envision OBsmith as an important step towards automated
testing and quality assurance of obfuscators and other semantic-preserving
toolchains.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [15] [Proceedings Twentieth International Workshop on Logical Frameworks and Meta-Languages: Theory and Practice](https://arxiv.org/abs/2510.11199)
*Kaustuv Chaudhuri,Daniele Nantes-Sobrinho*

Main category: cs.LO

TL;DR: LFMTP 2025国际研讨会论文集，包含逻辑框架和元语言理论与实践的贡献论文


<details>
  <summary>Details</summary>
Motivation: 促进逻辑框架和元语言领域的研究交流与发展

Method: 作为FSCD会议的卫星活动，通过学术研讨会形式组织论文展示和讨论

Result: 汇集了该领域的最新研究成果，为研究者提供了交流平台

Conclusion: LFMTP 2025成功举办了第20届国际研讨会，推动了逻辑框架和元语言领域的发展

Abstract: These are the contributed papers presented at the 20th International Workshop
on Logical Frameworks and Meta-Languages: Theory and Practice (LFMTP 2025), at
Birmingham, UK on 19 July as a satellite event of the FSCD conference. The
program committee for this edition of LFMTP was chaired by Kaustuv Chaudhuri
and Daniele Nantes-Sobrinho. More information about LFMTP can be found on
https://lfmtp.org.

</details>
