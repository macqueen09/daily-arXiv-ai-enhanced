<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]
- [cs.CL](#cs.CL) [Total: 1]
- [cs.SE](#cs.SE) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [HITrees: Higher-Order Interaction Trees](https://arxiv.org/abs/2510.14558)
*Amir Mohammad Fadaei Ayyam,Michael Sammler*

Main category: cs.PL

TL;DR: 提出了高阶交互树(HITrees)，这是第一个在非守卫类型理论中支持高阶效应的交互树变体，通过两种关键技术实现：设计效应概念以表达高阶输入的定点为归纳类型，以及使用反函数化将高阶输出编码为一阶表示。


<details>
  <summary>Details</summary>
Motivation: 现有交互树的效应概念不支持高阶效应（即接受或返回单子计算的效应），而这些效应对于建模并行组合和call/cc等复杂语义特性至关重要。

Method: 1. 设计效应概念使得高阶输入效应的定点可以在类型理论中表示为归纳类型；2. 使用反函数化技术将高阶输出编码为一阶表示；3. 在Lean证明助手中实现HITrees，并构建包含并发、递归和call/cc等效应的综合库。

Result: 成功实现了HITrees系统，提供了两种解释：作为状态转换系统和作为单子程序。通过定义具有并行组合和call/cc的语言语义，展示了HITrees的表达能力。

Conclusion: HITrees是第一个在非守卫类型理论中支持高阶效应的交互树变体，为建模复杂语义特性（如并行组合和call/cc）提供了有效的组合语义基础。

Abstract: Recent years have witnessed the rise of compositional semantics as a
foundation for formal verification of complex systems. In particular,
interaction trees have emerged as a popular denotational semantics. Interaction
trees achieve compositionality by providing a reusable library of effects.
However, their notion of effects does not support higher-order effects, i.e.,
effects that take or return monadic computations. Such effects are essential to
model complex semantic features like parallel composition and call/cc.
  We introduce Higher-Order Interaction Trees (HITrees), the first variant of
interaction trees to support higher-order effects in a non-guarded type theory.
HITrees accomplish this through two key techniques: first, by designing the
notion of effects such that the fixpoints of effects with higher-order input
can be expressed as inductive types inside the type theory; and second, using
defunctionalization to encode higher-order outputs into a first-order
representation. We implement HITrees in the Lean proof assistant, accompanied
by a comprehensive library of effects including concurrency, recursion, and
call/cc. Furthermore, we provide two interpretations of HITrees, as state
transition systems and as monadic programs. To demonstrate the expressiveness
of HITrees, we apply them to define the semantics of a language with parallel
composition and call/cc.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [2] [Tawa: Automatic Warp Specialization for Modern GPUs with Asynchronous References](https://arxiv.org/abs/2510.14719)
*Hongzheng Chen,Bin Fan,Alexander Collins,Bastian Hagedorn,Evghenii Gaburov,Masahiro Masuda,Matthew Brookhart,Chris Sullivan,Jason Knight,Zhiru Zhang,Vinod Grover*

Main category: cs.LG

TL;DR: Tawa是一个自动编译器，能从高级瓦片化程序生成高性能的warp专用代码，通过异步引用(aref)抽象简化GPU数据流编程，在H100 GPU上实现优于cuBLAS和Triton的性能。


<details>
  <summary>Details</summary>
Motivation: 现代GPU的专用硬件单元支持高性能异步数据流执行，但传统的SIMT编程模型与此任务并行硬件不匹配，导致程序性差距。硬件级warp专用化虽然能解锁峰值性能，但需要开发者手动编排复杂的低级通信和软件管道，工作量大且易出错。

Method: 提出Tawa自动编译器，采用新颖的异步引用(aref)IR抽象来表达warp级通信，无需暴露低级硬件细节。Tawa自动将程序划分为生产者-消费者角色，并管理复杂的数据流管道。

Result: 在NVIDIA H100 GPU上评估代表性LLM内核，Tawa实现了高硬件利用率，相比高度优化的cuBLAS GEMM内核达到1.1倍加速。对于注意力工作负载，相比Triton达到1.2倍加速，并与手工优化的CUTLASS C++ FlashAttention-3内核性能相当，但编程工作量大幅减少。

Conclusion: Tawa通过自动化warp专用代码生成，有效解决了GPU数据流编程的复杂性，在保持高性能的同时显著降低了开发难度。

Abstract: Modern GPUs feature specialized hardware units that enable high-performance,
asynchronous dataflow execution. However, the conventional SIMT programming
model is fundamentally misaligned with this task-parallel hardware, creating a
significant programmability gap. While hardware-level warp specialization is
the key to unlocking peak performance, it forces developers to manually
orchestrate complex, low-level communication and software pipelines--a process
that is labor-intensive, error-prone, and unsustainable. To address this
challenge, we present Tawa, an automated compiler that systematically generates
high-performance, warp-specialized code from a high-level, tile-based program.
Central to our approach is a novel IR abstraction, asynchronous references
(aref), which expresses warp-level communication without exposing low-level
hardware details. Using this abstraction, Tawa automatically partitions
programs into producer-consumer roles and manages the intricate dataflow
pipeline, relieving developers of invasive kernel rewriting. Evaluation on
NVIDIA H100 GPUs across representative LLM kernels shows that Tawa delivers
high hardware utilization, achieving up to 1.1$\times$ speedup over highly
optimized cuBLAS GEMM kernels. For attention workloads, Tawa attains
1.2$\times$ speedup over Triton and matches the performance of the
hand-optimized CUTLASS C++ FlashAttention-3 kernel with far less programming
effort.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [3] [TokDrift: When LLM Speaks in Subwords but Code Speaks in Grammar](https://arxiv.org/abs/2510.14972)
*Yinxi Li,Yuntian Deng,Pengyu Nie*

Main category: cs.CL

TL;DR: 论文提出了TokDrift框架，发现代码LLM对语义相同但格式不同的代码会产生不同的tokenization，导致模型行为显著变化，这揭示了subword tokenization与代码语法不匹配的问题。


<details>
  <summary>Details</summary>
Motivation: 当前代码LLM使用的subword tokenizer（如BPE）基于统计而非语法，导致语义相同的代码片段可能因格式差异（如空格、变量命名）而被不同地tokenize，影响模型可靠性。

Method: 引入TokDrift框架，应用语义保持的重写规则创建仅在tokenization上不同的代码变体，分析九个代码LLM（包括超过300亿参数的大模型）的行为变化。

Result: 即使微小的格式变化也会导致模型行为显著偏移，层间分析显示问题源于早期嵌入层，subword分割未能捕获语法token边界。

Conclusion: tokenization与语法不对齐是代码理解和生成的隐藏障碍，未来代码LLM需要语法感知的tokenization方法。

Abstract: Large language models (LLMs) for code rely on subword tokenizers, such as
byte-pair encoding (BPE), learned from mixed natural language text and
programming language code but driven by statistics rather than grammar. As a
result, semantically identical code snippets can be tokenized differently
depending on superficial factors such as whitespace or identifier naming. To
measure the impact of this misalignment, we introduce TokDrift, a framework
that applies semantic-preserving rewrite rules to create code variants
differing only in tokenization. Across nine code LLMs, including large ones
with over 30B parameters, even minor formatting changes can cause substantial
shifts in model behavior. Layer-wise analysis shows that the issue originates
in early embeddings, where subword segmentation fails to capture grammar token
boundaries. Our findings identify misaligned tokenization as a hidden obstacle
to reliable code understanding and generation, highlighting the need for
grammar-aware tokenization for future code LLMs.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [4] [Caruca: Effective and Efficient Specification Mining for Opaque Software Components](https://arxiv.org/abs/2510.14279)
*Evangelos Lamprou,Seong-Heon Jung,Mayank Keoliya,Lukas Lazarek,Konstantinos Kallas,Michael Greenberg,Nikos Vasilakis*

Main category: cs.SE

TL;DR: Caruca是一个自动为不透明命令挖掘规范的系统，通过LLM翻译文档、探索有效调用空间、系统调用拦截来提取命令属性，消除了手动规范创建的需求。


<details>
  <summary>Details</summary>
Motivation: 现有系统需要手动创建命令规范，这个过程费时费力且容易出错，限制了这些系统的实用性。

Method: 使用大语言模型将命令文档翻译为结构化调用语法，探索语法有效的命令调用和执行环境，通过系统调用和文件系统拦截提取命令属性。

Result: 在60个GNU Coreutils、POSIX和第三方命令上测试，Caruca为除一个案例外的所有情况生成了正确规范，完全消除了手动工作。

Conclusion: Caruca能够自动生成可立即使用的命令规范，为依赖规范的现有系统提供支持，显著提高了实用性。

Abstract: A wealth of state-of-the-art systems demonstrate impressive improvements in
performance, security, and reliability on programs composed of opaque
components, such as Unix shell commands. To reason about commands, these
systems require partial specifications. However, creating such specifications
is a manual, laborious, and error-prone process, limiting the practicality of
these systems. This paper presents Caruca, a system for automatic specification
mining for opaque commands. To overcome the challenge of language diversity
across commands, Caruca first instruments a large language model to translate a
command's user-facing documentation into a structured invocation syntax. Using
this representation, Caruca explores the space of syntactically valid command
invocations and execution environments. Caruca concretely executes each
command-environment pair, interposing at the system-call and filesystem level
to extract key command properties such as parallelizability and filesystem pre-
and post-conditions. These properties can be exported in multiple specification
formats and are immediately usable by existing systems. Applying Caruca across
60 GNU Coreutils, POSIX, and third-party commands across several
specification-dependent systems shows that Caruca generates correct
specifications for all but one case, completely eliminating manual effort from
the process and currently powering the full specifications for a
state-of-the-art static analysis tool.

</details>
