{"id": "2510.14558", "pdf": "https://arxiv.org/pdf/2510.14558", "abs": "https://arxiv.org/abs/2510.14558", "authors": ["Amir Mohammad Fadaei Ayyam", "Michael Sammler"], "title": "HITrees: Higher-Order Interaction Trees", "categories": ["cs.PL"], "comment": null, "summary": "Recent years have witnessed the rise of compositional semantics as a\nfoundation for formal verification of complex systems. In particular,\ninteraction trees have emerged as a popular denotational semantics. Interaction\ntrees achieve compositionality by providing a reusable library of effects.\nHowever, their notion of effects does not support higher-order effects, i.e.,\neffects that take or return monadic computations. Such effects are essential to\nmodel complex semantic features like parallel composition and call/cc.\n  We introduce Higher-Order Interaction Trees (HITrees), the first variant of\ninteraction trees to support higher-order effects in a non-guarded type theory.\nHITrees accomplish this through two key techniques: first, by designing the\nnotion of effects such that the fixpoints of effects with higher-order input\ncan be expressed as inductive types inside the type theory; and second, using\ndefunctionalization to encode higher-order outputs into a first-order\nrepresentation. We implement HITrees in the Lean proof assistant, accompanied\nby a comprehensive library of effects including concurrency, recursion, and\ncall/cc. Furthermore, we provide two interpretations of HITrees, as state\ntransition systems and as monadic programs. To demonstrate the expressiveness\nof HITrees, we apply them to define the semantics of a language with parallel\ncomposition and call/cc."}
{"id": "2510.14279", "pdf": "https://arxiv.org/pdf/2510.14279", "abs": "https://arxiv.org/abs/2510.14279", "authors": ["Evangelos Lamprou", "Seong-Heon Jung", "Mayank Keoliya", "Lukas Lazarek", "Konstantinos Kallas", "Michael Greenberg", "Nikos Vasilakis"], "title": "Caruca: Effective and Efficient Specification Mining for Opaque Software Components", "categories": ["cs.SE", "cs.PL"], "comment": null, "summary": "A wealth of state-of-the-art systems demonstrate impressive improvements in\nperformance, security, and reliability on programs composed of opaque\ncomponents, such as Unix shell commands. To reason about commands, these\nsystems require partial specifications. However, creating such specifications\nis a manual, laborious, and error-prone process, limiting the practicality of\nthese systems. This paper presents Caruca, a system for automatic specification\nmining for opaque commands. To overcome the challenge of language diversity\nacross commands, Caruca first instruments a large language model to translate a\ncommand's user-facing documentation into a structured invocation syntax. Using\nthis representation, Caruca explores the space of syntactically valid command\ninvocations and execution environments. Caruca concretely executes each\ncommand-environment pair, interposing at the system-call and filesystem level\nto extract key command properties such as parallelizability and filesystem pre-\nand post-conditions. These properties can be exported in multiple specification\nformats and are immediately usable by existing systems. Applying Caruca across\n60 GNU Coreutils, POSIX, and third-party commands across several\nspecification-dependent systems shows that Caruca generates correct\nspecifications for all but one case, completely eliminating manual effort from\nthe process and currently powering the full specifications for a\nstate-of-the-art static analysis tool."}
{"id": "2510.14719", "pdf": "https://arxiv.org/pdf/2510.14719", "abs": "https://arxiv.org/abs/2510.14719", "authors": ["Hongzheng Chen", "Bin Fan", "Alexander Collins", "Bastian Hagedorn", "Evghenii Gaburov", "Masahiro Masuda", "Matthew Brookhart", "Chris Sullivan", "Jason Knight", "Zhiru Zhang", "Vinod Grover"], "title": "Tawa: Automatic Warp Specialization for Modern GPUs with Asynchronous References", "categories": ["cs.LG", "cs.AR", "cs.PL"], "comment": null, "summary": "Modern GPUs feature specialized hardware units that enable high-performance,\nasynchronous dataflow execution. However, the conventional SIMT programming\nmodel is fundamentally misaligned with this task-parallel hardware, creating a\nsignificant programmability gap. While hardware-level warp specialization is\nthe key to unlocking peak performance, it forces developers to manually\norchestrate complex, low-level communication and software pipelines--a process\nthat is labor-intensive, error-prone, and unsustainable. To address this\nchallenge, we present Tawa, an automated compiler that systematically generates\nhigh-performance, warp-specialized code from a high-level, tile-based program.\nCentral to our approach is a novel IR abstraction, asynchronous references\n(aref), which expresses warp-level communication without exposing low-level\nhardware details. Using this abstraction, Tawa automatically partitions\nprograms into producer-consumer roles and manages the intricate dataflow\npipeline, relieving developers of invasive kernel rewriting. Evaluation on\nNVIDIA H100 GPUs across representative LLM kernels shows that Tawa delivers\nhigh hardware utilization, achieving up to 1.1$\\times$ speedup over highly\noptimized cuBLAS GEMM kernels. For attention workloads, Tawa attains\n1.2$\\times$ speedup over Triton and matches the performance of the\nhand-optimized CUTLASS C++ FlashAttention-3 kernel with far less programming\neffort."}
{"id": "2510.14972", "pdf": "https://arxiv.org/pdf/2510.14972", "abs": "https://arxiv.org/abs/2510.14972", "authors": ["Yinxi Li", "Yuntian Deng", "Pengyu Nie"], "title": "TokDrift: When LLM Speaks in Subwords but Code Speaks in Grammar", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PL", "cs.SE"], "comment": null, "summary": "Large language models (LLMs) for code rely on subword tokenizers, such as\nbyte-pair encoding (BPE), learned from mixed natural language text and\nprogramming language code but driven by statistics rather than grammar. As a\nresult, semantically identical code snippets can be tokenized differently\ndepending on superficial factors such as whitespace or identifier naming. To\nmeasure the impact of this misalignment, we introduce TokDrift, a framework\nthat applies semantic-preserving rewrite rules to create code variants\ndiffering only in tokenization. Across nine code LLMs, including large ones\nwith over 30B parameters, even minor formatting changes can cause substantial\nshifts in model behavior. Layer-wise analysis shows that the issue originates\nin early embeddings, where subword segmentation fails to capture grammar token\nboundaries. Our findings identify misaligned tokenization as a hidden obstacle\nto reliable code understanding and generation, highlighting the need for\ngrammar-aware tokenization for future code LLMs."}
