{"id": "2510.15912", "pdf": "https://arxiv.org/pdf/2510.15912", "abs": "https://arxiv.org/abs/2510.15912", "authors": ["Jack Cashman"], "title": "Latency Based Tiling", "categories": ["cs.PL", "cs.AR", "cs.PF"], "comment": null, "summary": "Latency Based Tiling provides a systems based approach to deriving\napproximate tiling solution that maximizes locality while maintaining a fast\ncompile time. The method uses triangular loops to characterize miss ratio\nscaling of a machine avoiding prefetcher distortion. Miss ratio scaling\ncaptures the relationship between data access latency and working set size with\nsharp increases in latency indicating the data footprint exceeds capacity from\na cache level. Through these noticeable increases in latency we can determine\nan approximate location for L1, L2, and L3 memory sizes. These sizes are\nexpected to be under approximations of a systems true memory sizes which is in\nline with our expectations given the shared nature of cache in a multi process\nsystem as described in defensive loop tiling. Unlike auto tuning, which can be\neffective but prohibitively slow, Latency Based Tiling achieves negligible\ncompile time overhead. The implementation in Rust enables a hardware agnostic\napproach which combined with a cache timing based techniques, yields a\nportable, memory safe system running wherever Rust is supported. The tiling\nstrategy is applied to a subset of the polyhedral model, where loop nestings\nare tiled based on both the derived memory hierarchy and the observed data\nfootprint per iteration."}
{"id": "2510.16133", "pdf": "https://arxiv.org/pdf/2510.16133", "abs": "https://arxiv.org/abs/2510.16133", "authors": ["Daniel Sainati", "Joseph W. Cutler", "Benjamin C. Pierce", "Stephanie Weirich"], "title": "Typing Strictness (Extended Version)", "categories": ["cs.PL"], "comment": "30 pages, 22 figures, extended version of a paper to be published at\n  POPL 2026", "summary": "Strictness analysis is critical to efficient implementation of languages with\nnon-strict evaluation, mitigating much of the performance overhead of laziness.\nHowever, reasoning about strictness at the source level can be challenging and\nunintuitive. We propose a new definition of strictness that refines the\ntraditional one by describing variable usage more precisely. We lay\ntype-theoretic foundations for this definition in both call-by-name and\ncall-by-push-value settings, drawing inspiration from the literature on type\nsystems tracking effects and coeffects. We prove via a logical relation that\nthe strictness attributes computed by our type systems accurately describe the\nuse of variables at runtime, and we offer a strictness-annotation-preserving\ntranslation from the call-by-name system to the call-by-push-value one. All our\nresults are mechanized in Rocq."}
{"id": "2510.16594", "pdf": "https://arxiv.org/pdf/2510.16594", "abs": "https://arxiv.org/abs/2510.16594", "authors": ["Moida Praneeth Jain", "Venkatesh Choppella"], "title": "SimpliPy: A Source-Tracking Notional Machine for Simplified Python", "categories": ["cs.PL", "F.3.2; F.1.1"], "comment": "15 pages, 1 figure, 1 table. Accepted at the 4th Workshop on Research\n  Highlights in Programming Languages (RHPL 2025), co-located with FSTTCS 2025.\n  Code available at: https://github.com/PraneethJain/simplipy", "summary": "Misconceptions about program execution hinder many novice programmers. We\nintroduce SimpliPy, a notional machine designed around a carefully chosen\nPython subset to clarify core control flow and scoping concepts. Its foundation\nis a precise operational semantics that explicitly tracks source code line\nnumbers for each execution step, making the link between code and behavior\nunambiguous. Complementing the dynamic semantics, SimpliPy uses static analysis\nto generate Control Flow Graphs (CFGs) and identify lexical scopes, helping\nstudents build a structural understanding before tracing. We also present an\ninteractive web-based debugger built on these principles. This tool embodies\nthe formal techniques, visualizing the operational state (environments, stack)\nand using the static CFG to animate control flow directly on the graph during\nstep-by-step execution. SimpliPy thus integrates formal semantics, program\nanalysis, and visualization to offer both a pedagogical approach and a\npractical demonstration of applying formal methods to program understanding."}
{"id": "2510.16883", "pdf": "https://arxiv.org/pdf/2510.16883", "abs": "https://arxiv.org/abs/2510.16883", "authors": ["Giulia Giusti", "Michele Pagani"], "title": "JAX Autodiff from a Linear Logic Perspective (Extended Version)", "categories": ["cs.PL", "cs.LO"], "comment": null, "summary": "Autodiff refers to the core of the automatic differentiation systems\ndeveloped in projects like JAX and Dex. Autodiff has recently been formalised\nin a linear typed calculus by Radul et al in arXiv:2204.10923. Although this\nformalisation suffices to express the main program transformations of Autodiff,\nthe calculus is very specific to this task, and it is not clear whether the\ntype system yields a substructural logic that has interest on its own.\n  We propose an encoding of Autodiff into a linear $\\lambda$-calculus that\nenjoys a Curry-Howard correspondence with Girard's linear logic. We prove that\nthe encoding is sound both qualitatively (the encoded terms are extensionally\nequivalent to the original ones) and quantitatively (the encoding preserves the\noriginal work cost as described in arXiv:2204.10923). As a byproduct, we show\nthat unzipping, one of the transformations used to implement backpropagation in\nAutodiff, is, in fact, optional."}
{"id": "2510.17429", "pdf": "https://arxiv.org/pdf/2510.17429", "abs": "https://arxiv.org/abs/2510.17429", "authors": ["Jin Sano", "Naoki Yamamoto", "Kazunori Ueda"], "title": "Introducing Linear Implication Types to $Î»_{GT}$ for Computing With Incomplete Graphs", "categories": ["cs.PL", "D.3.1"], "comment": "26 pages, 14 figures, This paper is submitted to PRO2025-3", "summary": "Designing programming languages that enable intuitive and safe manipulation\nof data structures is a critical research challenge. Conventional destructive\nmemory operations using pointers are complex and prone to errors. Existing type\nsystems, such as affine types and shape types, address this problem towards\nsafe manipulation of heaps and pointers, but design of high-level declarative\nlanguages that allow us to manipulate complex pointer data structures at a\nhigher level of abstraction is largely an open problem. The $\\lambda_{GT}$\nlanguage, a purely functional programming language that treats hypergraphs\n(hereafter referred to as graphs) as primary data structures, addresses some of\nthese challenges. By abstracting data with shared references and cycles as\ngraphs, it enables declarative operations through pattern matching and\nleverages its type system to guarantee safety of these operations.\nNevertheless, the previously proposed type system of $\\lambda_{GT}$ leaves two\nsignificant open challenges. First, the type system does not support\n\\emph{incomplete graphs}, that is, graphs in which some elements are missing\nfrom the graphs of user-defined types. Second, the type system relies on\ndynamic type checking during pattern matching. This study addresses these two\nchallenges by incorporating linear implication into the $\\lambda_{GT}$ type\nsystem, while introducing new constraints to ensure its soundness."}
{"id": "2510.17505", "pdf": "https://arxiv.org/pdf/2510.17505", "abs": "https://arxiv.org/abs/2510.17505", "authors": ["Jaeyeon Won", "Willow Ahrens", "Joel S. Emer", "Saman Amarasinghe"], "title": "Insum: Sparse GPU Kernels Simplified and Optimized with Indirect Einsums", "categories": ["cs.PL", "cs.PF"], "comment": null, "summary": "Programming high-performance sparse GPU kernels is notoriously difficult,\nrequiring both substantial effort and deep expertise. Sparse compilers aim to\nsimplify this process, but existing systems fall short in two key ways. First,\nthey are primarily designed for CPUs and rarely produce high-performance GPU\ncode. Second, when computations involve both sparse and dense regions, these\ncompilers often fail to optimize the dense portions effectively. In this paper,\nwe propose a new approach for expressing sparse computations. We start from\nformat-agnostic Einsums over sparse tensors and rewrite them into\nformat-conscious indirect Einsums, which explicitly encode format information\nby mapping sparse data and metadata onto dense tensor operations through\nindirect indexing. To execute indirect Einsums, we introduce the Insum\ncompiler, which generates efficient GPU code for these Einsums by lowering to\nthe PyTorch compiler, extended to better support Tensor Core-enabled indirect\nEinsums. We also present two fixed-length sparse formats, GroupCOO and\nBlockGroupCOO, designed to fit naturally with indirect Einsums. Our approach\nachieves 1.14x to 3.81x speedups across a range of sparse GPU applications\nwhile reducing lines of code by 202x to 4491x compared to hand-written\nimplementations."}
{"id": "2510.15914", "pdf": "https://arxiv.org/pdf/2510.15914", "abs": "https://arxiv.org/abs/2510.15914", "authors": ["Jiayu Zhao", "Song Chen"], "title": "VeriGRAG: Enhancing LLM-Based Verilog Code Generation with Structure-Aware Soft Prompts", "categories": ["cs.AR", "cs.AI", "cs.PL"], "comment": "9 pages, 5 figures", "summary": "Large language models (LLMs) have demonstrated strong capabilities in\ngenerating Verilog code from natural language descriptions. However, Verilog\ncode inherently encodes structural information of hardware circuits.\nEffectively leveraging this structural information to enhance the functional\nand syntactic correctness of LLM-generated Verilog code remains a significant\nchallenge. To address this challenge, we propose VeriGRAG , a novel framework\nthat extracts structural graph embeddings from Verilog code using graph neural\nnetworks (GNNs). A multimodal retriever then selects the graph embeddings most\nrelevant to the given generation task, which are aligned with the code modality\nthrough the VeriFormer module to generate structure-aware soft prompts. Our\nexperiments demonstrate that VeriGRAG substantially improves the correctness of\nVerilog code generation, achieving state-of-the-art or superior performance\nacross both VerilogEval and RTLLM benchmarks."}
{"id": "2510.16357", "pdf": "https://arxiv.org/pdf/2510.16357", "abs": "https://arxiv.org/abs/2510.16357", "authors": ["Jugal Gajjar", "Kamalasankari Subramaniakuppusamy"], "title": "MLCPD: A Unified Multi-Language Code Parsing Dataset with Universal AST Schema", "categories": ["cs.SE", "cs.LG", "cs.PL"], "comment": "12 pages, 7 figures, 4 tables, 2 algorithms, and 34 references.\n  HuggingFace:\n  https://huggingface.co/datasets/jugalgajjar/MultiLang-Code-Parser-Dataset\n  GitHub: https://github.com/JugalGajjar/MultiLang-Code-Parser-Dataset", "summary": "We introduce the MultiLang Code Parser Dataset (MLCPD), a large-scale,\nlanguage-agnostic dataset unifying syntactic and structural representations of\ncode across ten major programming languages. MLCPD contains over seven million\nparsed source files normalized under our proposed universal Abstract Syntax\nTree (AST) schema, enabling consistent cross-language reasoning, structural\nlearning, and multilingual software analysis. Unlike existing corpora that\nfocus purely on token-level code or isolated parsers, MLCPD provides both\nhierarchical tree representations and rich metadata for every file, ensuring\nlossless syntactic coverage and structural uniformity. Each entry includes a\nnormalized schema, language-level metadata, and abstracted node semantics\nstored in Parquet format for scalable retrieval. Empirical analyses reveal\nstrong cross-language structural regularities-demonstrating that syntactic\ngraphs from languages as diverse as Python, Java, and Go can be aligned under a\nshared schema. We release the dataset publicly on Hugging Face and the\naccompanying codebase on GitHub, which includes complete pipelines for dataset\nreproduction, grammar compilation, and a visualization tool for exploring the\nunified AST across languages. Together, these resources establish MLCPD as an\nopen, reproducible foundation for future research in cross-language\nrepresentation learning and program analysis."}
{"id": "2510.16809", "pdf": "https://arxiv.org/pdf/2510.16809", "abs": "https://arxiv.org/abs/2510.16809", "authors": ["Amirkia Rafiei Oskooei", "Kaan Baturalp Cosdan", "Husamettin Isiktas", "Mehmet S. Aktas"], "title": "When Many-Shot Prompting Fails: An Empirical Study of LLM Code Translation", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.PL", "68T50, 68N30, 68W40", "I.2.7; D.2.7; I.2.6"], "comment": null, "summary": "Large Language Models (LLMs) with vast context windows offer new avenues for\nin-context learning (ICL), where providing many examples (\"many-shot\"\nprompting) is often assumed to enhance performance. We investigate this\nassumption for the complex task of code translation. Through a large-scale\nempirical study of over 90,000 translations, we systematically evaluate the\nimpact of scaling in-context examples from zero-shot to many-shot\nconfigurations of up to 625 examples, with prompts spanning from approximately\n100,000 to 800,000 tokens. Our findings reveal a \"many-shot paradox\": while\nstatic similarity metrics may modestly improve with more examples, functional\ncorrectness consistently peaks with few-shot prompting (5-25 examples).\nProviding substantially more examples often degrades this crucial functional\nperformance. This study highlights that for code translation, the quality of a\nfew well-chosen examples outweighs sheer quantity, challenging the universal\nefficacy of \"more is better\" for ICL and underscoring the task-dependent nature\nof optimal prompting strategies. Our results have significant implications for\neffectively leveraging LLMs in software engineering."}
{"id": "2510.17220", "pdf": "https://arxiv.org/pdf/2510.17220", "abs": "https://arxiv.org/abs/2510.17220", "authors": ["Giulia Giusti"], "title": "Exploiting the Potential of Linearity in Automatic Differentiation and Computational Cryptography", "categories": ["cs.CR", "cs.LO", "cs.PL"], "comment": null, "summary": "The concept of linearity plays a central role in both mathematics and\ncomputer science, with distinct yet complementary meanings. In mathematics,\nlinearity underpins functions and vector spaces, forming the foundation of\nlinear algebra and functional analysis. In computer science, it relates to\nresource-sensitive computation. Linear Logic (LL), for instance, models\nassumptions that must be used exactly once, providing a natural framework for\ntracking computational resources such as time, memory, or data access. This\ndual perspective makes linearity essential to programming languages, type\nsystems, and formal models that express both computational complexity and\ncomposability. Bridging these interpretations enables rigorous yet practical\nmethodologies for analyzing and verifying complex systems.\n  This thesis explores the use of LL to model programming paradigms based on\nlinearity. It comprises two parts: ADLL and CryptoBLL. The former applies LL to\nAutomatic Differentiation (AD), modeling linear functions over the reals and\nthe transposition operation. The latter uses LL to express complexity\nconstraints on adversaries in computational cryptography.\n  In AD, two main approaches use linear type systems: a theoretical one\ngrounded in proof theory, and a practical one implemented in JAX, a Python\nlibrary developed by Google for machine learning research. In contrast,\nframeworks like PyTorch and TensorFlow support AD without linear types. ADLL\naims to bridge theory and practice by connecting JAX's type system to LL.\n  In modern cryptography, several calculi aim to model cryptographic proofs\nwithin the computational paradigm. These efforts face a trade-off between\nexpressiveness, to capture reductions, and simplicity, to abstract probability\nand complexity. CryptoBLL addresses this tension by proposing a framework for\nthe automatic analysis of protocols in computational cryptography."}
