<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 6]
- [cs.SE](#cs.SE) [Total: 2]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Latency Based Tiling](https://arxiv.org/abs/2510.15912)
*Jack Cashman*

Main category: cs.PL

TL;DR: Latency Based Tiling 是一种系统方法，通过三角循环表征机器失配率缩放来推导近似分块解，在保持快速编译时间的同时最大化局部性。


<details>
  <summary>Details</summary>
Motivation: 解决传统自动调优方法虽然有效但编译时间过长的问题，提供一种硬件无关的快速分块策略。

Method: 使用三角循环避免预取器失真，通过失配率缩放捕捉数据访问延迟与工作集大小的关系，根据延迟显著增加确定L1、L2、L3缓存大小的近似位置。

Result: 实现了可忽略的编译时间开销，在Rust中实现硬件无关方法，结合缓存计时技术，在支持Rust的任何地方提供可移植、内存安全的系统。

Conclusion: 该方法在多面体模型的子集中应用分块策略，基于推导的内存层次结构和观察到的每次迭代数据足迹进行循环嵌套分块，提供了一种高效的近似分块解决方案。

Abstract: Latency Based Tiling provides a systems based approach to deriving
approximate tiling solution that maximizes locality while maintaining a fast
compile time. The method uses triangular loops to characterize miss ratio
scaling of a machine avoiding prefetcher distortion. Miss ratio scaling
captures the relationship between data access latency and working set size with
sharp increases in latency indicating the data footprint exceeds capacity from
a cache level. Through these noticeable increases in latency we can determine
an approximate location for L1, L2, and L3 memory sizes. These sizes are
expected to be under approximations of a systems true memory sizes which is in
line with our expectations given the shared nature of cache in a multi process
system as described in defensive loop tiling. Unlike auto tuning, which can be
effective but prohibitively slow, Latency Based Tiling achieves negligible
compile time overhead. The implementation in Rust enables a hardware agnostic
approach which combined with a cache timing based techniques, yields a
portable, memory safe system running wherever Rust is supported. The tiling
strategy is applied to a subset of the polyhedral model, where loop nestings
are tiled based on both the derived memory hierarchy and the observed data
footprint per iteration.

</details>


### [2] [Typing Strictness (Extended Version)](https://arxiv.org/abs/2510.16133)
*Daniel Sainati,Joseph W. Cutler,Benjamin C. Pierce,Stephanie Weirich*

Main category: cs.PL

TL;DR: 提出了一种新的严格性定义，通过更精确地描述变量使用来改进传统定义，在按名调用和按值推送调用设置中建立了类型理论基础，并通过逻辑关系证明严格性属性准确描述了运行时变量使用。


<details>
  <summary>Details</summary>
Motivation: 严格性分析对于非严格求值语言的高效实现至关重要，但源级别的严格性推理具有挑战性和反直觉性。需要更精确的严格性定义来改进传统方法。

Method: 在按名调用和按值推送调用设置中建立类型理论基础，借鉴跟踪效果和共效果的类型系统文献，使用逻辑关系证明严格性属性的准确性，并提供严格性注释保留的翻译。

Result: 开发了新的严格性定义和类型系统，能够更精确地描述变量使用，并通过逻辑关系验证了运行时行为的准确性。

Conclusion: 提出的严格性定义和类型系统为严格性分析提供了更精确的理论基础，所有结果都在Rocq中机械化验证。

Abstract: Strictness analysis is critical to efficient implementation of languages with
non-strict evaluation, mitigating much of the performance overhead of laziness.
However, reasoning about strictness at the source level can be challenging and
unintuitive. We propose a new definition of strictness that refines the
traditional one by describing variable usage more precisely. We lay
type-theoretic foundations for this definition in both call-by-name and
call-by-push-value settings, drawing inspiration from the literature on type
systems tracking effects and coeffects. We prove via a logical relation that
the strictness attributes computed by our type systems accurately describe the
use of variables at runtime, and we offer a strictness-annotation-preserving
translation from the call-by-name system to the call-by-push-value one. All our
results are mechanized in Rocq.

</details>


### [3] [SimpliPy: A Source-Tracking Notional Machine for Simplified Python](https://arxiv.org/abs/2510.16594)
*Moida Praneeth Jain,Venkatesh Choppella*

Main category: cs.PL

TL;DR: SimpliPy是一个为Python子集设计的教学性概念机器，通过精确的操作语义和静态分析来澄清程序执行中的控制流和作用域概念，并提供了基于Web的交互式调试器来可视化执行过程。


<details>
  <summary>Details</summary>
Motivation: 解决新手程序员对程序执行的误解，特别是控制流和作用域等核心概念的理解困难。

Method: 设计SimpliPy概念机器，包含精确的操作语义（显式跟踪源代码行号）、静态分析生成控制流图(CFG)和识别词法作用域，并开发基于Web的交互式调试器来可视化执行状态。

Result: 开发了一个集成形式语义、程序分析和可视化的教学工具，能够清晰地展示代码与执行行为之间的联系。

Conclusion: SimpliPy成功地将形式语义、程序分析和可视化技术整合，为程序理解提供了有效的教学方法和实际应用演示。

Abstract: Misconceptions about program execution hinder many novice programmers. We
introduce SimpliPy, a notional machine designed around a carefully chosen
Python subset to clarify core control flow and scoping concepts. Its foundation
is a precise operational semantics that explicitly tracks source code line
numbers for each execution step, making the link between code and behavior
unambiguous. Complementing the dynamic semantics, SimpliPy uses static analysis
to generate Control Flow Graphs (CFGs) and identify lexical scopes, helping
students build a structural understanding before tracing. We also present an
interactive web-based debugger built on these principles. This tool embodies
the formal techniques, visualizing the operational state (environments, stack)
and using the static CFG to animate control flow directly on the graph during
step-by-step execution. SimpliPy thus integrates formal semantics, program
analysis, and visualization to offer both a pedagogical approach and a
practical demonstration of applying formal methods to program understanding.

</details>


### [4] [JAX Autodiff from a Linear Logic Perspective (Extended Version)](https://arxiv.org/abs/2510.16883)
*Giulia Giusti,Michele Pagani*

Main category: cs.PL

TL;DR: 本文提出了一种将Autodiff编码到线性λ演算的方法，该编码与Girard的线性逻辑具有Curry-Howard对应关系，并证明了编码的定性和定量正确性。


<details>
  <summary>Details</summary>
Motivation: 现有的Autodiff形式化虽然足以表达主要程序变换，但其类型系统非常特定于该任务，不清楚是否产生具有独立兴趣的子结构逻辑。

Method: 将Autodiff编码到线性λ演算中，该演算与线性逻辑具有Curry-Howard对应关系，并证明编码的定性和定量正确性。

Result: 编码在定性和定量上都是正确的，并且发现反向传播中使用的unzipping变换实际上是可选的。

Conclusion: 提出的编码方法不仅连接了Autodiff与线性逻辑，还揭示了实现细节中的冗余，为自动微分系统提供了更简洁的理论基础。

Abstract: Autodiff refers to the core of the automatic differentiation systems
developed in projects like JAX and Dex. Autodiff has recently been formalised
in a linear typed calculus by Radul et al in arXiv:2204.10923. Although this
formalisation suffices to express the main program transformations of Autodiff,
the calculus is very specific to this task, and it is not clear whether the
type system yields a substructural logic that has interest on its own.
  We propose an encoding of Autodiff into a linear $\lambda$-calculus that
enjoys a Curry-Howard correspondence with Girard's linear logic. We prove that
the encoding is sound both qualitatively (the encoded terms are extensionally
equivalent to the original ones) and quantitatively (the encoding preserves the
original work cost as described in arXiv:2204.10923). As a byproduct, we show
that unzipping, one of the transformations used to implement backpropagation in
Autodiff, is, in fact, optional.

</details>


### [5] [Introducing Linear Implication Types to $λ_{GT}$ for Computing With Incomplete Graphs](https://arxiv.org/abs/2510.17429)
*Jin Sano,Naoki Yamamoto,Kazunori Ueda*

Main category: cs.PL

TL;DR: 该研究通过在线性类型系统λ_GT中引入线性蕴含，解决了对不完整图的支持问题，并消除了模式匹配中的动态类型检查需求。


<details>
  <summary>Details</summary>
Motivation: 现有λ_GT语言的类型系统存在两个主要问题：不支持不完整图（即用户定义类型图中缺少某些元素的图），以及在模式匹配中依赖动态类型检查。

Method: 在λ_GT类型系统中引入线性蕴含，并添加新的约束条件来确保类型系统的正确性。

Result: 成功扩展了λ_GT类型系统，使其能够支持不完整图，同时消除了模式匹配中的动态类型检查需求。

Conclusion: 通过引入线性蕴含和相应约束，显著提升了λ_GT语言类型系统的表达能力和安全性。

Abstract: Designing programming languages that enable intuitive and safe manipulation
of data structures is a critical research challenge. Conventional destructive
memory operations using pointers are complex and prone to errors. Existing type
systems, such as affine types and shape types, address this problem towards
safe manipulation of heaps and pointers, but design of high-level declarative
languages that allow us to manipulate complex pointer data structures at a
higher level of abstraction is largely an open problem. The $\lambda_{GT}$
language, a purely functional programming language that treats hypergraphs
(hereafter referred to as graphs) as primary data structures, addresses some of
these challenges. By abstracting data with shared references and cycles as
graphs, it enables declarative operations through pattern matching and
leverages its type system to guarantee safety of these operations.
Nevertheless, the previously proposed type system of $\lambda_{GT}$ leaves two
significant open challenges. First, the type system does not support
\emph{incomplete graphs}, that is, graphs in which some elements are missing
from the graphs of user-defined types. Second, the type system relies on
dynamic type checking during pattern matching. This study addresses these two
challenges by incorporating linear implication into the $\lambda_{GT}$ type
system, while introducing new constraints to ensure its soundness.

</details>


### [6] [Insum: Sparse GPU Kernels Simplified and Optimized with Indirect Einsums](https://arxiv.org/abs/2510.17505)
*Jaeyeon Won,Willow Ahrens,Joel S. Emer,Saman Amarasinghe*

Main category: cs.PL

TL;DR: 提出了一种新的稀疏计算表达方法，通过将格式无关的稀疏张量Einsums重写为格式感知的间接Einsums，并开发Insum编译器生成高效GPU代码，在稀疏GPU应用中实现显著性能提升和代码简化。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏编译器主要针对CPU设计，难以生成高性能GPU代码，且在稀疏和密集混合计算中无法有效优化密集部分。

Method: 从格式无关的稀疏张量Einsums出发，重写为格式感知的间接Einsums，通过间接索引将稀疏数据和元数据映射到密集张量操作，并开发Insum编译器生成GPU代码。

Result: 在稀疏GPU应用中实现1.14x到3.81x的加速，相比手写实现减少202x到4491x的代码行数。

Conclusion: 提出的间接Einsums方法和Insum编译器能够有效解决稀疏GPU计算中的性能优化和编程复杂度问题。

Abstract: Programming high-performance sparse GPU kernels is notoriously difficult,
requiring both substantial effort and deep expertise. Sparse compilers aim to
simplify this process, but existing systems fall short in two key ways. First,
they are primarily designed for CPUs and rarely produce high-performance GPU
code. Second, when computations involve both sparse and dense regions, these
compilers often fail to optimize the dense portions effectively. In this paper,
we propose a new approach for expressing sparse computations. We start from
format-agnostic Einsums over sparse tensors and rewrite them into
format-conscious indirect Einsums, which explicitly encode format information
by mapping sparse data and metadata onto dense tensor operations through
indirect indexing. To execute indirect Einsums, we introduce the Insum
compiler, which generates efficient GPU code for these Einsums by lowering to
the PyTorch compiler, extended to better support Tensor Core-enabled indirect
Einsums. We also present two fixed-length sparse formats, GroupCOO and
BlockGroupCOO, designed to fit naturally with indirect Einsums. Our approach
achieves 1.14x to 3.81x speedups across a range of sparse GPU applications
while reducing lines of code by 202x to 4491x compared to hand-written
implementations.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [7] [MLCPD: A Unified Multi-Language Code Parsing Dataset with Universal AST Schema](https://arxiv.org/abs/2510.16357)
*Jugal Gajjar,Kamalasankari Subramaniakuppusamy*

Main category: cs.SE

TL;DR: MLCPD是一个大规模、语言无关的代码解析数据集，统一了10种主要编程语言的语法和结构表示，包含超过700万个解析后的源代码文件，采用统一的抽象语法树模式。


<details>
  <summary>Details</summary>
Motivation: 现有语料库主要关注词元级代码或孤立解析器，缺乏统一的跨语言结构表示，需要建立一个能够支持跨语言推理、结构学习和多语言软件分析的数据集。

Method: 提出通用抽象语法树模式，对10种编程语言的源代码进行解析和归一化处理，每个条目包含归一化模式、语言级元数据和抽象节点语义，以Parquet格式存储便于扩展检索。

Result: 经验分析显示存在强烈的跨语言结构规律性，证明Python、Java、Go等不同语言的语法图可以在共享模式下对齐。数据集已在Hugging Face公开发布，代码库在GitHub上提供。

Conclusion: MLCPD为跨语言表示学习和程序分析的未来研究建立了一个开放、可复现的基础。

Abstract: We introduce the MultiLang Code Parser Dataset (MLCPD), a large-scale,
language-agnostic dataset unifying syntactic and structural representations of
code across ten major programming languages. MLCPD contains over seven million
parsed source files normalized under our proposed universal Abstract Syntax
Tree (AST) schema, enabling consistent cross-language reasoning, structural
learning, and multilingual software analysis. Unlike existing corpora that
focus purely on token-level code or isolated parsers, MLCPD provides both
hierarchical tree representations and rich metadata for every file, ensuring
lossless syntactic coverage and structural uniformity. Each entry includes a
normalized schema, language-level metadata, and abstracted node semantics
stored in Parquet format for scalable retrieval. Empirical analyses reveal
strong cross-language structural regularities-demonstrating that syntactic
graphs from languages as diverse as Python, Java, and Go can be aligned under a
shared schema. We release the dataset publicly on Hugging Face and the
accompanying codebase on GitHub, which includes complete pipelines for dataset
reproduction, grammar compilation, and a visualization tool for exploring the
unified AST across languages. Together, these resources establish MLCPD as an
open, reproducible foundation for future research in cross-language
representation learning and program analysis.

</details>


### [8] [When Many-Shot Prompting Fails: An Empirical Study of LLM Code Translation](https://arxiv.org/abs/2510.16809)
*Amirkia Rafiei Oskooei,Kaan Baturalp Cosdan,Husamettin Isiktas,Mehmet S. Aktas*

Main category: cs.SE

TL;DR: 本文研究了代码翻译任务中的多示例提示效果，发现存在"多示例悖论"：虽然静态相似度指标随示例增加略有提升，但功能正确性在5-25个示例时达到峰值，更多示例反而会降低性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的长上下文窗口为多示例学习提供了新机会，但代码翻译任务中多示例提示的有效性假设尚未得到系统验证。

Method: 通过大规模实证研究，评估了超过90,000次翻译，系统测试了从零示例到625个示例的提示配置，提示长度从约10万到80万token不等。

Result: 研究发现功能正确性在少量示例（5-25个）时达到最佳，而提供更多示例会降低这一关键性能指标。

Conclusion: 对于代码翻译任务，少量精心选择的示例质量比数量更重要，挑战了"越多越好"的通用假设，强调了最优提示策略的任务依赖性。

Abstract: Large Language Models (LLMs) with vast context windows offer new avenues for
in-context learning (ICL), where providing many examples ("many-shot"
prompting) is often assumed to enhance performance. We investigate this
assumption for the complex task of code translation. Through a large-scale
empirical study of over 90,000 translations, we systematically evaluate the
impact of scaling in-context examples from zero-shot to many-shot
configurations of up to 625 examples, with prompts spanning from approximately
100,000 to 800,000 tokens. Our findings reveal a "many-shot paradox": while
static similarity metrics may modestly improve with more examples, functional
correctness consistently peaks with few-shot prompting (5-25 examples).
Providing substantially more examples often degrades this crucial functional
performance. This study highlights that for code translation, the quality of a
few well-chosen examples outweighs sheer quantity, challenging the universal
efficacy of "more is better" for ICL and underscoring the task-dependent nature
of optimal prompting strategies. Our results have significant implications for
effectively leveraging LLMs in software engineering.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [9] [VeriGRAG: Enhancing LLM-Based Verilog Code Generation with Structure-Aware Soft Prompts](https://arxiv.org/abs/2510.15914)
*Jiayu Zhao,Song Chen*

Main category: cs.AR

TL;DR: VeriGRAG是一个通过图神经网络提取Verilog代码结构信息，并利用多模态检索器选择相关图嵌入来生成结构感知软提示的框架，显著提升了LLM生成的Verilog代码的正确性。


<details>
  <summary>Details</summary>
Motivation: Verilog代码编码了硬件电路的结构信息，但现有方法未能有效利用这些结构信息来提升LLM生成的Verilog代码的功能和语法正确性。

Method: 使用GNN提取Verilog代码的结构图嵌入，通过多模态检索器选择相关图嵌入，并通过VeriFormer模块将其与代码模态对齐生成结构感知软提示。

Result: 在VerilogEval和RTLLM基准测试中实现了最先进或更优的性能，显著提高了Verilog代码生成的正确性。

Conclusion: VeriGRAG框架通过有效利用Verilog代码的结构信息，成功提升了LLM生成代码的质量和正确性。

Abstract: Large language models (LLMs) have demonstrated strong capabilities in
generating Verilog code from natural language descriptions. However, Verilog
code inherently encodes structural information of hardware circuits.
Effectively leveraging this structural information to enhance the functional
and syntactic correctness of LLM-generated Verilog code remains a significant
challenge. To address this challenge, we propose VeriGRAG , a novel framework
that extracts structural graph embeddings from Verilog code using graph neural
networks (GNNs). A multimodal retriever then selects the graph embeddings most
relevant to the given generation task, which are aligned with the code modality
through the VeriFormer module to generate structure-aware soft prompts. Our
experiments demonstrate that VeriGRAG substantially improves the correctness of
Verilog code generation, achieving state-of-the-art or superior performance
across both VerilogEval and RTLLM benchmarks.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [10] [Exploiting the Potential of Linearity in Automatic Differentiation and Computational Cryptography](https://arxiv.org/abs/2510.17220)
*Giulia Giusti*

Main category: cs.CR

TL;DR: 该论文探讨了线性逻辑在编程范式中的应用，分为ADLL和CryptoBLL两部分，分别应用于自动微分和计算密码学领域。


<details>
  <summary>Details</summary>
Motivation: 线性概念在数学和计算机科学中具有互补但不同的含义，数学中支撑函数和向量空间，计算机科学中涉及资源敏感计算。线性逻辑能够建模必须恰好使用一次的假设，为跟踪计算资源提供自然框架。

Method: ADLL部分将线性逻辑应用于自动微分，建模实数上的线性函数和转置操作；CryptoBLL部分使用线性逻辑表达计算密码学中对手的复杂性约束。

Result: ADLL旨在通过将JAX的类型系统与线性逻辑连接来弥合理论与实践的差距；CryptoBLL提出了一个用于计算密码学中协议自动分析的框架。

Conclusion: 通过线性逻辑的应用，该论文为分析复杂系统提供了严谨而实用的方法，特别是在自动微分和计算密码学领域。

Abstract: The concept of linearity plays a central role in both mathematics and
computer science, with distinct yet complementary meanings. In mathematics,
linearity underpins functions and vector spaces, forming the foundation of
linear algebra and functional analysis. In computer science, it relates to
resource-sensitive computation. Linear Logic (LL), for instance, models
assumptions that must be used exactly once, providing a natural framework for
tracking computational resources such as time, memory, or data access. This
dual perspective makes linearity essential to programming languages, type
systems, and formal models that express both computational complexity and
composability. Bridging these interpretations enables rigorous yet practical
methodologies for analyzing and verifying complex systems.
  This thesis explores the use of LL to model programming paradigms based on
linearity. It comprises two parts: ADLL and CryptoBLL. The former applies LL to
Automatic Differentiation (AD), modeling linear functions over the reals and
the transposition operation. The latter uses LL to express complexity
constraints on adversaries in computational cryptography.
  In AD, two main approaches use linear type systems: a theoretical one
grounded in proof theory, and a practical one implemented in JAX, a Python
library developed by Google for machine learning research. In contrast,
frameworks like PyTorch and TensorFlow support AD without linear types. ADLL
aims to bridge theory and practice by connecting JAX's type system to LL.
  In modern cryptography, several calculi aim to model cryptographic proofs
within the computational paradigm. These efforts face a trade-off between
expressiveness, to capture reductions, and simplicity, to abstract probability
and complexity. CryptoBLL addresses this tension by proposing a framework for
the automatic analysis of protocols in computational cryptography.

</details>
