{"id": "2510.17889", "pdf": "https://arxiv.org/pdf/2510.17889", "abs": "https://arxiv.org/abs/2510.17889", "authors": ["Eilene Tomkins-Flanagan", "Mary A. Kelly"], "title": "Hey Pentti, We Did It!: A Fully Vector-Symbolic Lisp", "categories": ["cs.PL", "cs.AI"], "comment": null, "summary": "Kanerva (2014) suggested that it would be possible to construct a complete\nLisp out of a vector-symbolic architecture. We present the general form of a\nvector-symbolic representation of the five Lisp elementary functions, lambda\nexpressions, and other auxiliary functions, found in the Lisp 1.5 specification\nMcCarthy (1960), which is near minimal and sufficient for Turing-completeness.\nOur specific implementation uses holographic reduced representations Plate\n(1995), with a lookup table cleanup memory. Lisp, as all Turing-complete\nlanguages, is a Cartesian closed category, unusual in its proximity to the\nmathematical abstraction. We discuss the mathematics, the purpose, and the\nsignificance of demonstrating vector-symbolic architectures' Cartesian-closure,\nas well as the importance of explicitly including cleanup memories in the\nspecification of the architecture."}
{"id": "2510.18479", "pdf": "https://arxiv.org/pdf/2510.18479", "abs": "https://arxiv.org/abs/2510.18479", "authors": ["Samuel Chassot", "Viktor Kunčak"], "title": "ZipLex: Verified Invertible Lexing with Memoized Derivatives and Zippers", "categories": ["cs.PL", "cs.FL"], "comment": null, "summary": "We present ZipLex, a verified framework for invertible lexical analysis.\nUnlike past verified lexers that focus only on satisfying the semantics of\nregular expressions and the maximal munch property, ZipLex also guarantees that\nlexing and printing are mutual inverses. Our design relies on two sets of\nideas: (1) a new abstraction of token sequences that captures the separability\nof tokens in a sequence while supporting their efficient manipulation, and (2)\na combination of verified data structures and optimizations, including Huet's\nzippers and memoized derivatives, to achieve practical performance. We\nimplemented ZipLex in Scala and verified its correctness, including\ninvertibility, using the Stainless verifier. Our evaluation demonstrates that\nZipLex supports realistic applications such as JSON processing and lexers of\nprogramming languages. In comparison to other verified lexers (which do not\nenforce invertibility), ZipLex is 4x slower than Coqlex and two orders of\nmagnitude faster than Verbatim++, showing that verified invertibility can be\nachieved without prohibitive cost."}
{"id": "2510.18651", "pdf": "https://arxiv.org/pdf/2510.18651", "abs": "https://arxiv.org/abs/2510.18651", "authors": ["Uraz Odyurt", "Ömer Sayilir", "Mariëlle Stoelinga", "Vadim Zaytsev"], "title": "CPSLint: A Domain-Specific Language Providing Data Validation and Sanitisation for Industrial Cyber-Physical Systems", "categories": ["cs.PL", "cs.SE"], "comment": null, "summary": "Raw datasets are often too large and unstructured to work with directly, and\nrequire a data preparation process. The domain of industrial Cyber-Physical\nSystems (CPS) is no exception, as raw data typically consists of large amounts\nof time-series data logging the system's status in regular time intervals. Such\ndata has to be sanity checked and preprocessed to be consumable by data-centric\nworkflows. We introduce CPSLint, a Domain-Specific Language designed to provide\ndata preparation for industrial CPS. We build up on the fact that many raw data\ncollections in the CPS domain require similar actions to render them suitable\nfor Machine-Learning (ML) solutions, e.g., Fault Detection and Identification\n(FDI) workflows, yet still vary enough to hope for one universally applicable\nsolution.\n  CPSLint's main features include type checking and enforcing constraints\nthrough validation and remediation for data columns, such as imputing missing\ndata from surrounding rows. More advanced features cover inference of extra\nCPS-specific data structures, both column-wise and row-wise. For instance, as\nrow-wise structures, descriptive execution phases are an effective method of\ndata compartmentalisation are extracted and prepared for ML-assisted FDI\nworkflows. We demonstrate CPSLint's features through a proof of concept\nimplementation."}
{"id": "2510.18418", "pdf": "https://arxiv.org/pdf/2510.18418", "abs": "https://arxiv.org/abs/2510.18418", "authors": ["Nathanaëlle Courant", "Xavier Leroy"], "title": "A Lazy, Concurrent Convertibility Checker", "categories": ["cs.LO", "cs.PL"], "comment": null, "summary": "Convertibility checking - determining whether two lambda-terms are equal up\nto reductions - is a crucial component of proof assistants and\ndependently-typed languages. Practical implementations often use heuristics to\nquickly conclude that two terms are or are not convertible without reducing\nthem to normal form. However, these heuristics can backfire, triggering huge\namounts of unnecessary computation. This paper presents a novel\nconvertibility-checking algorithm that relies crucially on laziness and\nconcurrency} Laziness is used to share computations, while concurrency is used\nto explore multiple convertibility subproblems in parallel or via fair\ninterleaving. Unlike heuristics-based approaches, our algorithm always finds an\neasy solution to the convertibility problem, if one exists. The paper presents\nthe algorithm in process calculus style and discusses its mechanized proof of\npartial correctness, its complexity, and its lightweight experimental\nevaluation."}
{"id": "2510.18496", "pdf": "https://arxiv.org/pdf/2510.18496", "abs": "https://arxiv.org/abs/2510.18496", "authors": ["Anamitra Ghorui", "Uday P. Khedker"], "title": "LatticeHashForest: An Efficient Data Structure for Repetitive Data and Operations", "categories": ["cs.DS", "cs.IT", "cs.OS", "cs.PL", "math.IT", "E.1; F.3.2; H.0"], "comment": "Author's original manuscript. 22 pages of main content. Submitted to\n  the Programming Journal (programming-journal.org)", "summary": "Analysis of entire programs as a single unit, or whole-program analysis,\ninvolves propagation of large amounts of information through the control flow\nof the program. This is especially true for pointer analysis, where, unless\nsignificant compromises are made in the precision of the analysis, there is a\ncombinatorial blowup of information. One of the key problems we observed in our\nown efforts is that a lot of duplicate data was being propagated, and many\nlow-level data structure operations were repeated a large number of times.\n  We present what we consider to be a novel and generic data structure,\nLatticeHashForest (LHF), to store and operate on such information in a manner\nthat eliminates a majority of redundant computations and duplicate data in\nscenarios similar to those encountered in compilers and program optimization.\nLHF differs from similar work in this vein, such as hash-consing, ZDDs, and\nBDDs, by not only providing a way to efficiently operate on large, aggregate\nstructures, but also modifying the elements of such structures in a manner that\nthey can be deduplicated immediately. LHF also provides a way to perform a\nnested construction of elements such that they can be deduplicated at multiple\nlevels, cutting down the need for additional, nested computations.\n  We provide a detailed structural description, along with an abstract model of\nthis data structure. An entire C++ implementation of LHF is provided as an\nartifact along with evaluations of LHF using examples and benchmark programs.\nWe also supply API documentation and a user manual for users to make\nindependent applications of LHF. Our main use case in the realm of pointer\nanalysis shows memory usage reduction to an almost negligible fraction, and\nspeedups beyond 4x for input sizes approaching 10 million when compared to\nother implementations."}
