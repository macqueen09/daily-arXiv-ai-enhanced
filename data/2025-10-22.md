<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 3]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Hey Pentti, We Did It!: A Fully Vector-Symbolic Lisp](https://arxiv.org/abs/2510.17889)
*Eilene Tomkins-Flanagan,Mary A. Kelly*

Main category: cs.PL

TL;DR: 该论文展示了如何在向量符号架构中实现完整的Lisp语言，包括五个基本函数、lambda表达式等，证明了向量符号架构的笛卡尔闭包性质。


<details>
  <summary>Details</summary>
Motivation: 验证Kanerva(2014)的假设，即可以在向量符号架构上构建完整的Lisp语言，并探讨向量符号架构的数学性质。

Method: 使用全息简化表示和查找表清理内存，实现了Lisp 1.5规范中的五个基本函数、lambda表达式等辅助函数。

Result: 成功构建了向量符号表示的Lisp实现，证明了向量符号架构具有笛卡尔闭包性质，能够支持图灵完备性。

Conclusion: 向量符号架构确实能够实现完整的Lisp语言，证明了其笛卡尔闭包性质，并强调了清理内存在架构规范中的重要性。

Abstract: Kanerva (2014) suggested that it would be possible to construct a complete
Lisp out of a vector-symbolic architecture. We present the general form of a
vector-symbolic representation of the five Lisp elementary functions, lambda
expressions, and other auxiliary functions, found in the Lisp 1.5 specification
McCarthy (1960), which is near minimal and sufficient for Turing-completeness.
Our specific implementation uses holographic reduced representations Plate
(1995), with a lookup table cleanup memory. Lisp, as all Turing-complete
languages, is a Cartesian closed category, unusual in its proximity to the
mathematical abstraction. We discuss the mathematics, the purpose, and the
significance of demonstrating vector-symbolic architectures' Cartesian-closure,
as well as the importance of explicitly including cleanup memories in the
specification of the architecture.

</details>


### [2] [ZipLex: Verified Invertible Lexing with Memoized Derivatives and Zippers](https://arxiv.org/abs/2510.18479)
*Samuel Chassot,Viktor Kunčak*

Main category: cs.PL

TL;DR: ZipLex是一个经过验证的可逆词法分析框架，不仅保证正则表达式语义和最大匹配属性，还确保词法分析和打印互为逆操作。


<details>
  <summary>Details</summary>
Motivation: 现有验证词法分析器只关注正则表达式语义和最大匹配属性，但缺乏对词法分析和打印互为逆操作的保证。

Method: 采用两种创新设计：(1)新的token序列抽象，捕获序列中token的可分离性并支持高效操作；(2)结合验证数据结构（Huet的zippers）和优化技术（记忆化导数）实现实用性能。

Result: 在Scala中实现ZipLex并使用Stainless验证器验证其正确性（包括可逆性）。评估显示ZipLex支持JSON处理和编程语言词法分析等实际应用，与其他验证词法分析器相比，比Coqlex慢4倍，但比Verbatim++快两个数量级。

Conclusion: 验证可逆性可以在不付出过高代价的情况下实现，ZipLex在性能和可逆性保证之间取得了良好平衡。

Abstract: We present ZipLex, a verified framework for invertible lexical analysis.
Unlike past verified lexers that focus only on satisfying the semantics of
regular expressions and the maximal munch property, ZipLex also guarantees that
lexing and printing are mutual inverses. Our design relies on two sets of
ideas: (1) a new abstraction of token sequences that captures the separability
of tokens in a sequence while supporting their efficient manipulation, and (2)
a combination of verified data structures and optimizations, including Huet's
zippers and memoized derivatives, to achieve practical performance. We
implemented ZipLex in Scala and verified its correctness, including
invertibility, using the Stainless verifier. Our evaluation demonstrates that
ZipLex supports realistic applications such as JSON processing and lexers of
programming languages. In comparison to other verified lexers (which do not
enforce invertibility), ZipLex is 4x slower than Coqlex and two orders of
magnitude faster than Verbatim++, showing that verified invertibility can be
achieved without prohibitive cost.

</details>


### [3] [CPSLint: A Domain-Specific Language Providing Data Validation and Sanitisation for Industrial Cyber-Physical Systems](https://arxiv.org/abs/2510.18651)
*Uraz Odyurt,Ömer Sayilir,Mariëlle Stoelinga,Vadim Zaytsev*

Main category: cs.PL

TL;DR: CPSLint是一种领域特定语言，专门为工业信息物理系统(CPS)的数据预处理设计，提供类型检查、约束验证和修复功能，支持机器学习工作流的数据准备。


<details>
  <summary>Details</summary>
Motivation: 工业CPS产生的原始时间序列数据通常庞大且非结构化，需要进行数据预处理才能用于机器学习解决方案，如故障检测和识别工作流。

Method: 开发了CPSLint领域特定语言，提供类型检查、约束验证和修复功能，包括缺失数据插补、CPS特定数据结构推断等高级特性。

Result: 通过概念验证实现展示了CPSLint的功能，能够有效提取和准备描述性执行阶段等行级数据结构，用于ML辅助的FDI工作流。

Conclusion: CPSLint为工业CPS数据预处理提供了一种有效的解决方案，能够处理相似但存在差异的数据准备需求，支持机器学习工作流的数据消费。

Abstract: Raw datasets are often too large and unstructured to work with directly, and
require a data preparation process. The domain of industrial Cyber-Physical
Systems (CPS) is no exception, as raw data typically consists of large amounts
of time-series data logging the system's status in regular time intervals. Such
data has to be sanity checked and preprocessed to be consumable by data-centric
workflows. We introduce CPSLint, a Domain-Specific Language designed to provide
data preparation for industrial CPS. We build up on the fact that many raw data
collections in the CPS domain require similar actions to render them suitable
for Machine-Learning (ML) solutions, e.g., Fault Detection and Identification
(FDI) workflows, yet still vary enough to hope for one universally applicable
solution.
  CPSLint's main features include type checking and enforcing constraints
through validation and remediation for data columns, such as imputing missing
data from surrounding rows. More advanced features cover inference of extra
CPS-specific data structures, both column-wise and row-wise. For instance, as
row-wise structures, descriptive execution phases are an effective method of
data compartmentalisation are extracted and prepared for ML-assisted FDI
workflows. We demonstrate CPSLint's features through a proof of concept
implementation.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [4] [LatticeHashForest: An Efficient Data Structure for Repetitive Data and Operations](https://arxiv.org/abs/2510.18496)
*Anamitra Ghorui,Uday P. Khedker*

Main category: cs.DS

TL;DR: 提出了一种名为LatticeHashForest（LHF）的新型数据结构，用于在编译器优化等场景中高效存储和操作大规模信息，显著减少冗余计算和重复数据。


<details>
  <summary>Details</summary>
Motivation: 在指针分析等全程序分析中，信息传播会导致大量重复数据和冗余计算，传统方法在精度和效率之间存在权衡。

Method: 设计了LHF数据结构，通过即时去重和多层嵌套构造来消除冗余，不同于哈希一致性、ZDDs和BDDs等现有技术。

Result: 在指针分析用例中，内存使用减少到几乎可忽略的程度，对于接近1000万规模的输入，速度提升超过4倍。

Conclusion: LHF是一种有效的数据结构，能够显著提高程序分析效率，特别适用于需要处理大规模信息的场景。

Abstract: Analysis of entire programs as a single unit, or whole-program analysis,
involves propagation of large amounts of information through the control flow
of the program. This is especially true for pointer analysis, where, unless
significant compromises are made in the precision of the analysis, there is a
combinatorial blowup of information. One of the key problems we observed in our
own efforts is that a lot of duplicate data was being propagated, and many
low-level data structure operations were repeated a large number of times.
  We present what we consider to be a novel and generic data structure,
LatticeHashForest (LHF), to store and operate on such information in a manner
that eliminates a majority of redundant computations and duplicate data in
scenarios similar to those encountered in compilers and program optimization.
LHF differs from similar work in this vein, such as hash-consing, ZDDs, and
BDDs, by not only providing a way to efficiently operate on large, aggregate
structures, but also modifying the elements of such structures in a manner that
they can be deduplicated immediately. LHF also provides a way to perform a
nested construction of elements such that they can be deduplicated at multiple
levels, cutting down the need for additional, nested computations.
  We provide a detailed structural description, along with an abstract model of
this data structure. An entire C++ implementation of LHF is provided as an
artifact along with evaluations of LHF using examples and benchmark programs.
We also supply API documentation and a user manual for users to make
independent applications of LHF. Our main use case in the realm of pointer
analysis shows memory usage reduction to an almost negligible fraction, and
speedups beyond 4x for input sizes approaching 10 million when compared to
other implementations.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [5] [A Lazy, Concurrent Convertibility Checker](https://arxiv.org/abs/2510.18418)
*Nathanaëlle Courant,Xavier Leroy*

Main category: cs.LO

TL;DR: 提出了一种基于惰性和并发性的新型可转换性检查算法，避免启发式方法可能导致的冗余计算问题


<details>
  <summary>Details</summary>
Motivation: 现有可转换性检查方法依赖启发式策略，虽然能快速判断lambda项是否可转换，但可能触发大量不必要的计算

Method: 采用进程演算风格设计算法，利用惰性实现计算共享，通过并发并行或公平交错探索多个可转换性子问题

Result: 算法总能找到可转换性问题的简单解（如果存在），并提供了部分正确性的机械化证明、复杂度分析和轻量级实验评估

Conclusion: 基于惰性和并发的算法为可转换性检查提供了可靠且高效的解决方案，避免了启发式方法的潜在问题

Abstract: Convertibility checking - determining whether two lambda-terms are equal up
to reductions - is a crucial component of proof assistants and
dependently-typed languages. Practical implementations often use heuristics to
quickly conclude that two terms are or are not convertible without reducing
them to normal form. However, these heuristics can backfire, triggering huge
amounts of unnecessary computation. This paper presents a novel
convertibility-checking algorithm that relies crucially on laziness and
concurrency} Laziness is used to share computations, while concurrency is used
to explore multiple convertibility subproblems in parallel or via fair
interleaving. Unlike heuristics-based approaches, our algorithm always finds an
easy solution to the convertibility problem, if one exists. The paper presents
the algorithm in process calculus style and discusses its mechanized proof of
partial correctness, its complexity, and its lightweight experimental
evaluation.

</details>
