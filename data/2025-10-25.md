<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 6]
- [cs.LG](#cs.LG) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Prompt Decorators: A Declarative and Composable Syntax for Reasoning, Formatting, and Control in LLMs](https://arxiv.org/abs/2510.19850)
*Mostapha Kalami Heris*

Main category: cs.PL

TL;DR: 提出Prompt Decorators框架，通过声明式、可组合的控制令牌来管理LLM行为，实现推理透明度提升、提示复杂度降低和模型行为标准化。


<details>
  <summary>Details</summary>
Motivation: 传统提示工程依赖冗长的自然语言指令，限制了可重复性、模块化和可解释性，用户缺乏对LLM推理和输出表达的一致控制。

Method: 引入Prompt Decorators语法，使用紧凑控制令牌（如+++Reasoning、+++Tone）来修改行为维度，定义20个核心装饰器，分为认知与生成、表达与系统两个功能家族，并建立统一语法、作用域模型和确定性处理流程。

Result: 通过解耦任务意图和执行行为，创建了可重用和可解释的提示设计接口，展示了改进的推理透明度、降低的提示复杂度和跨领域标准化模型行为。

Conclusion: 该框架对互操作性、行为一致性以及可扩展AI系统的声明式接口开发具有重要意义。

Abstract: Large Language Models (LLMs) are central to reasoning, writing, and
decision-support workflows, yet users lack consistent control over how they
reason and express outputs. Conventional prompt engineering relies on verbose
natural-language instructions, limiting reproducibility, modularity, and
interpretability. This paper introduces Prompt Decorators, a declarative,
composable syntax that governs LLM behavior through compact control tokens such
as +++Reasoning, +++Tone(style=formal), and +++Import(topic="Systems
Thinking"). Each decorator modifies a behavioral dimension, such as reasoning
style, structure, or tone, without changing task content. The framework
formalizes twenty core decorators organized into two functional families
(Cognitive & Generative and Expressive & Systemic), each further decomposed
into subcategories that govern reasoning, interaction, expression, and
session-control. It defines a unified syntax, scoping model, and deterministic
processing pipeline enabling predictable and auditable behavior composition. By
decoupling task intent from execution behavior, Prompt Decorators create a
reusable and interpretable interface for prompt design. Illustrative use cases
demonstrate improved reasoning transparency, reduced prompt complexity, and
standardized model behavior across domains. The paper concludes with
implications for interoperability, behavioral consistency, and the development
of declarative interfaces for scalable AI systems.

</details>


### [2] [A Specification's Realm: Characterizing the Knowledge Required for Executing a Given Algorithm Specification](https://arxiv.org/abs/2510.19853)
*Assaf Marron,David Harel*

Main category: cs.PL

TL;DR: 本文提出了算法规范的"领域"概念，即执行算法规范所需的前提知识集合，包括语法语义、领域知识、实体关系、因果规则等，这些知识可以整理成实用规模的文档。


<details>
  <summary>Details</summary>
Motivation: 算法规范需要足够清晰明确以支持机械执行，但现有规范往往缺乏对执行所需前提知识的系统描述。

Method: 提出算法规范领域的概念，通过系统分析过程识别和总结执行算法所需的知识，部分过程可利用大语言模型和现有文档实现自动化。

Result: 算法规范领域包含语法语义、受限领域知识、实体关系、因果规则和操作指令等要素，有助于算法在不同系统中的方法论实现和形式化验证。

Conclusion: 算法规范领域的概念为算法实现和验证提供了方法论基础，同时提出了执行忠实度评估的问题，这与正确性评估有所区别。

Abstract: An algorithm specification in natural language or pseudocode is expected to
be clear and explicit enough to enable mechanical execution. In this position
paper we contribute an initial characterization of the knowledge that an
executing agent, human or machine, should possess in order to be able to carry
out the instructions of a given algorithm specification as a stand-alone
entity, independent of any system implementation. We argue that, for that
algorithm specification, such prerequisite knowledge, whether unique or shared
with other specifications, can be summarized in a document of practical size.
We term this document the realm of the algorithm specification. The generation
of such a realm is itself a systematic analytical process, significant parts of
which can be automated with the help of large language models and the reuse of
existing documents. The algorithm-specification's realm would consist of
specification language syntax and semantics, domain knowledge restricted to the
referenced entities, inter-entity relationships, relevant underlying
cause-and-effect rules, and detailed instructions and means for carrying out
certain operations. Such characterization of the realm can contribute to
methodological implementation of the algorithm specification in diverse systems
and to its formalization for mechanical verification. The paper also touches
upon the question of assessing execution faithfulness, which is distinct from
correctness: in the absence of a reference interpretation of natural language
or pseudocode specification with a given vocabulary, how can we determine if an
observed agent's execution indeed complies with the input specification.

</details>


### [3] [Deconstructed Proto-Quipper: A Rational Reconstruction](https://arxiv.org/abs/2510.20018)
*Ryan Kavanagh,Chuta Sano,Brigitte Pientka*

Main category: cs.PL

TL;DR: Proto-Quipper-A是Proto-Quipper语言家族的重构版本，使用线性λ演算和伴随逻辑基础来简化量子电路编程，具有简单的操作语义和标准化特性。


<details>
  <summary>Details</summary>
Motivation: Proto-Quipper语言的操作语义复杂，依赖于集合论操作和新鲜名称生成，难以用标准编程语言技术进行推理和机械化。

Method: 使用线性λ演算描述量子电路，通过伴随逻辑基础将电路语言与线性/非线性函数语言集成，重构Proto-Quipper的电路编程抽象。

Result: Proto-Quipper-A具有简单的按值调用归约语义，被证明是标准化的，可以使用标准逻辑关系证明线性系统的标准化。

Conclusion: Proto-Quipper-A为Proto-Quipper语言提供了更易处理的基础，避免了现有线性逻辑关系的复杂性。

Abstract: The Proto-Quipper family of programming languages aims to provide a formal
foundation for the Quipper quantum programming language. Unfortunately,
Proto-Quipper languages have complex operational semantics: they are inherently
effectful, and they rely on set-theoretic operations and fresh name generation
to manipulate quantum circuits. This makes them difficult to reason about using
standard programming language techniques and, ultimately, to mechanize. We
introduce Proto-Quipper-A, a rational reconstruction of Proto-Quipper languages
for static circuit generation. It uses a linear $\lambda$-calculus to describe
quantum circuits with normal forms that closely correspond to box-and-wire
circuit diagrams. Adjoint-logical foundations integrate this circuit language
with a linear/non-linear functional language and let us reconstruct
Proto-Quipper's circuit programming abstractions using more primitive
adjoint-logical operations. Proto-Quipper-A enjoys a simple call-by-value
reduction semantics, and to illustrate its tractability as a foundation for
Proto-Quipper languages, we show that it is normalizing. We show how to use
standard logical relations to prove normalization of linear and substructural
systems, thereby avoiding the inherent complexity of existing linear logical
relations.

</details>


### [4] [Deciding not to Decide: Sound and Complete Effect Inference in the Presence of Higher-Rank Polymorphism](https://arxiv.org/abs/2510.20532)
*Patrycja Balik,Szymon Jędras,Piotr Polesiuk*

Main category: cs.PL

TL;DR: 提出了一种用于类型和效应系统的效应推断算法，支持子类型化、高阶多态性和直观的集合语义，通过将效应约束转换为命题逻辑公式来处理作用域问题。


<details>
  <summary>Details</summary>
Motivation: 类型和效应系统尚未广泛采用，因为现有推断算法在表达能力、直观性和可判定性之间需要妥协，需要更复杂但实用的解决方案。

Method: 通过将效应约束转换为命题逻辑公式来延迟求解，处理高阶多态性的作用域问题，并在Rocq证明助手中形式化验证。

Result: 算法在声明性类型和效应系统中被证明具有完备性和正确性，并已在现实编程语言中成功实现。

Conclusion: 该工作展示了在保持表达能力的同时实现直观效应语义的可行性，为类型和效应系统的实际应用提供了基础。

Abstract: Type-and-effect systems help the programmer to organize data and
computational effects in a program. While for traditional type systems
expressive variants with sophisticated inference algorithms have been developed
and widely used in programming languages, type-and-effect systems did not yet
gain widespread adoption. One reason for this is that type-and-effect systems
are more complex and the existing inference algorithms make compromises between
expressiveness, intuitiveness, and decidability. In this work, we present an
effect inference algorithm for a type-and-effect system with subtyping,
expressive higher-rank polymorphism, and intuitive set-like semantics of
effects. In order to deal with scoping issues of higher-rank polymorphism, we
delay solving of effect constraints by transforming them into formulae of
propositional logic. We prove soundness and completeness of our algorithm with
respect to a declarative type-and-effect system. All the presented results have
been formalized in the Rocq proof assistant, and the algorithm has been
successfully implemented in a realistic programming language.

</details>


### [5] [Compiling the Mimosa programming language to RTOS tasks](https://arxiv.org/abs/2510.20547)
*Nikolaus Huber,Susanne Graf,Philipp Rümmer,Wang Yi*

Main category: cs.PL

TL;DR: 提出了Mimosa编程语言的编译方案，基于MIMOS计算模型，将嵌入式系统软件描述为时间触发进程集合，通过FIFO队列通信。


<details>
  <summary>Details</summary>
Motivation: 为Mimosa语言开发形式化的编译方案，将协调层映射到实时操作系统原语。

Method: 采用Lustre编译方案的适配版本，针对Mimosa语义进行形式化描述。

Result: 成功实现了Mimosa程序到实时操作系统原语的编译映射。

Conclusion: 该编译方案能够有效支持Mimosa语言在嵌入式系统中的实现和应用。

Abstract: This paper introduces a compilation scheme for programs written in the Mimosa
programming language, which builds upon the MIMOS model of computation. Mimosa
describes embedded systems software as a collection of time-triggered processes
which communicate through FIFO queues. We formally describe an adaptation of
the Lustre compilation scheme to the semantics of Mimosa and show how the
coordination layer can be mapped to real-time operating system primitives.

</details>


### [6] [SafeFFI: Efficient Sanitization at the Boundary Between Safe and Unsafe Code in Rust and Mixed-Language Applications](https://arxiv.org/abs/2510.20688)
*Oliver Braunsdorf,Tim Lange,Konrad Hohentanner,Julian Horsch,Johannes Kinder*

Main category: cs.PL

TL;DR: SafeFFI是一种优化Rust二进制文件中内存安全检测的系统，通过在unsafe和safe代码边界进行检查，将内存安全执行从检测器转移到Rust类型系统，显著减少检测开销。


<details>
  <summary>Details</summary>
Motivation: Unsafe Rust代码在与C/C++库互操作和实现底层数据结构时是必要的，但可能导致内存安全违规。现有检测器会引入许多不必要的检查，即使对Rust类型系统保证安全的内存访问也是如此。

Method: SafeFFI系统优化内存安全检测，使得检查发生在unsafe和safe代码的边界处，将内存安全执行从检测器转移到Rust类型系统。该方法避免了昂贵的全程序分析，编译时开销更小。

Result: 在流行的Rust crate和已知易受攻击的Rust代码上，SafeFFI相比最先进系统实现了更优性能，将检测器检查减少了高达98%，同时保持正确性并标记所有空间和时间内存安全违规。

Conclusion: SafeFFI通过智能地在unsafe/safe代码边界放置检查，显著降低了内存安全检测的开销，同时保持了检测效果，编译时开销仅为2.64倍（相比之前的8.83倍以上）。

Abstract: Unsafe Rust code is necessary for interoperability with C/C++ libraries and
implementing low-level data structures, but it can cause memory safety
violations in otherwise memory-safe Rust programs. Sanitizers can catch such
memory errors at runtime, but introduce many unnecessary checks even for memory
accesses guaranteed safe by the Rust type system. We introduce SafeFFI, a
system for optimizing memory safety instrumentation in Rust binaries such that
checks occur at the boundary between unsafe and safe code, handing over the
enforcement of memory safety from the sanitizer to the Rust type system. Unlike
previous approaches, our design avoids expensive whole-program analysis and
adds much less compile-time overhead (2.64x compared to over 8.83x). On a
collection of popular Rust crates and known vulnerable Rust code, SafeFFI
achieves superior performance compared to state-of-the-art systems, reducing
sanitizer checks by up to 98%, while maintaining correctness and flagging all
spatial and temporal memory safety violations.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [7] [From Large to Small: Transferring CUDA Optimization Expertise via Reasoning Graph](https://arxiv.org/abs/2510.19873)
*Junfeng Gong,Zhiyi Wei,Junying Chen,Cheng Liu,Huawei Li*

Main category: cs.LG

TL;DR: ReGraphT是一个无需训练的检索增强生成框架，通过将CUDA优化轨迹组织成结构化推理图，使用蒙特卡洛图搜索进行高效探索，使小型语言模型能够达到大型语言模型的推理水平。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型在生成优化CUDA代码方面表现出色，但存在代码泄露风险和计算成本高的问题。小型语言模型更轻量且隐私友好，但在复杂CUDA生成任务中推理能力有限。

Method: 将CUDA优化轨迹组织成结构化推理图，将组合CUDA优化建模为状态转换，并利用蒙特卡洛图搜索进行高效探索。

Result: ReGraphT在CUDAEval和ParEval上实现了平均2.33倍加速，优于HPC特定微调模型和其他检索增强方法。与DeepSeek-Coder-V2-Lite-Instruct和Qwen2.5-Coder-7B-Instruct配合使用时，使小型语言模型接近大型语言模型性能。

Conclusion: ReGraphT框架有效解决了小型语言模型在复杂CUDA代码生成中的推理能力不足问题，实现了隐私友好且计算高效的解决方案。

Abstract: Despite significant evolution of CUDA programming and domain-specific
libraries, effectively utilizing GPUs with massively parallel engines remains
difficult. Large language models (LLMs) show strong potential in generating
optimized CUDA code from sequential code. However, using LLMs in practice faces
two major challenges: cloud-based APIs pose risks of code leakage, and local
deployment is often computationally expensive and inefficient. These drawbacks
have spurred interest in small language models (SLMs), which are more
lightweight and privacy-friendly. Encouragingly, recent studies show that SLMs
can achieve performance comparable to LLMs on specific tasks. While SLMs can
match LLMs on domain-specific tasks, their limited reasoning abilities lead to
suboptimal performance in complex CUDA generation according to our experiments.
To bridge this gap, we propose ReGraphT, a training-free, retrieval-augmented
generation framework that transfers LLM-level reasoning to smaller models.
ReGraphT organizes CUDA optimization trajectories into a structured reasoning
graph, modeling the combined CUDA optimizations as state transitions, and
leverages Monte Carlo Graph Search (MCGS) for efficient exploration. We also
present a CUDA-specific benchmark with difficulty tiers defined by reasoning
complexity to evaluate models more comprehensively. Experiments show that
ReGraphT outperforms HPC-specific fine-tuned models and other
retrieval-augmented approaches, achieving an average 2.33X speedup on CUDAEval
and ParEval. When paired with DeepSeek-Coder-V2-Lite-Instruct and
Qwen2.5-Coder-7B-Instruct, ReGraphT enables SLMs to approach LLM-level
performance without the associated privacy risks or excessive computing
overhead.

</details>
