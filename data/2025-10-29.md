<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [VisCoder2: Building Multi-Language Visualization Coding Agents](https://arxiv.org/abs/2510.23642)
*Yuansheng Ni,Songcheng Cai,Xiangchao Chen,Jiarong Liang,Zhiheng Lyu,Jiaqi Deng,Kai Zou,Ping Nie,Fei Yuan,Xiang Yue,Wenhu Chen*

Main category: cs.SE

TL;DR: 提出了VisCode-Multi-679K数据集、VisPlotBench基准和VisCoder2模型，用于改进可视化编码代理，支持多语言和多轮调试，显著提升了执行成功率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在可视化编码中面临语言覆盖有限、执行不可靠和缺乏迭代修正机制的问题，现有数据集和基准过于狭窄。

Method: 构建了包含679K验证样本的多语言数据集VisCode-Multi-679K，创建了VisPlotBench评估基准，并训练了多语言可视化模型VisCoder2。

Result: VisCoder2显著优于开源基线，接近GPT-4.1性能，通过迭代自调试在32B规模达到82.4%的执行通过率。

Conclusion: 提出的资源和模型有效解决了可视化编码代理的挑战，特别是在符号或编译器依赖语言中表现优异。

Abstract: Large language models (LLMs) have recently enabled coding agents capable of
generating, executing, and revising visualization code. However, existing
models often fail in practical workflows due to limited language coverage,
unreliable execution, and lack of iterative correction mechanisms. Progress has
been constrained by narrow datasets and benchmarks that emphasize single-round
generation and single-language tasks. To address these challenges, we introduce
three complementary resources for advancing visualization coding agents.
VisCode-Multi-679K is a large-scale, supervised dataset containing 679K
validated and executable visualization samples with multi-turn correction
dialogues across 12 programming languages. VisPlotBench is a benchmark for
systematic evaluation, featuring executable tasks, rendered outputs, and
protocols for both initial generation and multi-round self-debug. Finally, we
present VisCoder2, a family of multi-language visualization models trained on
VisCode-Multi-679K. Experiments show that VisCoder2 significantly outperforms
strong open-source baselines and approaches the performance of proprietary
models like GPT-4.1, with further gains from iterative self-debug, reaching
82.4% overall execution pass rate at the 32B scale, particularly in symbolic or
compiler-dependent languages.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [2] [Chain of Execution Supervision Promotes General Reasoning in Large Language Models](https://arxiv.org/abs/2510.23629)
*Nuo Chen,Zehua Li,Keqin Bao,Junyang Lin,Dayiheng Liu*

Main category: cs.LG

TL;DR: TracePile是一个包含260万样本的大规模语料库，将代码执行转换为显式的逐步推理过程（Chain of Execution），在数学、代码、逻辑和算法等多个基准测试中显著提升了大型语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 代码作为训练源具有丰富的逻辑结构和多样化的推理范式，但原始代码中的推理过程通常是隐式的，并与语法或实现噪声纠缠在一起，直接训练效果不佳。

Method: 构建TracePile语料库，将代码执行转换为显式的链式执行推理过程，包含变量追踪问题和代码重写，采用三种训练设置：继续预训练、预训练后指令调优和两阶段微调。

Result: 在四个基础模型（LLaMA 3、LLaMA 3.1、Qwen-2.5和Qwen-2.5 Coder）和20个基准测试上的实验显示了一致的改进，LLaMA3.1-8B在九个数学数据集上平均提升了7.1%。

Conclusion: TracePile通过将隐式代码推理转换为显式链式执行，有效提升了大型语言模型在多种推理任务上的性能。

Abstract: Building robust and general reasoning ability is a central goal in the
development of large language models (LLMs). Recent efforts increasingly turn
to code as a rich training source, given its inherent logical structure and
diverse reasoning paradigms such as divide-and-conquer, topological ordering,
and enumeration. However, reasoning in code is often expressed implicitly and
entangled with syntactic or implementation noise, making direct training on raw
code suboptimal.To address this, we introduce TracePile, a large-scale corpus
of 2.6 million samples that transforms code execution into explicit,
step-by-step chain-of-thought-style rationales, which we call Chain of
Execution (CoE). The corpus spans domains including mathematics, classical
algorithms and algorithmic competition, and is enriched with variable-tracing
questions and code rewritings to enhance logical granularity and code
diversity. We evaluate TracePile using three training setups:
continue-pretraining, instruction tuning after pretraining, and two-stage
finetuning. Experiments across four base models (LLaMA 3, LLaMA 3.1, Qwen-2.5,
and Qwen-2.5 Coder) and 20 benchmarks covering math, code, logic, and
algorithms demonstrate consistent improvements. Notably, TracePile boosts
LLaMA3.1-8B by 7.1\% on average across nine math datasets and delivers clear
gains on LiveCodeBench, CRUX, and MMLU under two-stage fine-tuning.

</details>
