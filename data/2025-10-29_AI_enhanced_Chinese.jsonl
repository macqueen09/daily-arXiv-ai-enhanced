{"id": "2510.23629", "pdf": "https://arxiv.org/pdf/2510.23629", "abs": "https://arxiv.org/abs/2510.23629", "authors": ["Nuo Chen", "Zehua Li", "Keqin Bao", "Junyang Lin", "Dayiheng Liu"], "title": "Chain of Execution Supervision Promotes General Reasoning in Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.PL"], "comment": null, "summary": "Building robust and general reasoning ability is a central goal in the\ndevelopment of large language models (LLMs). Recent efforts increasingly turn\nto code as a rich training source, given its inherent logical structure and\ndiverse reasoning paradigms such as divide-and-conquer, topological ordering,\nand enumeration. However, reasoning in code is often expressed implicitly and\nentangled with syntactic or implementation noise, making direct training on raw\ncode suboptimal.To address this, we introduce TracePile, a large-scale corpus\nof 2.6 million samples that transforms code execution into explicit,\nstep-by-step chain-of-thought-style rationales, which we call Chain of\nExecution (CoE). The corpus spans domains including mathematics, classical\nalgorithms and algorithmic competition, and is enriched with variable-tracing\nquestions and code rewritings to enhance logical granularity and code\ndiversity. We evaluate TracePile using three training setups:\ncontinue-pretraining, instruction tuning after pretraining, and two-stage\nfinetuning. Experiments across four base models (LLaMA 3, LLaMA 3.1, Qwen-2.5,\nand Qwen-2.5 Coder) and 20 benchmarks covering math, code, logic, and\nalgorithms demonstrate consistent improvements. Notably, TracePile boosts\nLLaMA3.1-8B by 7.1\\% on average across nine math datasets and delivers clear\ngains on LiveCodeBench, CRUX, and MMLU under two-stage fine-tuning.", "AI": {"tldr": "TracePile\u662f\u4e00\u4e2a\u5305\u542b260\u4e07\u6837\u672c\u7684\u5927\u89c4\u6a21\u8bed\u6599\u5e93\uff0c\u5c06\u4ee3\u7801\u6267\u884c\u8f6c\u6362\u4e3a\u663e\u5f0f\u7684\u9010\u6b65\u63a8\u7406\u8fc7\u7a0b\uff08Chain of Execution\uff09\uff0c\u5728\u6570\u5b66\u3001\u4ee3\u7801\u3001\u903b\u8f91\u548c\u7b97\u6cd5\u7b49\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u4ee3\u7801\u4f5c\u4e3a\u8bad\u7ec3\u6e90\u5177\u6709\u4e30\u5bcc\u7684\u903b\u8f91\u7ed3\u6784\u548c\u591a\u6837\u5316\u7684\u63a8\u7406\u8303\u5f0f\uff0c\u4f46\u539f\u59cb\u4ee3\u7801\u4e2d\u7684\u63a8\u7406\u8fc7\u7a0b\u901a\u5e38\u662f\u9690\u5f0f\u7684\uff0c\u5e76\u4e0e\u8bed\u6cd5\u6216\u5b9e\u73b0\u566a\u58f0\u7ea0\u7f20\u5728\u4e00\u8d77\uff0c\u76f4\u63a5\u8bad\u7ec3\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u6784\u5efaTracePile\u8bed\u6599\u5e93\uff0c\u5c06\u4ee3\u7801\u6267\u884c\u8f6c\u6362\u4e3a\u663e\u5f0f\u7684\u94fe\u5f0f\u6267\u884c\u63a8\u7406\u8fc7\u7a0b\uff0c\u5305\u542b\u53d8\u91cf\u8ffd\u8e2a\u95ee\u9898\u548c\u4ee3\u7801\u91cd\u5199\uff0c\u91c7\u7528\u4e09\u79cd\u8bad\u7ec3\u8bbe\u7f6e\uff1a\u7ee7\u7eed\u9884\u8bad\u7ec3\u3001\u9884\u8bad\u7ec3\u540e\u6307\u4ee4\u8c03\u4f18\u548c\u4e24\u9636\u6bb5\u5fae\u8c03\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u7840\u6a21\u578b\uff08LLaMA 3\u3001LLaMA 3.1\u3001Qwen-2.5\u548cQwen-2.5 Coder\uff09\u548c20\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\u4e86\u4e00\u81f4\u7684\u6539\u8fdb\uff0cLLaMA3.1-8B\u5728\u4e5d\u4e2a\u6570\u5b66\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u63d0\u5347\u4e867.1%\u3002", "conclusion": "TracePile\u901a\u8fc7\u5c06\u9690\u5f0f\u4ee3\u7801\u63a8\u7406\u8f6c\u6362\u4e3a\u663e\u5f0f\u94fe\u5f0f\u6267\u884c\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u79cd\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002"}}
{"id": "2510.23642", "pdf": "https://arxiv.org/pdf/2510.23642", "abs": "https://arxiv.org/abs/2510.23642", "authors": ["Yuansheng Ni", "Songcheng Cai", "Xiangchao Chen", "Jiarong Liang", "Zhiheng Lyu", "Jiaqi Deng", "Kai Zou", "Ping Nie", "Fei Yuan", "Xiang Yue", "Wenhu Chen"], "title": "VisCoder2: Building Multi-Language Visualization Coding Agents", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.PL"], "comment": null, "summary": "Large language models (LLMs) have recently enabled coding agents capable of\ngenerating, executing, and revising visualization code. However, existing\nmodels often fail in practical workflows due to limited language coverage,\nunreliable execution, and lack of iterative correction mechanisms. Progress has\nbeen constrained by narrow datasets and benchmarks that emphasize single-round\ngeneration and single-language tasks. To address these challenges, we introduce\nthree complementary resources for advancing visualization coding agents.\nVisCode-Multi-679K is a large-scale, supervised dataset containing 679K\nvalidated and executable visualization samples with multi-turn correction\ndialogues across 12 programming languages. VisPlotBench is a benchmark for\nsystematic evaluation, featuring executable tasks, rendered outputs, and\nprotocols for both initial generation and multi-round self-debug. Finally, we\npresent VisCoder2, a family of multi-language visualization models trained on\nVisCode-Multi-679K. Experiments show that VisCoder2 significantly outperforms\nstrong open-source baselines and approaches the performance of proprietary\nmodels like GPT-4.1, with further gains from iterative self-debug, reaching\n82.4% overall execution pass rate at the 32B scale, particularly in symbolic or\ncompiler-dependent languages.", "AI": {"tldr": "\u63d0\u51fa\u4e86VisCode-Multi-679K\u6570\u636e\u96c6\u3001VisPlotBench\u57fa\u51c6\u548cVisCoder2\u6a21\u578b\uff0c\u7528\u4e8e\u6539\u8fdb\u53ef\u89c6\u5316\u7f16\u7801\u4ee3\u7406\uff0c\u652f\u6301\u591a\u8bed\u8a00\u548c\u591a\u8f6e\u8c03\u8bd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6267\u884c\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709LLM\u5728\u53ef\u89c6\u5316\u7f16\u7801\u4e2d\u9762\u4e34\u8bed\u8a00\u8986\u76d6\u6709\u9650\u3001\u6267\u884c\u4e0d\u53ef\u9760\u548c\u7f3a\u4e4f\u8fed\u4ee3\u4fee\u6b63\u673a\u5236\u7684\u95ee\u9898\uff0c\u73b0\u6709\u6570\u636e\u96c6\u548c\u57fa\u51c6\u8fc7\u4e8e\u72ed\u7a84\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b679K\u9a8c\u8bc1\u6837\u672c\u7684\u591a\u8bed\u8a00\u6570\u636e\u96c6VisCode-Multi-679K\uff0c\u521b\u5efa\u4e86VisPlotBench\u8bc4\u4f30\u57fa\u51c6\uff0c\u5e76\u8bad\u7ec3\u4e86\u591a\u8bed\u8a00\u53ef\u89c6\u5316\u6a21\u578bVisCoder2\u3002", "result": "VisCoder2\u663e\u8457\u4f18\u4e8e\u5f00\u6e90\u57fa\u7ebf\uff0c\u63a5\u8fd1GPT-4.1\u6027\u80fd\uff0c\u901a\u8fc7\u8fed\u4ee3\u81ea\u8c03\u8bd5\u572832B\u89c4\u6a21\u8fbe\u523082.4%\u7684\u6267\u884c\u901a\u8fc7\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684\u8d44\u6e90\u548c\u6a21\u578b\u6709\u6548\u89e3\u51b3\u4e86\u53ef\u89c6\u5316\u7f16\u7801\u4ee3\u7406\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u7b26\u53f7\u6216\u7f16\u8bd1\u5668\u4f9d\u8d56\u8bed\u8a00\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
