<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.LO](#cs.LO) [Total: 2]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.SE](#cs.SE) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Fair intersection of seekable iterators](https://arxiv.org/abs/2510.26016)
*Michael Arntzenius*

Main category: cs.PL

TL;DR: 该论文展示了通过有界工作实现公平性的思想，既适用于miniKanren的搜索策略，也适用于最坏情况最优连接的实现。


<details>
  <summary>Details</summary>
Motivation: 研究如何实现公平且高效的搜索策略，以及如何将这种思想扩展到数据库连接操作中。

Method: 采用有界工作的方法来保证公平性，在miniKanren中通过限制每个分支的探索工作量来实现公平交替执行，在最坏情况最优连接中通过可搜索迭代器接口实现组合式方法。

Result: 证明了有界工作实现公平性的思想具有通用性，能够优雅地应用于不同领域（逻辑编程和数据库连接）。

Conclusion: 有界工作实现公平性的思想是一个强大的通用原则，可以统一地解决逻辑编程中的搜索公平性和数据库中的最坏情况最优连接问题。

Abstract: miniKanren's key semantic advance over Prolog is to implement a complete yet
efficient search strategy, fairly interleaving execution between disjuncts.
This fairness is accomplished by bounding how much work is done exploring one
disjunct before switching to the next. We show that the same idea -- fairness
via bounded work -- underlies an elegant compositional approach to implementing
worst-case optimal joins using a seekable iterator interface, suitable for
shallow embedding in functional languages.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [2] [Finding Regular Herbrand Models for CHCs using Answer Set Programming](https://arxiv.org/abs/2510.26428)
*Gregoire Maire,Thomas Genet*

Main category: cs.LO

TL;DR: 提出使用Clingo将带代数数据类型的约束Horn子句编码为SAT问题，构建树自动机来识别Herbrand模型，实现半完备的可满足性检查器。


<details>
  <summary>Details</summary>
Motivation: 研究带代数数据类型的约束Horn子句的可满足性证明问题，寻求构建识别Herbrand模型的树自动机的方法。

Method: 将约束Horn子句与代数数据类型编码为ASP问题，使用Clingo SAT求解器构建树自动机。

Result: 开发出半完备的可满足性检查器：当存在正则Herbrand模型时找到树自动机，当问题不可满足时找到反例。

Conclusion: 提出的ASP编码方法能够有效处理带代数数据类型的约束Horn子句，为可满足性检查提供了新途径。

Abstract: We are interested in proving satisfiability of Constrained Horn Clauses
(CHCs) over Algebraic Data Types (ADTs). We propose to prove satisfiability by
building a tree automaton recognizing the Herbrand model of the CHCs. If such
an automaton exists then the model is said to be regular, i.e., the Herbrand
model is a regular set of atoms. Kostyukov et al. have shown how to derive an
automaton when CVC4 finds a finite model of the CHCs. We propose an alternative
way to build the automaton using an encoding into a SAT problem using Clingo,
an Answer Set Programming (ASP) tool. We implemented a translation of CHCs with
ADTs into an ASP problem. Combined with Clingo, we obtain a semi-complete
satisfiability checker: it finds a tree automaton if a regular Herbrand model
exists or finds a counter-example if the problem is unsatisfiable.

</details>


### [3] [Semantic Properties of Computations Defined by Elementary Inference Systems](https://arxiv.org/abs/2510.26429)
*Salvador Lucas*

Main category: cs.LO

TL;DR: 本文提出使用初等推理系统定义集合/关系/计算，通过一阶理论Th(I)和规范模型来分析语义属性，并展示了如何在任意模型中验证这些属性，应用于编程语言和重写系统的分析。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过初等推理系统来描述编程语言和计算系统的语义属性，解决规范模型通常不可计算的问题，提供在任意模型中验证语义属性的方法。

Method: 使用Smullyan的初等形式系统和Gentzen的推理规则表示法，构建一阶理论Th(I)，通过规范模型或任意模型来验证语义属性的满足性。

Result: 提出了在任意模型而非仅规范模型中验证语义属性的方法，使得不可计算的规范模型问题得到解决，能够有效分析编程语言和重写系统的性质。

Conclusion: 初等推理系统结合一阶理论和模型论方法为编程语言和计算系统的语义分析提供了有效框架，特别是在重写系统等领域的应用具有实用价值。

Abstract: We consider sets/relations/computations defined by *Elementary Inference
Systems* I, which are obtained from Smullyan's *elementary formal systems*
using Gentzen's notation for inference rules, and proof trees for atoms
P(t_1,...,t_n), where predicate P represents the considered
set/relation/computation. A first-order theory Th(I), actually a set of
definite Horn clauses, is given to I. Properties of objects defined by I are
expressed as first-order sentences F, which are proved true or false by
*satisfaction* M |= F of F in a *canonical* model M of Th(I). For this reason,
we call F a *semantic property* of I. Since canonical models are, in general,
incomputable, we show how to (dis)prove semantic properties by satisfiability
in an *arbitrary* model A of Th(I). We apply these ideas to the analysis of
properties of programming languages and systems whose computations can be
described by means of an elementary inference system. In particular,
rewriting-based systems.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [4] [SymCode: A Neurosymbolic Approach to Mathematical Reasoning via Verifiable Code Generation](https://arxiv.org/abs/2510.25975)
*Sina Bagheri Nezhad,Yao Li,Ameeta Agrawal*

Main category: cs.CL

TL;DR: SymCode是一个神经符号框架，通过将数学问题解决重新定义为使用SymPy库的可验证代码生成任务，显著提高了LLM在复杂数学推理中的准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂数学推理中经常失败，基于文本的生成导致未经验证和算术上不健全的解决方案。现有提示策略如思维链仍在这种不可靠的媒介中操作，缺乏确定性验证机制。

Method: 引入SymCode神经符号框架，将数学问题解决重新定义为使用SymPy库的可验证代码生成任务，将LLM推理建立在确定性符号引擎基础上。

Result: 在MATH-500和OlympiadBench等挑战性基准测试中，相比基线方法准确率提高了高达13.6个百分点，不仅更节省token，还将模型失败从模糊的逻辑谬误转变为透明的程序化错误。

Conclusion: 通过将LLM推理建立在确定性符号引擎基础上，SymCode代表了在形式化领域实现更准确和可信AI的关键一步。

Abstract: Large Language Models (LLMs) often struggle with complex mathematical
reasoning, where prose-based generation leads to unverified and arithmetically
unsound solutions. Current prompting strategies like Chain of Thought still
operate within this unreliable medium, lacking a mechanism for deterministic
verification. To address these limitations, we introduce SymCode, a
neurosymbolic framework that reframes mathematical problem-solving as a task of
verifiable code generation using the SymPy library. We evaluate SymCode on
challenging benchmarks, including MATH-500 and OlympiadBench, demonstrating
significant accuracy improvements of up to 13.6 percentage points over
baselines. Our analysis shows that SymCode is not only more token-efficient but
also fundamentally shifts model failures from opaque logical fallacies towards
transparent, programmatic errors. By grounding LLM reasoning in a deterministic
symbolic engine, SymCode represents a key step towards more accurate and
trustworthy AI in formal domains.

</details>


### [5] [QCoder Benchmark: Bridging Language Generation and Quantum Hardware through Simulator-Based Feedback](https://arxiv.org/abs/2510.26101)
*Taku Mikuriya,Tatsuya Ishigaki,Masayuki Kawarada,Shunya Minami,Tadashi Kadowaki,Yohichi Suzuki,Soshun Naito,Shunya Takata,Takumi Kato,Tamotsu Basseda,Reo Yamada,Hiroya Takamura*

Main category: cs.CL

TL;DR: 提出了QCoder Benchmark评估框架，用于评估LLM在量子编程任务中的表现，支持量子模拟器环境和真实人类代码对比分析。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在需要与硬件设备交互的领域（如量子编程）中的应用研究不足，需要专门的评估框架来测试模型性能。

Method: 开发了QCoder Benchmark，包含两个关键特性：1）支持量子模拟器环境评估，提供电路深度、执行时间等域特定指标；2）整合真实编程竞赛中的人类代码提交，进行定量和定性分析。

Result: 实验显示GPT-4o等先进模型准确率仅18.97%，而基于推理的模型如o3达到78%准确率，超过了人类代码的平均成功率（39.98%）。

Conclusion: QCoder Benchmark揭示了量子编程任务的挑战性，推理模型表现优于传统模型，发布了数据集和公共评估API以支持进一步研究。

Abstract: Large language models (LLMs) have increasingly been applied to automatic
programming code generation. This task can be viewed as a language generation
task that bridges natural language, human knowledge, and programming logic.
However, it remains underexplored in domains that require interaction with
hardware devices, such as quantum programming, where human coders write Python
code that is executed on a quantum computer. To address this gap, we introduce
QCoder Benchmark, an evaluation framework that assesses LLMs on quantum
programming with feedback from simulated hardware devices. Our benchmark offers
two key features. First, it supports evaluation using a quantum simulator
environment beyond conventional Python execution, allowing feedback of
domain-specific metrics such as circuit depth, execution time, and error
classification, which can be used to guide better generation. Second, it
incorporates human-written code submissions collected from real programming
contests, enabling both quantitative comparisons and qualitative analyses of
LLM outputs against human-written codes. Our experiments reveal that even
advanced models like GPT-4o achieve only around 18.97% accuracy, highlighting
the difficulty of the benchmark. In contrast, reasoning-based models such as o3
reach up to 78% accuracy, outperforming averaged success rates of human-written
codes (39.98%). We release the QCoder Benchmark dataset and public evaluation
API to support further research.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [6] [CHCVerif: A Portfolio-Based Solver for Constrained Horn Clauses](https://arxiv.org/abs/2510.26431)
*Mihály Dobos-Kovács,Levente Bajczi,András Vörös*

Main category: cs.SE

TL;DR: CHCVERIF是一个基于组合策略的CHC求解器，采用软件验证方法解决约束Horn子句问题，能够重用成熟的软件验证工具处理涉及位向量和低级语义的基准测试。


<details>
  <summary>Details</summary>
Motivation: 约束Horn子句（CHCs）被广泛用作各种验证任务的中间表示，但现有方法在处理涉及位向量和低级语义的基准测试时存在局限性。

Method: 开发了CHCVERIF组合求解器，采用软件验证方法，重用成熟的软件验证工具作为后端来解决CHC问题。

Result: 在线性整数算术基准测试中表现一般，在位向量基准测试中取得适度成功，证明了使用软件验证工具作为CHC求解后端的可行性和潜力。

Conclusion: 使用软件验证工具作为CHC求解后端是可行的，特别是在精心构建的组合策略支持下，这种方法在处理位向量和低级语义方面具有潜力。

Abstract: Constrained Horn Clauses (CHCs) are widely adopted as intermediate
representations for a variety of verification tasks, including safety checking,
invariant synthesis, and interprocedural analysis. This paper introduces
CHCVERIF, a portfolio-based CHC solver that adopts a software verification
approach for solving CHCs. This approach enables us to reuse mature software
verification tools to tackle CHC benchmarks, particularly those involving
bitvectors and low-level semantics. Our evaluation shows that while the method
enjoys only moderate success with linear integer arithmetic, it achieves modest
success on bitvector benchmarks. Moreover, our results demonstrate the
viability and potential of using software verification tools as backends for
CHC solving, particularly when supported by a carefully constructed portfolio.

</details>
