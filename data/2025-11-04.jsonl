{"id": "2511.00403", "pdf": "https://arxiv.org/pdf/2511.00403", "abs": "https://arxiv.org/abs/2511.00403", "authors": ["Wentao Peng", "Ruyi Ji", "Yingfei Xiong"], "title": "Equality Saturation Guided by Large Language Models", "categories": ["cs.PL"], "comment": "presented at EGRAPHS 2025", "summary": "One critical issue with large language models (LLMs) is their inability to\nguarantee correctness. Although this problem can be addressed by applying LLMs\nto formal rewrite systems, current LLMs are still far from adequate to generate\nsound rewrite chains. To bridge this gap, this paper proposes LLM-guided\nequality saturation, dubbed LGuess, by incorporating e-graphs as an\nintermediate layer between LLMs and rewrite systems. LGuess queries LLMs only\nfor high-level rewrite checkpoints and uses e-graphs to supply low-level\nrewrite chains between these checkpoints. The key technical challenge in this\nprocedure lies in effectively extracting a suitable checkpoint from a saturated\ne-graph, which LGuess addresses by learning a probabilistic model from the LLM.\nThe model predicts probable checkpoints while remaining simple enough for\neffective extraction. We implement a prototype of LGuess and evaluate it on the\nproblem of factorizing multivariable polynomials. The results demonstrate a\nsignificant advantage of LGuess compared to both straightforward equality\nsaturation and the approach that queries the LLM directly for the rewrite\nchain."}
{"id": "2511.00488", "pdf": "https://arxiv.org/pdf/2511.00488", "abs": "https://arxiv.org/abs/2511.00488", "authors": ["Jun Gao", "Yun Peng", "Xiaoxue Ren"], "title": "\\texttt{ReMind}: Understanding Deductive Code Reasoning in LLMs", "categories": ["cs.PL", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable progress in\ncode-related tasks. Despite their advancement, empirical evidence reveals that\nthey still struggle with \\emph{deductive code reasoning}, the ability to reason\nabout the program execution process. While prior studies have recognized this\nlimitation, the underlying causes remain largely underexplored. In this paper,\nwe begin by presenting a comprehensive empirical study that reveals three key\nchallenges undermining deductive code reasoning: (1) an intrinsic gap between\ngeneration and reasoning abilities, (2) a consistent bias towards code sources,\nand (3) weak zero-shot generalization on complex benchmarks. In light of these\nchallenges, we propose \\texttt{ReMind}, a multi-agent framework composed of\n\\texttt{Mutator}, \\texttt{Executor}, and \\texttt{Inspector}. The\n\\texttt{Mutator} generates code variants to mitigate bias towards code sources,\nthe \\texttt{Executor} traces variable states step-by-step to expose\ninconsistency, and the \\texttt{Inspector} identifies problematic reasoning\nsteps and provides control-flow refinement to bridge the intrinsic reasoning\ngap. Through their coordinated collaboration, \\texttt{ReMind} systematically\nidentifies and refines reasoning flaws, achieving outstanding performance and\nenabling robust zero-shot generalization. Extensive experiments on two\nbenchmarks with five LLMs demonstrate the superior advantages of\n\\texttt{ReMind} compared to baseline approaches in deductive code reasoning."}
{"id": "2511.00592", "pdf": "https://arxiv.org/pdf/2511.00592", "abs": "https://arxiv.org/abs/2511.00592", "authors": ["Massinissa Merouani", "Islem Kara Bernou", "Riyadh Baghdadi"], "title": "Agentic Auto-Scheduling: An Experimental Study of LLM-Guided Loop Optimization", "categories": ["cs.PL", "cs.DC", "cs.LG", "cs.PF"], "comment": "Accepted at the 34th International Conference on Parallel\n  Architectures and Compilation Techniques (PACT 2025). 12 pages, plus appendix", "summary": "Automatic code optimization remains a difficult challenge, particularly for\ncomplex loop nests on modern hardware. This paper investigates a novel approach\nto code optimization where Large Language Models (LLMs) guide the process\nthrough a closed-loop interaction with a compiler. We present ComPilot, an\nexperimental framework that leverages off-the-shelf LLMs, without any\ntask-specific fine-tuning, as interactive optimization agents. ComPilot\nestablishes a feedback loop where an LLM proposes transformations for a given\nloop nest to a compiler. The compiler attempts the transformations, reporting\nback legality status and measured speedup or slowdown. The LLM utilizes this\nconcrete feedback to iteratively refine its optimization strategy. Our\nextensive evaluation across the PolyBench benchmark suite demonstrates the\neffectiveness of this zero-shot approach. ComPilot achieves geometric mean\nspeedups of 2.66x (single run) and 3.54x (best-of-5 runs) over the original\ncode. Furthermore, ComPilot demonstrates competitive performance against the\nstate-of-the-art Pluto polyhedral optimizer, outperforming it in many cases.\nThis experimental study demonstrates that general-purpose LLMs can effectively\nguide the code optimization process when grounded by compiler feedback, opening\npromising research directions for agentic AI in code optimization."}
{"id": "2511.00740", "pdf": "https://arxiv.org/pdf/2511.00740", "abs": "https://arxiv.org/abs/2511.00740", "authors": ["Igor Engel", "Ekaterina Verbitskaia"], "title": "Typed Embedding of miniKanren for Functional Conversion", "categories": ["cs.PL"], "comment": null, "summary": "Relational programming enables program synthesis through a verifier-to-solver\napproach. An earlier paper introduced a functional conversion that mitigated\nsome of the inherent performance overhead. However, the conversion was\ninelegant: it was oblivious to types, demanded determinism annotations, and\nimplicit generator threading. In this paper, we address these issues by\nproviding a typed tagless-final embedding of miniKanren into Haskell. This\nimprovement significantly reduces boilerplate while preserving, and sometimes\nenhancing, earlier speedups."}
{"id": "2511.01736", "pdf": "https://arxiv.org/pdf/2511.01736", "abs": "https://arxiv.org/abs/2511.01736", "authors": ["Charles Yuan"], "title": "Cobble: Compiling Block Encodings for Quantum Computational Linear Algebra", "categories": ["cs.PL", "quant-ph"], "comment": "20 pages, 12 figures", "summary": "Quantum algorithms for computational linear algebra promise up to exponential\nspeedups for applications such as simulation and regression, making them prime\ncandidates for hardware realization. But these algorithms execute in a model\nthat cannot efficiently store matrices in memory like a classical algorithm\ndoes, instead requiring developers to implement complex expressions for matrix\narithmetic in terms of correct and efficient quantum circuits. Among the\nchallenges for the developer is navigating a cost model in which conventional\noptimizations for linear algebra, such as subexpression reuse, can be\ninapplicable or unprofitable.\n  In this work, we present Cobble, a language for programming with quantum\ncomputational linear algebra. Cobble enables developers to express and\nmanipulate the quantum representations of matrices, known as block encodings,\nusing high-level notation that automatically compiles to correct quantum\ncircuits. Cobble features analyses that estimate leading factors in time and\nspace usage of programs, as well as optimizations that reduce overhead and\ngenerate efficient circuits using leading techniques such as the quantum\nsingular value transformation. We evaluate Cobble on benchmark kernels for\nsimulation, regression, search, and other applications, showing 2.6x-25.4x\nspeedups not achieved by existing circuit optimizers on these benchmarks."}
{"id": "2511.00125", "pdf": "https://arxiv.org/pdf/2511.00125", "abs": "https://arxiv.org/abs/2511.00125", "authors": ["√Ålvaro Silva", "Alexandra Mendes", "Ruben Martins"], "title": "Inferring multiple helper Dafny assertions with LLMs", "categories": ["cs.SE", "cs.AI", "cs.LO", "cs.PL"], "comment": null, "summary": "The Dafny verifier provides strong correctness guarantees but often requires\nnumerous manual helper assertions, creating a significant barrier to adoption.\nWe investigate the use of Large Language Models (LLMs) to automatically infer\nmissing helper assertions in Dafny programs, with a primary focus on cases\ninvolving multiple missing assertions. To support this study, we extend the\nDafnyBench benchmark with curated datasets where one, two, or all assertions\nare removed, and we introduce a taxonomy of assertion types to analyze\ninference difficulty. Our approach refines fault localization through a hybrid\nmethod that combines LLM predictions with error-message heuristics. We\nimplement this approach in a new tool called DAISY (Dafny Assertion Inference\nSYstem). While our focus is on multiple missing assertions, we also evaluate\nDAISY on single-assertion cases. DAISY verifies 63.4% of programs with one\nmissing assertion and 31.7% with multiple missing assertions. Notably, many\nprograms can be verified with fewer assertions than originally present,\nhighlighting that proofs often admit multiple valid repair strategies and that\nrecovering every original assertion is unnecessary. These results demonstrate\nthat automated assertion inference can substantially reduce proof engineering\neffort and represent a step toward more scalable and accessible formal\nverification."}
{"id": "2511.00626", "pdf": "https://arxiv.org/pdf/2511.00626", "abs": "https://arxiv.org/abs/2511.00626", "authors": ["Alexis Saurin"], "title": "Proceedings Twelfth Workshop on Fixed Points in Computer Science", "categories": ["cs.LO", "cs.PL"], "comment": null, "summary": "This EPTCS volume contains the post-proceedings of the Twelfth International\nWorkshop on Fixed Points in Computer Science, presenting a selection of the\nworks presented during the workshop that took place in Naples (Italy) on the\n19th and 20th of February 2024 as a satellite of the International Conference\non Computer Science Logic (CSL 2024)."}
{"id": "2511.00865", "pdf": "https://arxiv.org/pdf/2511.00865", "abs": "https://arxiv.org/abs/2511.00865", "authors": ["Hangdong Zhao", "Zhenghong Yu", "Srinag Rao", "Simon Frisk", "Zhiwei Fan", "Paraschos Koutris"], "title": "FlowLog: Efficient and Extensible Datalog via Incrementality", "categories": ["cs.DB", "cs.PL"], "comment": "Accepted to VLDB 2026", "summary": "Datalog-based languages are regaining popularity as a powerful abstraction\nfor expressing recursive computations in domains such as program analysis and\ngraph processing. However, existing systems often face a trade-off between\nefficiency and extensibility. Engines like Souffle achieve high efficiency\nthrough domain-specific designs, but lack general-purpose flexibility. Others,\nlike RecStep, offer modularity by layering Datalog on traditional databases,\nbut struggle to integrate Datalog-specific optimizations.\n  This paper bridges this gap by presenting FlowLog, a new Datalog engine that\nuses an explicit relational IR per-rule to cleanly separate recursive control\n(e.g., semi-naive execution) from each rule's logical plan. This boundary lets\nus retain fine-grained, Datalog-aware optimizations at the logical layer, but\nalso reuse off-the-shelf database primitives at execution. At the logical level\n(i.e. IR), we apply proven SQL optimizations, such as logic fusion and subplan\nreuse. To address high volatility in recursive workloads, we adopt a\nrobustness-first approach that pairs a structural optimizer (avoiding\nworst-case joins) with sideways information passing (early filtering). Built\natop Differential Dataflow--a mature framework for streaming analytics--FlowLog\nsupports both batch and incremental Datalog and adds novel recursion-aware\noptimizations called Boolean (or algebraic) specialization. Our evaluation\nshows that FlowLog outperforms state-of-the-art Datalog engines and modern\ndatabases across a broad range of recursive workloads, achieving superior\nscalability while preserving a simple and extensible architecture."}
{"id": "2511.01529", "pdf": "https://arxiv.org/pdf/2511.01529", "abs": "https://arxiv.org/abs/2511.01529", "authors": ["Murali Sridharan", "Mikel Robredo", "Leevi Rantala", "Matteo Esposito", "Valentina Lenarduzzi", "Mika Mantyla"], "title": "Hidden in Plain Sight: Where Developers Confess Self-Admitted Technical Debt", "categories": ["cs.SE", "cs.CL", "cs.PL"], "comment": null, "summary": "Context. Detecting Self-Admitted Technical Debt (SATD) is crucial for\nproactive software maintenance. Previous research has primarily targeted\ndetecting and prioritizing SATD, with little focus on the source code afflicted\nwith SATD. Our goal in this work is to connect the SATD comments with source\ncode constructs that surround them.\n  Method. We leverage the extensive SATD dataset PENTACET, containing code\ncomments from over 9000 Java Open Source Software (OSS) repositories. We\nquantitatively infer where SATD most commonly occurs and which code\nconstructs/statements it most frequently affects.\n  Results and Conclusions. Our large-scale study links over 225,000 SATD\ncomments to their surrounding code, showing that SATD mainly arises in inline\ncode near definitions, conditionals, and exception handling, where developers\nface uncertainty and trade-offs, revealing it as an intentional signal of\nawareness during change rather than mere neglect."}
{"id": "2511.01753", "pdf": "https://arxiv.org/pdf/2511.01753", "abs": "https://arxiv.org/abs/2511.01753", "authors": ["Zachary Hansen", "Yuliya Lierler"], "title": "SM-based Semantics for Answer Set Programs Containing Conditional Literals and Arithmetic", "categories": ["cs.LO", "cs.AI", "cs.PL", "F.4.1"], "comment": "This version corrects the review of tau for negated atoms, and\n  clarifies the distinction between global and local variables in conditional\n  literals (the supporting proofs are also updated accordingly)", "summary": "Modern answer set programming solvers such as CLINGO support advanced\nlanguage constructs that improve the expressivity and conciseness of logic\nprograms. Conditional literals are one such construct. They form \"subformulas\"\nthat behave as nested implications within the bodies of logic rules. Their\ninclusion brings the form of rules closer to the less restrictive syntax of\nfirst-order logic. These qualities make conditional literals useful tools for\nknowledge representation. In this paper, we propose a semantics for logic\nprograms with conditional literals and arithmetic based on the SM operator.\nThese semantics do not require grounding, unlike the established semantics for\nsuch programs that relies on a translation to infinitary propositional logic.\nThe main result of this paper establishes the precise correspondence between\nthe proposed and existing semantics."}
