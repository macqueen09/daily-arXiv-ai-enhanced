{"id": "2511.02491", "pdf": "https://arxiv.org/pdf/2511.02491", "abs": "https://arxiv.org/abs/2511.02491", "authors": ["Roland Meyer", "Jakob Tepe"], "title": "Oriented Metrics for Bottom-Up Enumerative Synthesis", "categories": ["cs.PL"], "comment": "37 pages, 16 figures", "summary": "In syntax-guided synthesis, one of the challenges is to reduce the enormous\nsize of the search space. We observe that most search spaces are not just flat\nsets of programs, but can be endowed with a structure that we call an oriented\nmetric. Oriented metrics measure the distance between programs, like ordinary\nmetrics do, but are designed for settings in which operations have an\norientation. Our focus is on the string and the bitvector domains, where\noperations like concatenation and bitwise conjunction transform an input into\nan output in a way that is not symmetric. We develop several new oriented\nmetrics for these domains. Oriented metrics are designed for search space\nreduction, and we present four techniques: (i) pruning the search space to a\nball around the ground truth, (ii) factorizing the search space by an\nequivalence that is induced by the oriented metric, (iii) abstracting the\noriented metric (and hence the equivalence) and refining it, and (iv) improving\nthe enumeration order by learning from abstract information. We acknowledge\nthat these techniques are inspired by developments in the literature. By\nunderstanding their roots in oriented metrics, we can substantially increase\ntheir applicability and efficiency. We have integrated these techniques into a\nnew synthesis algorithm and implemented the algorithm in a new solver. Notably,\nour solver is generic in the oriented metric over which it computes. We\nconducted experiments in the string and the bitvector domains, and consistently\nimprove the performance over the state-of-the-art by more than an order of\nmagnitude."}
{"id": "2511.01872", "pdf": "https://arxiv.org/pdf/2511.01872", "abs": "https://arxiv.org/abs/2511.01872", "authors": ["Etash Guha", "Tianxiao Jiang", "Andrew Deng", "Jian Zhang", "Muthu Annamalai"], "title": "Learned Cost Model for Placement on Reconfigurable Dataflow Hardware", "categories": ["cs.DC", "cs.LG", "cs.PL"], "comment": "7 pages, 2 figures, 2 tables, DAC Conference style (2022)", "summary": "Mapping a dataflow-graph of an ML model onto a reconfigurable system is\ndifficult, as different mappings have different throughputs and consume\nresource constraints differently. To solve this, a model to evaluate the\nthroughput of mappings is necessary as measuring throughput completely is\nexpensive. Many use a hand-designed analytical model, relying on proxy features\nor intuition, introducing error. We provide a Learned Approach that predicts\nthroughput 31%-52% more accurately over a variety of graphs. In addition, our\napproach shows no accuracy degradation after removing performance annotations.\nWe show that using this approach results in 5.6% faster compiled graphs."}
{"id": "2511.02164", "pdf": "https://arxiv.org/pdf/2511.02164", "abs": "https://arxiv.org/abs/2511.02164", "authors": ["Eric Vin", "Kyle A. Miller", "Inigo Incer", "Sanjit A. Seshia", "Daniel J. Fremont"], "title": "ScenicProver: A Framework for Compositional Probabilistic Verification of Learning-Enabled Systems", "categories": ["cs.LO", "cs.AI", "cs.LG", "cs.PL"], "comment": "26 pages, 4 figures. Full version (including appendices) of a paper\n  submitted to TACAS 2026", "summary": "Full verification of learning-enabled cyber-physical systems (CPS) has long\nbeen intractable due to challenges including black-box components and complex\nreal-world environments. Existing tools either provide formal guarantees for\nlimited types of systems or test the system as a monolith, but no general\nframework exists for compositional analysis of learning-enabled CPS using\nvaried verification techniques over complex real-world environments. This paper\nintroduces ScenicProver, a verification framework that aims to fill this gap.\nBuilt upon the Scenic probabilistic programming language, the framework\nsupports: (1) compositional system description with clear component interfaces,\nranging from interpretable code to black boxes; (2) assume-guarantee contracts\nover those components using an extension of Linear Temporal Logic containing\narbitrary Scenic expressions; (3) evidence generation through testing, formal\nproofs via Lean 4 integration, and importing external assumptions; (4)\nsystematic combination of generated evidence using contract operators; and (5)\nautomatic generation of assurance cases tracking the provenance of system-level\nguarantees. We demonstrate the framework's effectiveness through a case study\non an autonomous vehicle's automatic emergency braking system with sensor\nfusion. By leveraging manufacturer guarantees for radar and laser sensors and\nfocusing testing efforts on uncertain conditions, our approach enables stronger\nprobabilistic guarantees than monolithic testing with the same computational\nbudget."}
{"id": "2511.02285", "pdf": "https://arxiv.org/pdf/2511.02285", "abs": "https://arxiv.org/abs/2511.02285", "authors": ["Zhuorui Zhao", "Bing Li", "Grace Li Zhang", "Ulf Schlichtmann"], "title": "VFocus: Better Verilog Generation from Large Language Model via Focused Reasoning", "categories": ["cs.AR", "cs.PL", "cs.SE"], "comment": "accepted by SOCC 2025", "summary": "Large Language Models (LLMs) have shown impressive potential in generating\nVerilog codes, but ensuring functional correctness remains a challenge.\nExisting approaches often rely on self-consistency or simulation feedback to\nselect the best candidate, but they miss opportunities to focus LLM reasoning\non the most informative parts of the design. We propose VFocus, a three-stage\nframework that enhances Verilog generation by sharpening the focus of LLM\nreasoning onto critical decision points in the code generation process. In the\n\\textbf{pre-ranking stage}, VFocus generates multiple code candidates through\nLLM prompting, retries for syntactically valid outputs, and introduces a\n\\textit{Density-guided Filtering} to retain candidates that fall within the\n\"reasoning sweet spot\" for functional correctness. In the \\textbf{ranking\nstage}, we simulate each code candidate using an automatically generated\ntestbench and apply self-consistency-based clustering to identify the most\nconsistent outputs. Finally, in the \\textbf{post-ranking refinement stage},\nVFocus performs inconsistency mining on top-ranked candidates and invokes\nreasoning-augmented LLM prompts for candidate refinement. Experiments on the\nVerilogEval-Human benchmark show that VFocus significantly improves the pass@1\ncorrectness across multiple reasoning LLMs, demonstrating its effectiveness in\nenhancing Verilog generation for complex hardware design tasks."}
{"id": "2511.02595", "pdf": "https://arxiv.org/pdf/2511.02595", "abs": "https://arxiv.org/abs/2511.02595", "authors": ["RÃ©my Cerda"], "title": "Nominal Algebraic-Coalgebraic Data Types, with Applications to Infinitary Lambda-Calculi", "categories": ["cs.LO", "cs.PL", "F.3.3; F.4.1; D.3.3"], "comment": "In Proceedings FICS 2024, arXiv:2511.00626", "summary": "Ten years ago, it was shown that nominal techniques can be used to design\ncoalgebraic data types with variable binding, so that alpha-equivalence classes\nof infinitary terms are directly endowed with a corecursion principle. We\nintroduce \"mixed\" binding signatures, as well as the corresponding type of\nmixed inductive-coinductive terms. We extend the aforementioned work to this\nsetting. In particular, this allows for a nominal description of the sets\nLambda_abc of abc-infinitary lambda-terms (for a, b, c in {0,1}) and of\ncapture-avoiding substitution on alpha-equivalence classes of such terms."}
{"id": "2511.02610", "pdf": "https://arxiv.org/pdf/2511.02610", "abs": "https://arxiv.org/abs/2511.02610", "authors": ["Nadia Daoudi", "Ivan Alfonso", "Jordi Cabot"], "title": "Neural Network Interoperability Across Platforms", "categories": ["cs.LG"], "comment": null, "summary": "The development of smart systems (i.e., systems enhanced with AI components)\nhas thrived thanks to the rapid advancements in neural networks (NNs). A wide\nrange of libraries and frameworks have consequently emerged to support NN\ndesign and implementation. The choice depends on factors such as available\nfunctionalities, ease of use, documentation and community support. After\nadopting a given NN framework, organizations might later choose to switch to\nanother if performance declines, requirements evolve, or new features are\nintroduced. Unfortunately, migrating NN implementations across libraries is\nchallenging due to the lack of migration approaches specifically tailored for\nNNs. This leads to increased time and effort to modernize NNs, as manual\nupdates are necessary to avoid relying on outdated implementations and ensure\ncompatibility with new features. In this paper, we propose an approach to\nautomatically migrate neural network code across deep learning frameworks. Our\nmethod makes use of a pivot NN model to create an abstraction of the NN prior\nto migration. We validate our approach using two popular NN frameworks, namely\nPyTorch and TensorFlow. We also discuss the challenges of migrating code\nbetween the two frameworks and how they were approached in our method.\nExperimental evaluation on five NNs shows that our approach successfully\nmigrates their code and produces NNs that are functionally equivalent to the\noriginals. Artefacts from our work are available online."}
