<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.LO](#cs.LO) [Total: 2]
- [cs.LG](#cs.LG) [Total: 1]
- [cs.DC](#cs.DC) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Oriented Metrics for Bottom-Up Enumerative Synthesis](https://arxiv.org/abs/2511.02491)
*Roland Meyer,Jakob Tepe*

Main category: cs.PL

TL;DR: 提出了面向度量概念来减少语法引导综合中的搜索空间，在字符串和位向量域开发了新的面向度量，并提出了四种搜索空间缩减技术，实验显示性能比现有技术提升一个数量级以上。


<details>
  <summary>Details</summary>
Motivation: 语法引导综合面临搜索空间过大的挑战，大多数搜索空间具有结构特性，需要设计适合非对称操作环境的度量方法。

Method: 开发了字符串和位向量域的面向度量，提出了四种技术：修剪搜索空间到真实值周围的球、基于面向度量的等价类分解、抽象和精化面向度量、利用抽象信息改进枚举顺序。

Result: 实现了新的综合算法和求解器，在字符串和位向量域的实验表明，性能相比现有技术提升了一个数量级以上。

Conclusion: 通过面向度量的概念统一理解现有技术，显著提高了其适用性和效率，为语法引导综合提供了有效的搜索空间缩减方法。

Abstract: In syntax-guided synthesis, one of the challenges is to reduce the enormous
size of the search space. We observe that most search spaces are not just flat
sets of programs, but can be endowed with a structure that we call an oriented
metric. Oriented metrics measure the distance between programs, like ordinary
metrics do, but are designed for settings in which operations have an
orientation. Our focus is on the string and the bitvector domains, where
operations like concatenation and bitwise conjunction transform an input into
an output in a way that is not symmetric. We develop several new oriented
metrics for these domains. Oriented metrics are designed for search space
reduction, and we present four techniques: (i) pruning the search space to a
ball around the ground truth, (ii) factorizing the search space by an
equivalence that is induced by the oriented metric, (iii) abstracting the
oriented metric (and hence the equivalence) and refining it, and (iv) improving
the enumeration order by learning from abstract information. We acknowledge
that these techniques are inspired by developments in the literature. By
understanding their roots in oriented metrics, we can substantially increase
their applicability and efficiency. We have integrated these techniques into a
new synthesis algorithm and implemented the algorithm in a new solver. Notably,
our solver is generic in the oriented metric over which it computes. We
conducted experiments in the string and the bitvector domains, and consistently
improve the performance over the state-of-the-art by more than an order of
magnitude.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [2] [VFocus: Better Verilog Generation from Large Language Model via Focused Reasoning](https://arxiv.org/abs/2511.02285)
*Zhuorui Zhao,Bing Li,Grace Li Zhang,Ulf Schlichtmann*

Main category: cs.AR

TL;DR: VFocus是一个三阶段框架，通过将LLM推理聚焦于代码生成过程中的关键决策点来增强Verilog生成功能正确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖自一致性或仿真反馈来选择最佳候选代码，但未能充分利用LLM推理聚焦于设计中最关键部分的机会。

Method: 三阶段框架：预排序阶段生成多个代码候选并过滤；排序阶段通过仿真和自一致性聚类识别最一致输出；后排序细化阶段对排名靠前的候选进行不一致性挖掘和推理增强的LLM提示细化。

Result: 在VerilogEval-Human基准测试中，VFocus显著提高了多个推理LLM的pass@1正确率。

Conclusion: VFocus通过聚焦LLM推理于关键决策点，有效提升了复杂硬件设计任务中Verilog生成的功能正确性。

Abstract: Large Language Models (LLMs) have shown impressive potential in generating
Verilog codes, but ensuring functional correctness remains a challenge.
Existing approaches often rely on self-consistency or simulation feedback to
select the best candidate, but they miss opportunities to focus LLM reasoning
on the most informative parts of the design. We propose VFocus, a three-stage
framework that enhances Verilog generation by sharpening the focus of LLM
reasoning onto critical decision points in the code generation process. In the
\textbf{pre-ranking stage}, VFocus generates multiple code candidates through
LLM prompting, retries for syntactically valid outputs, and introduces a
\textit{Density-guided Filtering} to retain candidates that fall within the
"reasoning sweet spot" for functional correctness. In the \textbf{ranking
stage}, we simulate each code candidate using an automatically generated
testbench and apply self-consistency-based clustering to identify the most
consistent outputs. Finally, in the \textbf{post-ranking refinement stage},
VFocus performs inconsistency mining on top-ranked candidates and invokes
reasoning-augmented LLM prompts for candidate refinement. Experiments on the
VerilogEval-Human benchmark show that VFocus significantly improves the pass@1
correctness across multiple reasoning LLMs, demonstrating its effectiveness in
enhancing Verilog generation for complex hardware design tasks.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [3] [ScenicProver: A Framework for Compositional Probabilistic Verification of Learning-Enabled Systems](https://arxiv.org/abs/2511.02164)
*Eric Vin,Kyle A. Miller,Inigo Incer,Sanjit A. Seshia,Daniel J. Fremont*

Main category: cs.LO

TL;DR: ScenicProver是一个用于学习型信息物理系统的组合验证框架，支持组件化描述、假设-保证契约、多种证据生成方式，并能自动生成保证案例。


<details>
  <summary>Details</summary>
Motivation: 现有的学习型信息物理系统验证工具要么只能为有限类型的系统提供形式化保证，要么将系统作为整体进行测试，缺乏在复杂现实环境中使用多种验证技术进行组合分析的通用框架。

Method: 基于Scenic概率编程语言构建的验证框架，支持：组件化系统描述、使用扩展线性时序逻辑的假设-保证契约、通过测试生成证据、通过Lean 4集成进行形式化证明、导入外部假设、系统化组合证据、自动生成保证案例。

Result: 通过在自动驾驶车辆自动紧急制动系统的案例研究中应用该框架，利用雷达和激光传感器的制造商保证，并在不确定条件下集中测试，相比整体测试方法，在相同计算预算下实现了更强的概率保证。

Conclusion: ScenicProver填补了学习型信息物理系统组合验证的空白，能够有效处理黑盒组件和复杂现实环境，提供更强的系统级保证。

Abstract: Full verification of learning-enabled cyber-physical systems (CPS) has long
been intractable due to challenges including black-box components and complex
real-world environments. Existing tools either provide formal guarantees for
limited types of systems or test the system as a monolith, but no general
framework exists for compositional analysis of learning-enabled CPS using
varied verification techniques over complex real-world environments. This paper
introduces ScenicProver, a verification framework that aims to fill this gap.
Built upon the Scenic probabilistic programming language, the framework
supports: (1) compositional system description with clear component interfaces,
ranging from interpretable code to black boxes; (2) assume-guarantee contracts
over those components using an extension of Linear Temporal Logic containing
arbitrary Scenic expressions; (3) evidence generation through testing, formal
proofs via Lean 4 integration, and importing external assumptions; (4)
systematic combination of generated evidence using contract operators; and (5)
automatic generation of assurance cases tracking the provenance of system-level
guarantees. We demonstrate the framework's effectiveness through a case study
on an autonomous vehicle's automatic emergency braking system with sensor
fusion. By leveraging manufacturer guarantees for radar and laser sensors and
focusing testing efforts on uncertain conditions, our approach enables stronger
probabilistic guarantees than monolithic testing with the same computational
budget.

</details>


### [4] [Nominal Algebraic-Coalgebraic Data Types, with Applications to Infinitary Lambda-Calculi](https://arxiv.org/abs/2511.02595)
*Rémy Cerda*

Main category: cs.LO

TL;DR: 该论文将名义技术扩展到混合归纳-余归纳项，为带有变量绑定的abc-无穷lambda项提供了alpha等价类的余递归原理和替换操作。


<details>
  <summary>Details</summary>
Motivation: 扩展十年前关于名义技术和余归纳数据类型的研究，处理混合归纳-余归纳项，特别是abc-无穷lambda项及其alpha等价类上的捕获避免替换。

Method: 引入混合绑定签名和对应的混合归纳-余归纳项类型，将名义技术扩展到这种设置，为alpha等价类提供余递归原理。

Result: 成功为Lambda_abc集合（a,b,c∈{0,1}的abc-无穷lambda项）提供了名义描述，并在其alpha等价类上定义了捕获避免替换操作。

Conclusion: 名义技术可以有效地扩展到混合归纳-余归纳设置，为带有变量绑定的复杂项结构提供形式化基础。

Abstract: Ten years ago, it was shown that nominal techniques can be used to design
coalgebraic data types with variable binding, so that alpha-equivalence classes
of infinitary terms are directly endowed with a corecursion principle. We
introduce "mixed" binding signatures, as well as the corresponding type of
mixed inductive-coinductive terms. We extend the aforementioned work to this
setting. In particular, this allows for a nominal description of the sets
Lambda_abc of abc-infinitary lambda-terms (for a, b, c in {0,1}) and of
capture-avoiding substitution on alpha-equivalence classes of such terms.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [5] [Neural Network Interoperability Across Platforms](https://arxiv.org/abs/2511.02610)
*Nadia Daoudi,Ivan Alfonso,Jordi Cabot*

Main category: cs.LG

TL;DR: 提出一种自动迁移神经网络代码的方法，使用中间模型抽象NN结构，在PyTorch和TensorFlow之间成功迁移5个神经网络，保持功能等效性。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏专门的神经网络迁移方法，组织在切换深度学习框架时面临困难，需要手动更新代码，耗费大量时间和精力。

Method: 使用中间神经网络模型作为抽象层，在迁移前创建NN的抽象表示，然后自动生成目标框架的代码。

Result: 在PyTorch和TensorFlow之间成功迁移了5个神经网络，生成的网络与原网络功能等效。

Conclusion: 该方法能够有效自动迁移神经网络代码，解决了框架迁移的挑战，相关工具已在线提供。

Abstract: The development of smart systems (i.e., systems enhanced with AI components)
has thrived thanks to the rapid advancements in neural networks (NNs). A wide
range of libraries and frameworks have consequently emerged to support NN
design and implementation. The choice depends on factors such as available
functionalities, ease of use, documentation and community support. After
adopting a given NN framework, organizations might later choose to switch to
another if performance declines, requirements evolve, or new features are
introduced. Unfortunately, migrating NN implementations across libraries is
challenging due to the lack of migration approaches specifically tailored for
NNs. This leads to increased time and effort to modernize NNs, as manual
updates are necessary to avoid relying on outdated implementations and ensure
compatibility with new features. In this paper, we propose an approach to
automatically migrate neural network code across deep learning frameworks. Our
method makes use of a pivot NN model to create an abstraction of the NN prior
to migration. We validate our approach using two popular NN frameworks, namely
PyTorch and TensorFlow. We also discuss the challenges of migrating code
between the two frameworks and how they were approached in our method.
Experimental evaluation on five NNs shows that our approach successfully
migrates their code and produces NNs that are functionally equivalent to the
originals. Artefacts from our work are available online.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [6] [Learned Cost Model for Placement on Reconfigurable Dataflow Hardware](https://arxiv.org/abs/2511.01872)
*Etash Guha,Tianxiao Jiang,Andrew Deng,Jian Zhang,Muthu Annamalai*

Main category: cs.DC

TL;DR: 提出了一种学习方法来预测数据流图在可重构系统上的吞吐量，比传统手工分析方法准确度提高31%-52%，并能生成5.6%更快的编译图


<details>
  <summary>Details</summary>
Motivation: 将ML模型的数据流图映射到可重构系统很困难，不同映射具有不同吞吐量且消耗不同资源约束。完全测量吞吐量成本高昂，而传统手工分析模型依赖代理特征或直觉，会引入误差

Method: 使用学习方法来预测映射的吞吐量，该方法在移除性能注释后仍能保持准确度

Result: 该方法在各种图上的吞吐量预测准确度比传统方法提高31%-52%，使用该方法生成的编译图速度提高5.6%

Conclusion: 学习方法是预测数据流图在可重构系统上吞吐量的有效方法，显著优于传统手工分析模型

Abstract: Mapping a dataflow-graph of an ML model onto a reconfigurable system is
difficult, as different mappings have different throughputs and consume
resource constraints differently. To solve this, a model to evaluate the
throughput of mappings is necessary as measuring throughput completely is
expensive. Many use a hand-designed analytical model, relying on proxy features
or intuition, introducing error. We provide a Learned Approach that predicts
throughput 31%-52% more accurately over a variety of graphs. In addition, our
approach shows no accuracy degradation after removing performance annotations.
We show that using this approach results in 5.6% faster compiled graphs.

</details>
