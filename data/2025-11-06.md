<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Analysis of AdvFusion: Adapter-based Multilingual Learning for Code Large Language Models](https://arxiv.org/abs/2511.02869)
*Amirreza Esmaeili,Fahd Seddik,Yongyi Ji,Fatemeh Fard,Fuxiang Chen*

Main category: cs.SE

TL;DR: AdvFusion是一种参数高效微调方法，在多语言代码任务中表现不一：在代码生成任务中优于AdapterFusion但不如其他PEFT方法；在提交消息生成中不如AdapterFusion；在代码翻译中整体表现较差且随模型增大差距扩大。


<details>
  <summary>Details</summary>
Motivation: 扩展AdvFusion方法到代码大语言模型，研究其在更多软件工程任务（代码生成、代码翻译、提交消息生成）中的表现，探索不同模型/任务的特点。

Method: 使用AdvFusion方法在代码大语言模型上进行参数高效微调，与AdapterFusion、LoRA、Compacter、TaskAdapter等其他PEFT方法进行比较。

Result: 不同任务表现各异：代码生成中AdvFusion优于AdapterFusion但不如其他PEFT方法；提交消息生成中AdapterFusion更好；代码翻译中AdvFusion整体表现较差且差距随模型增大而扩大。

Conclusion: AdvFusion在不同代码任务中的表现存在显著差异，表明需要针对特定任务选择最合适的参数高效微调方法，没有一种方法在所有任务中都表现最佳。

Abstract: Programming languages can benefit from one another by utilizing a language
model for software engineering tasks. Full fine-tuning and Parameter Efficient
Fine-Tuning (PEFT) of Code Language Models (Code-LMs) has been explored for
multilingual knowledge transfer. AdapterFusion is a PEFT architecture that aims
to enhance task performance by leveraging information from multiple programming
languages, but primarily focuses on the target programming language.
  In our previous work, we proposed AdvFusion, a novel PEFT-based approach that
effectively learns from other programming languages before adapting to the
target task. Though previous experiments showed that AdvFusion outperformed
AdapterFusion and LoRA, it was applied on pre-trained Code-LMs and was limited
to only two tasks, code summarization and method name prediction. In this
study, we expanded our work and investigated AdvFusion on Code Large Language
Models (Code-LLMs), considering three new tasks: code generation, code
translation, and commit message generation. We observed that different
Code-LLMs/tasks exhibit different characteristics. In code generation,
AdvFusion outperformed AdapterFusion but not other PEFT methods (LoRA,
Compacter, and TaskAdapter). In commit message generation, AdapterFusion
performed better than AdvFusion, and contrary to code generation, we found that
the other PEFT methods do not have better performance. In code translation,
AdvFusion performed worse than AdapterFusion overall, with the performance gap
marginally widening as the model size increases. However, consistent with code
generation, other PEFT methods showed better performance.

</details>
