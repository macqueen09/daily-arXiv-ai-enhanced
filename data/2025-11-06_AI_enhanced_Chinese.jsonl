{"id": "2511.02869", "pdf": "https://arxiv.org/pdf/2511.02869", "abs": "https://arxiv.org/abs/2511.02869", "authors": ["Amirreza Esmaeili", "Fahd Seddik", "Yongyi Ji", "Fatemeh Fard", "Fuxiang Chen"], "title": "Analysis of AdvFusion: Adapter-based Multilingual Learning for Code Large Language Models", "categories": ["cs.SE", "cs.AI", "cs.PL"], "comment": null, "summary": "Programming languages can benefit from one another by utilizing a language\nmodel for software engineering tasks. Full fine-tuning and Parameter Efficient\nFine-Tuning (PEFT) of Code Language Models (Code-LMs) has been explored for\nmultilingual knowledge transfer. AdapterFusion is a PEFT architecture that aims\nto enhance task performance by leveraging information from multiple programming\nlanguages, but primarily focuses on the target programming language.\n  In our previous work, we proposed AdvFusion, a novel PEFT-based approach that\neffectively learns from other programming languages before adapting to the\ntarget task. Though previous experiments showed that AdvFusion outperformed\nAdapterFusion and LoRA, it was applied on pre-trained Code-LMs and was limited\nto only two tasks, code summarization and method name prediction. In this\nstudy, we expanded our work and investigated AdvFusion on Code Large Language\nModels (Code-LLMs), considering three new tasks: code generation, code\ntranslation, and commit message generation. We observed that different\nCode-LLMs/tasks exhibit different characteristics. In code generation,\nAdvFusion outperformed AdapterFusion but not other PEFT methods (LoRA,\nCompacter, and TaskAdapter). In commit message generation, AdapterFusion\nperformed better than AdvFusion, and contrary to code generation, we found that\nthe other PEFT methods do not have better performance. In code translation,\nAdvFusion performed worse than AdapterFusion overall, with the performance gap\nmarginally widening as the model size increases. However, consistent with code\ngeneration, other PEFT methods showed better performance.", "AI": {"tldr": "AdvFusion\u662f\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u5728\u591a\u8bed\u8a00\u4ee3\u7801\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4e00\uff1a\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u4f18\u4e8eAdapterFusion\u4f46\u4e0d\u5982\u5176\u4ed6PEFT\u65b9\u6cd5\uff1b\u5728\u63d0\u4ea4\u6d88\u606f\u751f\u6210\u4e2d\u4e0d\u5982AdapterFusion\uff1b\u5728\u4ee3\u7801\u7ffb\u8bd1\u4e2d\u6574\u4f53\u8868\u73b0\u8f83\u5dee\u4e14\u968f\u6a21\u578b\u589e\u5927\u5dee\u8ddd\u6269\u5927\u3002", "motivation": "\u6269\u5c55AdvFusion\u65b9\u6cd5\u5230\u4ee3\u7801\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u7814\u7a76\u5176\u5728\u66f4\u591a\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\uff08\u4ee3\u7801\u751f\u6210\u3001\u4ee3\u7801\u7ffb\u8bd1\u3001\u63d0\u4ea4\u6d88\u606f\u751f\u6210\uff09\u4e2d\u7684\u8868\u73b0\uff0c\u63a2\u7d22\u4e0d\u540c\u6a21\u578b/\u4efb\u52a1\u7684\u7279\u70b9\u3002", "method": "\u4f7f\u7528AdvFusion\u65b9\u6cd5\u5728\u4ee3\u7801\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u8fdb\u884c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff0c\u4e0eAdapterFusion\u3001LoRA\u3001Compacter\u3001TaskAdapter\u7b49\u5176\u4ed6PEFT\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u4e0d\u540c\u4efb\u52a1\u8868\u73b0\u5404\u5f02\uff1a\u4ee3\u7801\u751f\u6210\u4e2dAdvFusion\u4f18\u4e8eAdapterFusion\u4f46\u4e0d\u5982\u5176\u4ed6PEFT\u65b9\u6cd5\uff1b\u63d0\u4ea4\u6d88\u606f\u751f\u6210\u4e2dAdapterFusion\u66f4\u597d\uff1b\u4ee3\u7801\u7ffb\u8bd1\u4e2dAdvFusion\u6574\u4f53\u8868\u73b0\u8f83\u5dee\u4e14\u5dee\u8ddd\u968f\u6a21\u578b\u589e\u5927\u800c\u6269\u5927\u3002", "conclusion": "AdvFusion\u5728\u4e0d\u540c\u4ee3\u7801\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u8868\u660e\u9700\u8981\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u9009\u62e9\u6700\u5408\u9002\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u6ca1\u6709\u4e00\u79cd\u65b9\u6cd5\u5728\u6240\u6709\u4efb\u52a1\u4e2d\u90fd\u8868\u73b0\u6700\u4f73\u3002"}}
