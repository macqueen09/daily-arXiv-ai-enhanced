{"id": "2511.03946", "pdf": "https://arxiv.org/pdf/2511.03946", "abs": "https://arxiv.org/abs/2511.03946", "authors": ["Marcelo P. Fiore", "Ohad Kammar", "Georg Moser", "Sam Staton"], "title": "Modular abstract syntax trees (MAST): substitution tensors with second-class sorts", "categories": ["cs.PL"], "comment": null, "summary": "We adapt Fiore, Plotkin, and Turi's treatment of abstract syntax with\nbinding, substitution, and holes to account for languages with second-class\nsorts. These situations include programming calculi such as the Call-by-Value\nlambda-calculus (CBV) and Levy's Call-by-Push-Value (CBPV). Prohibiting\nsecond-class sorts from appearing in variable contexts changes the\ncharacterisation of the abstract syntax from monoids in monoidal categories to\nactions in actegories. We reproduce much of the development through\nbicategorical arguments. We apply the resulting theory by proving substitution\nlemmata for varieties of CBV.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86Fiore\u7b49\u4eba\u7684\u62bd\u8c61\u8bed\u6cd5\u7406\u8bba\uff0c\u4ee5\u5904\u7406\u5177\u6709\u7b2c\u4e8c\u7c7b\u6392\u5e8f\u7684\u8bed\u8a00\uff08\u5982CBV\u548cCBPV\uff09\uff0c\u901a\u8fc7\u5c06\u7279\u5f81\u4ece\u5e7a\u534a\u8303\u7574\u4e2d\u7684\u5e7a\u534a\u7fa4\u6539\u4e3aactegories\u4e2d\u7684\u52a8\u4f5c\uff0c\u5e76\u5e94\u7528\u53cc\u8303\u7574\u8bba\u8bc1\u6765\u8bc1\u660eCBV\u53d8\u4f53\u7684\u66ff\u6362\u5f15\u7406\u3002", "motivation": "\u73b0\u6709\u62bd\u8c61\u8bed\u6cd5\u7406\u8bba\u65e0\u6cd5\u5145\u5206\u5904\u7406\u5177\u6709\u7b2c\u4e8c\u7c7b\u6392\u5e8f\u7684\u7f16\u7a0b\u6f14\u7b97\uff08\u5982CBV\u548cCBPV\uff09\uff0c\u9700\u8981\u6269\u5c55\u7406\u8bba\u4ee5\u652f\u6301\u8fd9\u4e9b\u8bed\u8a00\u7684\u8bed\u6cd5\u3001\u7ed1\u5b9a\u548c\u66ff\u6362\u64cd\u4f5c\u3002", "method": "\u5c06Fiore\u7b49\u4eba\u7684\u62bd\u8c61\u8bed\u6cd5\u7406\u8bba\u4ece\u5e7a\u534a\u8303\u7574\u4e2d\u7684\u5e7a\u534a\u7fa4\u63a8\u5e7f\u5230actegories\u4e2d\u7684\u52a8\u4f5c\uff0c\u4f7f\u7528\u53cc\u8303\u7574\u8bba\u8bc1\u8fdb\u884c\u5f00\u53d1\uff0c\u5e76\u5e94\u7528\u4e8eCBV\u53d8\u4f53\u7684\u66ff\u6362\u5f15\u7406\u8bc1\u660e\u3002", "result": "\u6210\u529f\u5efa\u7acb\u4e86\u9002\u7528\u4e8e\u7b2c\u4e8c\u7c7b\u6392\u5e8f\u8bed\u8a00\u7684\u62bd\u8c61\u8bed\u6cd5\u7406\u8bba\u6846\u67b6\uff0c\u80fd\u591f\u5904\u7406CBV\u548cCBPV\u7b49\u6f14\u7b97\u7684\u8bed\u6cd5\u7ed3\u6784\uff0c\u5e76\u8bc1\u660e\u4e86\u76f8\u5173\u66ff\u6362\u5f15\u7406\u3002", "conclusion": "\u901a\u8fc7\u5c06\u62bd\u8c61\u8bed\u6cd5\u7406\u8bba\u6269\u5c55\u5230actegories\u6846\u67b6\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u5904\u7406\u5177\u6709\u7b2c\u4e8c\u7c7b\u6392\u5e8f\u7684\u7f16\u7a0b\u8bed\u8a00\uff0c\u4e3a\u8fd9\u7c7b\u8bed\u8a00\u7684\u8bed\u4e49\u5206\u6790\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2511.03866", "pdf": "https://arxiv.org/pdf/2511.03866", "abs": "https://arxiv.org/abs/2511.03866", "authors": ["Arijit Bhattacharjee", "Ali TehraniJamsaz", "Le Chen", "Niranjan Hasabnis", "Mihai Capota", "Nesreen Ahmed", "Ali Jannesari"], "title": "OMPILOT: Harnessing Transformer Models for Auto Parallelization to Shared Memory Computing Paradigms", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.PF", "cs.PL"], "comment": null, "summary": "Recent advances in large language models (LLMs) have significantly\naccelerated progress in code translation, enabling more accurate and efficient\ntransformation across programming languages. While originally developed for\nnatural language processing, LLMs have shown strong capabilities in modeling\nprogramming language syntax and semantics, outperforming traditional rule-based\nsystems in both accuracy and flexibility. These models have streamlined\ncross-language conversion, reduced development overhead, and accelerated legacy\ncode migration. In this paper, we introduce OMPILOT, a novel domain-specific\nencoder-decoder transformer tailored for translating C++ code into OpenMP,\nenabling effective shared-memory parallelization. OMPILOT leverages custom\npre-training objectives that incorporate the semantics of parallel constructs\nand combines both unsupervised and supervised learning strategies to improve\ncode translation robustness. Unlike previous work that focused primarily on\nloop-level transformations, OMPILOT operates at the function level to capture a\nwider semantic context. To evaluate our approach, we propose OMPBLEU, a novel\ncomposite metric specifically crafted to assess the correctness and quality of\nOpenMP parallel constructs, addressing limitations in conventional translation\nmetrics.", "AI": {"tldr": "OMPILOT\u662f\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u5c06C++\u4ee3\u7801\u7ffb\u8bd1\u6210OpenMP\u7684\u9886\u57df\u7279\u5b9a\u7f16\u7801\u5668-\u89e3\u7801\u5668\u8f6c\u6362\u5668\uff0c\u901a\u8fc7\u51fd\u6570\u7ea7\u7ffb\u8bd1\u548c\u5b9a\u5236\u9884\u8bad\u7ec3\u76ee\u6807\u5b9e\u73b0\u6709\u6548\u7684\u5171\u4eab\u5185\u5b58\u5e76\u884c\u5316\u3002", "motivation": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u7ffb\u8bd1\u65b9\u9762\u7684\u8fdb\u5c55\uff0c\u89e3\u51b3\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u7cfb\u7edf\u5728\u51c6\u786e\u6027\u548c\u7075\u6d3b\u6027\u4e0a\u7684\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u9488\u5bf9C++\u5230OpenMP\u7684\u5e76\u884c\u5316\u7ffb\u8bd1\u9700\u6c42\u3002", "method": "\u91c7\u7528\u9886\u57df\u7279\u5b9a\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u8f6c\u6362\u5668\u67b6\u6784\uff0c\u7ed3\u5408\u5b9a\u5236\u9884\u8bad\u7ec3\u76ee\u6807\uff08\u5305\u542b\u5e76\u884c\u6784\u9020\u8bed\u4e49\uff09\uff0c\u4f7f\u7528\u65e0\u76d1\u7763\u548c\u76d1\u7763\u5b66\u4e60\u7b56\u7565\uff0c\u5728\u51fd\u6570\u7ea7\u522b\u8fdb\u884c\u7ffb\u8bd1\u4ee5\u6355\u83b7\u66f4\u5e7f\u6cdb\u7684\u8bed\u4e49\u4e0a\u4e0b\u6587\u3002", "result": "\u5f00\u53d1\u4e86OMPBLEU\u8bc4\u4f30\u6307\u6807\u6765\u4e13\u95e8\u8bc4\u4f30OpenMP\u5e76\u884c\u6784\u9020\u7684\u6b63\u786e\u6027\u548c\u8d28\u91cf\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u7ffb\u8bd1\u6307\u6807\u7684\u5c40\u9650\u6027\u3002", "conclusion": "OMPILOT\u901a\u8fc7\u51fd\u6570\u7ea7\u7ffb\u8bd1\u548c\u4e13\u95e8\u8bbe\u8ba1\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u4e3aC++\u4ee3\u7801\u5230OpenMP\u7684\u7ffb\u8bd1\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u548c\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u4ee3\u7801\u5e76\u884c\u5316\u7684\u53d1\u5c55\u3002"}}
{"id": "2511.04115", "pdf": "https://arxiv.org/pdf/2511.04115", "abs": "https://arxiv.org/abs/2511.04115", "authors": ["Ruksit Rojpaisarnkit", "Youmei Fan", "Kenichi Matsumoto", "Raula Gaikovina Kula"], "title": "How Natural Language Proficiency Shapes GenAI Code for Software Engineering Tasks", "categories": ["cs.SE", "cs.PL"], "comment": "7 pages, 4 tables, 1 figure", "summary": "With the widespread adoption of Foundation Model (FM)-powered tools in\nsoftware engineering, the natural language prompt has become a critical\ninterface between developers and Large Language Models (LLMs). While much\nresearch has focused on prompt structure, the natural language proficiency is\nan underexplored factor that can influence the quality of generated code. This\npaper investigates whether the English language proficiency itself independent\nof the prompting technique affects the proficiency and correctness of code\ngenerated by LLMs. Using the HumanEval dataset, we systematically varied the\nEnglish proficiency of prompts from basic to advanced for 164 programming tasks\nand measured the resulting code proficiency and correctness. Our findings show\nthat LLMs default to an intermediate (B2) natural language level. While the\neffect on the resulting code proficiency was model-dependent, we found that\nhigher-proficiency prompts consistently yielded more correct code across all\nmodels. These results demonstrate that natural language proficiency is a key\nlever for controlling code generation, helping developers tailor AI output and\nimprove the reliability of solutions.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u82f1\u8bed\u8bed\u8a00\u80fd\u529b\u662f\u5426\u72ec\u7acb\u4e8e\u63d0\u793a\u6280\u672f\u5f71\u54cdLLM\u751f\u6210\u4ee3\u7801\u7684\u8d28\u91cf\u548c\u6b63\u786e\u6027\uff0c\u53d1\u73b0\u9ad8\u7ea7\u82f1\u8bed\u63d0\u793a\u80fd\u6301\u7eed\u4ea7\u751f\u66f4\u6b63\u786e\u7684\u4ee3\u7801\u3002", "motivation": "\u968f\u7740\u57fa\u7840\u6a21\u578b\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u6210\u4e3a\u5f00\u53d1\u8005\u4e0eLLM\u7684\u5173\u952e\u63a5\u53e3\u3002\u867d\u7136\u5df2\u6709\u7814\u7a76\u5173\u6ce8\u63d0\u793a\u7ed3\u6784\uff0c\u4f46\u81ea\u7136\u8bed\u8a00\u80fd\u529b\u8fd9\u4e00\u53ef\u80fd\u5f71\u54cd\u751f\u6210\u4ee3\u7801\u8d28\u91cf\u7684\u56e0\u7d20\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u4f7f\u7528HumanEval\u6570\u636e\u96c6\uff0c\u5bf9164\u4e2a\u7f16\u7a0b\u4efb\u52a1\u7cfb\u7edf\u6027\u5730\u5c06\u63d0\u793a\u7684\u82f1\u8bed\u80fd\u529b\u4ece\u57fa\u7840\u5230\u9ad8\u7ea7\u8fdb\u884c\u53d8\u5316\uff0c\u5e76\u6d4b\u91cf\u751f\u6210\u7684\u4ee3\u7801\u80fd\u529b\u548c\u6b63\u786e\u6027\u3002", "result": "LLM\u9ed8\u8ba4\u4f7f\u7528\u4e2d\u7ea7(B2)\u81ea\u7136\u8bed\u8a00\u6c34\u5e73\u3002\u867d\u7136\u5bf9\u4ee3\u7801\u80fd\u529b\u7684\u5f71\u54cd\u56e0\u6a21\u578b\u800c\u5f02\uff0c\u4f46\u9ad8\u7ea7\u82f1\u8bed\u63d0\u793a\u5728\u6240\u6709\u6a21\u578b\u4e2d\u90fd\u80fd\u6301\u7eed\u4ea7\u751f\u66f4\u6b63\u786e\u7684\u4ee3\u7801\u3002", "conclusion": "\u81ea\u7136\u8bed\u8a00\u80fd\u529b\u662f\u63a7\u5236\u4ee3\u7801\u751f\u6210\u7684\u5173\u952e\u6760\u6746\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u5b9a\u5236AI\u8f93\u51fa\u5e76\u63d0\u9ad8\u89e3\u51b3\u65b9\u6848\u7684\u53ef\u9760\u6027\u3002"}}
