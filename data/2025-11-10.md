<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 1]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [FuseFlow: A Fusion-Centric Compilation Framework for Sparse Deep Learning on Streaming Dataflow](https://arxiv.org/abs/2511.04768)
*Rubens Lacouture,Nathan Zhang,Ritvik Sharma,Marco Siracusa,Fredrik Kjolstad,Kunle Olukotun,Olivia Hsu*

Main category: cs.LG

TL;DR: FuseFlow是一个将PyTorch稀疏模型转换为融合稀疏数据流图的编译器，支持跨表达式融合和多种优化，在GPT-3等应用中实现2.7倍加速


<details>
  <summary>Details</summary>
Motivation: 随着深度学习模型规模扩大，稀疏计算和专用数据流硬件成为提升效率的重要方案，需要编译器支持稀疏操作的融合优化

Method: 开发FuseFlow编译器，支持跨表达式融合、并行化、数据流排序和稀疏分块等优化，使用周期精确的数据流模拟器进行微架构分析

Result: 在四个真实世界机器学习应用中验证，发现完全融合并非总是最优，融合粒度取决于模型特性。GPT-3 BigBird稀疏注意力实现2.7倍加速

Conclusion: FuseFlow证明了稀疏模型中融合策略的重要性，提供了识别和修剪次优配置的启发式方法，融合粒度需要根据具体模型调整

Abstract: As deep learning models scale, sparse computation and specialized dataflow
hardware have emerged as powerful solutions to address efficiency. We propose
FuseFlow, a compiler that converts sparse machine learning models written in
PyTorch to fused sparse dataflow graphs for reconfigurable dataflow
architectures (RDAs). FuseFlow is the first compiler to support general
cross-expression fusion of sparse operations. In addition to fusion across
kernels (expressions), FuseFlow also supports optimizations like
parallelization, dataflow ordering, and sparsity blocking. It targets a
cycle-accurate dataflow simulator for microarchitectural analysis of fusion
strategies. We use FuseFlow for design-space exploration across four real-world
machine learning applications with sparsity, showing that full fusion (entire
cross-expression fusion across all computation in an end-to-end model) is not
always optimal for sparse models-fusion granularity depends on the model
itself. FuseFlow also provides a heuristic to identify and prune suboptimal
configurations. Using Fuseflow, we achieve performance improvements, including
a ~2.7x speedup over an unfused baseline for GPT-3 with BigBird block-sparse
attention.

</details>
