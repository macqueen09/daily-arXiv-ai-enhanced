{"id": "2511.11055", "pdf": "https://arxiv.org/pdf/2511.11055", "abs": "https://arxiv.org/abs/2511.11055", "authors": ["Michael Schwarz", "Julian Erhard"], "title": "Data Race Detection by Digest-Driven Abstract Interpretation (Extended Version)", "categories": ["cs.PL", "cs.SE"], "comment": "Extended of paper accepted to appear at VMCAI'26; 29 Pages, including 2 Appendices", "summary": "Sound static analysis can prove the absence of data races by establishing that no two conflicting memory accesses can occur at the same time. We repurpose the concept of digests -- summaries of computational histories originally introduced to bring tunable concurrency-sensitivity to thread-modular value analysis by abstract interpretation, extending this idea to race detection: We use digests to capture the conditions under which conflicting accesses may not happen in parallel. To formalize this, we give a definition of data races in the thread-modular local trace semantics and show how exclusion criteria for potential conflicts can be expressed as digests. We report on our implementation of digest-driven data race detection in the static analyzer Goblint, and evaluate it on the SV-COMP benchmark suite. Combining the lockset digest with digests reasoning on thread ids and thread joins increases the number of correctly solved tasks by more than a factor of five compared to lockset reasoning alone."}
{"id": "2511.11070", "pdf": "https://arxiv.org/pdf/2511.11070", "abs": "https://arxiv.org/abs/2511.11070", "authors": ["Sangho Lim", "Hyoungjin Lim", "Wonyeol Lee", "Xavier Rival", "Hongseok Yang"], "title": "Optimising Density Computations in Probabilistic Programs via Automatic Loop Vectorisation", "categories": ["cs.PL"], "comment": "70 pages, 19 figures, the first two authors contributed equally to this work, accepted at POPL'26", "summary": "Probabilistic programming languages (PPLs) are a popular tool for high-level modelling across many fields. They provide a range of algorithms for probabilistic inference, which analyse models by learning their parameters from a dataset or estimating their posterior distributions. However, probabilistic inference is known to be very costly. One of the bottlenecks of probabilistic inference stems from the iteration over entries of a large dataset or a long series of random samples. Vectorisation can mitigate this cost, but manual vectorisation is error-prone, and existing automatic techniques are often ad-hoc and limited, unable to handle general repetition structures, such as nested loops and loops with data-dependent control flow, without significant user intervention. To address this bottleneck, we propose a sound and effective method for automatically vectorising loops in probabilistic programs. Our method achieves high throughput using speculative parallel execution of loop iterations, while preserving the semantics of the original loop through a fixed-point check. We formalise our method as a translation from an imperative PPL into a lower-level target language with primitives geared towards vectorisation. We implemented our method for the Pyro PPL and evaluated it on a range of probabilistic models. Our experiments show significant performance gains against an existing vectorisation baseline, achieving $1.1$--$6\\times$ speedups and reducing GPU memory usage in many cases. Unlike the baseline, which is limited to a subset of models, our method effectively handled all the tested models."}
{"id": "2511.11264", "pdf": "https://arxiv.org/pdf/2511.11264", "abs": "https://arxiv.org/abs/2511.11264", "authors": ["Tobias Kappé", "Alexandra Silva", "Jana Wagemaker"], "title": "Kleene Algebra", "categories": ["cs.PL"], "comment": null, "summary": "This booklet serves as an introduction to Kleene Algebra (KA), a set of laws that can be used to study general equivalences between programs. It discusses how general programs can be modeled using regular expressions, how those expressions correspond to automata, and how this correspondence can be exploited to obtain the central result of KA, namely that an equivalence of regular expressions is true if and only if it can be proved using the laws of KA. Each chapter closes with a set of exercises to further build intuition and understanding, and there is an optional chapter that develops automata theory through the lens of coalgebra."}
{"id": "2511.11292", "pdf": "https://arxiv.org/pdf/2511.11292", "abs": "https://arxiv.org/abs/2511.11292", "authors": ["Santiago Arranz-Olmos", "Gilles Barthe", "Lionel Blatter", "Benjamin Grégoire", "Vincent Laporte", "Paolo Torrini"], "title": "The Jasmin Compiler Preserves Cryptographic Security", "categories": ["cs.PL", "cs.CR"], "comment": null, "summary": "Jasmin is a programming and verification framework for developing efficient, formally verified, cryptographic implementations. A main component of the framework is the Jasmin compiler, which empowers programmers to write efficient implementations of state-of-the-art cryptographic primitives, including post-quantum cryptographic standards. The Jasmin compiler is proven functionally correct in the Rocq prover. However, this functional correctness statement does not apply to nonterminating or probabilistic computations, which are essential features in cryptography.\n  In this paper, we significantly enhance the guarantees of the compiler by showing, in the Rocq prover, that its front-end (25 out of 30 passes) preserves cryptographic security. To this end, we first define a Relational Hoare Logic tailored for compiler correctness proofs. We prove the soundness of our logic w.r.t. a new denotational semantics of Jasmin programs based on interaction trees. Secondly, we use our program logic to prove the functional correctness of the (unmodified) Jasmin compiler w.r.t. said semantics. Lastly, we formalize cryptographic security -- focusing on IND-CCA -- with interaction trees and prove that the Jasmin compiler preserves cryptographic security."}
{"id": "2511.11445", "pdf": "https://arxiv.org/pdf/2511.11445", "abs": "https://arxiv.org/abs/2511.11445", "authors": ["Ian Benson", "Alexei Semenov"], "title": "AI as a component in the action research tradition of learning-by-doing", "categories": ["cs.CY", "cs.PL", "math.HO"], "comment": "14 pages, 2 figures", "summary": "We consider learning mathematics through action research, hacking, discovery, inquiry, learning-by-doing as opposed to the instruct and perform, industrial model of the 19th century. A learning model based on self-awareness, types, functions, structured drawing and formal diagrams addresses the weaknesses of drill and practice and the pitfalls of statistical prediction with Large Language Models.\n  In other words, we build mathematics/informatics education on the activity of a professional mathematician in mathematical modelling and designing programs. This tradition emphasises the role of dialogue and doing mathematics. In the Language/Action approach the teacher designs mathematising situations that scaffold previously encountered, or not-known-how-to-solve problems for the learner while teachers and teacher/interlocutors supervise the process.\n  A critical feature is the written-oral dialogue between the learner and the teacher. As a rule, this is 1 to 1 communication. The role of the teacher/interlocutor, a more knowledgeable other, is mostly performed by a more senior student, 1 per 5 to 7 pupils. After Doug Engelbart we propose the metaphor of human intellect augmented by digital technologies such as interactive development environments or AI. Every human has their bio and digital parts. The bio part of the learner reacts to their work through dialogue in the mind. The digital part poses questions, interprets code and proposes not necessarily sound ideas."}
