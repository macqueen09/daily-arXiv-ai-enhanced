<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 4]
- [cs.SE](#cs.SE) [Total: 2]
- [cs.LG](#cs.LG) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Modular GPU Programming with Typed Perspectives](https://arxiv.org/abs/2511.11939)
*Manya Bansal,Daniel Sainati,Joseph W. Cutler,Saman Amarasinghe,Jonathan Ragan-Kelley*

Main category: cs.PL

TL;DR: Prism是一个新的GPU编程语言，通过类型化视角在类型层面体现线程控制粒度，解决了GPU编程中个体线程控制与集体操作之间的模块化矛盾，既保证了安全性又不牺牲性能。


<details>
  <summary>Details</summary>
Motivation: 现代GPU编程需要在控制单个线程行为的同时跟踪多个线程协同执行集体操作（如Tensor Core指令），这种矛盾使得模块化编程容易出错。封装集体操作的函数虽然按线程调用，但必须由线程组协作执行。

Method: 引入Prism语言，核心思想是类型化视角，在类型层面体现程序员控制线程行为的粒度。设计了Prism语言，实现了编译器，并基于核心演算Bundl建立了理论基础。

Result: 在Prism中实现了先进的GPU内核，发现它提供了足够的安全保证，使程序员能够自信地编写模块化代码，同时不牺牲性能。

Conclusion: Prism通过类型化视角成功恢复了GPU编程的模块性，同时为程序员提供了对集体操作的必要低级控制，实现了安全性与高性能的平衡。

Abstract: To achieve peak performance on modern GPUs, one must balance two frames of mind: issuing instructions to individual threads to control their behavior, while simultaneously tracking the convergence of many threads acting in concert to perform collective operations like Tensor Core instructions. The tension between these two mindsets makes modular programming error prone. Functions that encapsulate collective operations, despite being called per-thread, must be executed cooperatively by groups of threads.
  In this work, we introduce Prism, a new GPU language that restores modularity while still giving programmers the low-level control over collective operations necessary for high performance. Our core idea is typed perspectives, which materialize, at the type level, the granularity at which the programmer is controlling the behavior of threads. We describe the design of Prism, implement a compiler for it, and lay its theoretical foundations in a core calculus called Bundl. We implement state-of-the-art GPU kernels in Prism and find that it offers programmers the safety guarantees needed to confidently write modular code without sacrificing performance.

</details>


### [2] [The Search for Constrained Random Generators](https://arxiv.org/abs/2511.12253)
*Harrison Goldstein,Hila Peleg,Cassia Torczon,Daniel Sainati,Leonidas Lampropoulos,Benjamin C. Pierce*

Main category: cs.PL

TL;DR: 本文提出了一种基于演绎程序综合的新方法来解决基于属性测试中的约束随机生成问题，通过将递归谓词重写为catamorphisms并与适当的anamorphisms匹配来合成正确的生成器。


<details>
  <summary>Details</summary>
Motivation: 基于属性测试面临的主要挑战是约束随机生成问题：给定程序值的谓词，需要从满足该谓词的所有值中随机采样。由于PBT中使用的可执行规范通常有输入值必须满足的前提条件，且满足条件的值往往稀疏分布，因此高效解决此问题至关重要。

Method: 提出基于生成器指称语义的综合规则集，形成自动合成正确生成器的程序。系统通过将递归谓词重写为catamorphisms并与适当的anamorphisms匹配来处理递归谓词。实现为Lean定理证明器中的可扩展库Palamedes，综合算法基于标准证明搜索策略构建。

Result: 开发了Palamedes实现，这是一个在Lean定理证明器中的可扩展库。该方法理论上比其他递归函数综合方法更简单，同时仍保持极高的表达能力。

Conclusion: 该方法通过演绎程序综合技术有效解决了PBT中的约束随机生成问题，利用定理证明器的证明自动化能力降低了实现负担，并为处理递归谓词提供了理论简单且表达力强的解决方案。

Abstract: Among the biggest challenges in property-based testing (PBT) is the constrained random generation problem: given a predicate on program values, randomly sample from the set of all values satisfying that predicate, and only those values. Efficient solutions to this problem are critical, since the executable specifications used by PBT often have preconditions that input values must satisfy in order to be valid test cases, and satisfying values are often sparsely distributed.
  We propose a novel approach to this problem using ideas from deductive program synthesis. We present a set of synthesis rules, based on a denotational semantics of generators, that give rise to an automatic procedure for synthesizing correct generators. Our system handles recursive predicates by rewriting them as catamorphisms and then matching with appropriate anamorphisms; this is theoretically simpler than other approaches to synthesis for recursive functions, yet still extremely expressive.
  Our implementation, Palamedes, is an extensible library for the Lean theorem prover. The synthesis algorithm itself is built on standard proof-search tactics, reducing implementation burden and allowing the algorithm to benefit from further advances in Lean proof automation.

</details>


### [3] [Equivalence Checking of ML GPU Kernels](https://arxiv.org/abs/2511.12638)
*Kshitij Dubey,Benjamin Driscoll,Anjiang Wei,Neeraj Kayal,Rahul Sharma,Alex Aiken*

Main category: cs.PL

TL;DR: 提出了第一个GPU内核等价检查器VOLTA，用于形式化验证手工优化、LLM生成和编译器优化的机器学习内核的正确性。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习和大型语言模型的发展，GPU内核执行成本高昂，成为优化的重要目标。但现有LLM生成的内核缺乏形式化保证，需要验证其正确性。

Method: 开发了VOLTA等价检查器，针对特定类别的GPU内核（包括机器学习计算）进行验证，证明其对于该类内核是完备的。

Result: VOLTA能够验证卷积、矩阵乘法和各种注意力机制等机器学习计算，为GPU内核优化提供了形式化验证工具。

Conclusion: 该工作填补了GPU内核形式化验证的空白，为机器学习内核优化提供了可靠的正确性保证。

Abstract: With the rapid progress of deep learning and large language models (LLMs), companies now spend enormous sums executing GPU kernels. These kernels have, therefore, become prime targets for aggressive optimization. Recent efforts increasingly leverage LLMs to generate GPU kernels, but make no formal guarantees about the generated kernels. We present the first equivalence checker for GPU kernels and use it to formally verify the correctness of machine learning (ML) kernels optimized by hand, by LLMs, and by compilers. We show that our equivalence checker is sound and, for a well-defined class of GPU kernels which includes the programs of interest, complete. Our implementation, VOLTA, can verify ML computations such as convolutions, matrix multiplications, and various attention mechanisms.

</details>


### [4] [Cost-Driven Synthesis of Sound Abstract Interpreters](https://arxiv.org/abs/2511.13663)
*Qiuhan Gu,Avaljot Singh,Gagandeep Singh*

Main category: cs.PL

TL;DR: 使用现代LLM合成全局可靠的抽象解释器，用于神经网络验证，通过约束优化和数学基础的成本函数来保证声音性。


<details>
  <summary>Details</summary>
Motivation: 构建提供全局可靠性保证的抽象解释器仍然是抽象解释中的主要障碍，研究是否可以利用现代LLM来减轻这一负担。

Method: 将合成建模为约束优化问题，引入基于数学的成本函数来衡量声音性，开发统一框架结合LLM生成、语法语义验证和成本引导反馈机制。

Result: 实证结果表明，该框架不仅匹配手工构建的转换器质量，更重要的是发现了复杂非线性算子的高精度可靠转换器，这些在现有文献中缺失。

Conclusion: LLM能够成功合成跨多个抽象域的非平凡抽象解释器，为神经网络验证提供可靠保证。

Abstract: Constructing abstract interpreters that provide global soundness guarantees remains a major obstacle in abstract interpretation. We investigate whether modern LLMs can reduce this burden by leveraging them to synthesize sound, non-trivial abstract interpreters across multiple abstract domains in the setting of neural network verification. We formulate synthesis as a constrained optimization problem and introduce a novel mathematically grounded cost function for measuring unsoundness under strict syntactic and semantic constraints. Based on this formulation, we develop a unified framework that unifies LLM-based generation with syntactic and semantic validation and a quantitative cost-guided feedback mechanism. Empirical results demonstrate that our framework not only matches the quality of handcrafted transformers, but more importantly, discovers sound, high-precision transformers for complex nonlinear operators that are absent from existing literature.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [5] [Reflections on the design, applications and implementations of the normative specification language eFLINT](https://arxiv.org/abs/2511.12276)
*L. Thomas van Binsbergen,Christopher A. Esterhuyse,Tim Müller*

Main category: cs.SE

TL;DR: 本文介绍了eFLINT领域特定软件语言，该语言结合声明式和过程式元素，用于自动化法律合规检查，涵盖软件系统运行前、中、后的合规验证。


<details>
  <summary>Details</summary>
Motivation: 随着软件在社会实践中的普及，软件合规性检查变得越来越重要且成本高昂。法律和法规不断更新，主观的法律解释过程以及法律与软件专业知识的交叉需求使得自动化合规变得困难。

Method: 开发了eFLINT领域特定软件语言，该语言结合声明式元素（用于推理情况）和过程式元素（用于推理场景），形式化法律概念与计算概念之间的联系。

Result: eFLINT语言能够自动化执行软件系统运行前、运行中和运行后的合规检查，并通过各种应用场景验证了其有效性。

Conclusion: eFLINT语言的设计经验和应用案例为自动化合规领域的语言开发者提供了有价值的见解，展示了如何平衡冲突需求并实现有效的合规自动化解决方案。

Abstract: Checking the compliance of software against laws, regulations and contracts is increasingly important and costly as the embedding of software into societal practices is getting more pervasive. Moreover, the digitalised services provided by governmental organisations and companies are governed by an increasing amount of laws and regulations, requiring highly adaptable compliance practices. A potential solution is to automate compliance using software. However, automating compliance is difficult for various reasons. Legal practices involve subjective processes such as interpretation and qualification. New laws and regulations come into effect regularly and laws and regulations, as well as their interpretations, are subjected to constant revision. In addition, computational reasoning with laws requires a cross-disciplinary process involving both legal and software expertise.
  This paper reflects on the domain-specific software language eFLINT developed to experiment with novel solutions. The language combines declarative and procedural elements to reason about situations and scenarios respectively, explicates and formalises connections between legal concepts and computational concepts, and is designed to automate compliance checks both before, during and after a software system runs. The various goals and applications areas for the language give rise to (conflicting) requirements. This paper reflects on the current design of the language by recalling various applications, the requirements they imposed, and subsequent design decisions. As such, this paper reports on results and insights of an investigation that can benefit language developers within the field of automated compliance.

</details>


### [6] [Enhancing LLM Code Generation Capabilities through Test-Driven Development and Code Interpreter](https://arxiv.org/abs/2511.12823)
*Sajed Jalil,Shuvo Saha,Hossain Mohammad Seym*

Main category: cs.SE

TL;DR: 提出了一种结合测试驱动开发(TDD)和代码解释器(CI)的新方法，使用开源模型提升孟加拉语代码生成能力，无需微调即可达到85%准确率，最小模型可达最大模型98%的性能。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语拥有2.42亿母语使用者，但在LLM训练中关注不足。现有代码生成技术需要大量专业知识和资源，目标是为资源受限的新兴市场用户提供母语代码生成工具。

Method: 结合测试驱动开发(TDD)和代码解释器(CI)，使用开源权重模型，无需微调即可提升孟加拉语提示的代码生成性能。

Result: 将孟加拉语代码生成的基线准确率提升至85%，同一系列中最小的模型可达到最大模型98%的准确率。

Conclusion: 该方法有效提升了孟加拉语代码生成能力，证明了无需微调即可在资源受限环境中实现高性能代码生成，所有结果已在GitHub公开。

Abstract: Over the past few years, improving LLM code generation capabilities has been a key focus in NLP research. Despite Bengali having 242 million native speakers worldwide, it receives little attention when it comes to training LLMs. More recently, various fine-tuning and augmented generation techniques have been employed to significantly enhance code generation performance. However, they require considerable expertise and resources to utilize effectively as an end user. The goal of our work is to democratize access to powerful code generation tools in resource-constrained emerging markets, enabling users to leverage them in their native language.
  We introduce a novel approach that combines Test-Driven Development (TDD) and Code Interpreter (CI), utilizing open-weight models, which improves the baseline accuracy for code generation with Bengali prompts and achieves an overall accuracy of 85%. Our approach requires no finetuning and proves that even the smallest models in the same family can attain up to 98% accuracy compared to the largest models. All of our results are publicly shared in GitHub for validation and reproducibility.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [7] [The Anatomy of a Triton Attention Kernel](https://arxiv.org/abs/2511.11581)
*Burkhard Ringlein,Jan van Lunteren,Radu Stoica,Thomas Parnell*

Main category: cs.LG

TL;DR: 开发了一个基于Triton的跨平台LLM推理系统，通过paged attention内核在NVIDIA和AMD GPU上实现最先进性能，将通用Triton注意力内核性能从19.7%提升到105.9%。


<details>
  <summary>Details</summary>
Motivation: 实现跨硬件架构的可移植LLM推理平台，消除低层手动调优需求，同时保持最佳效率。

Method: 使用Triton领域特定即时编译语言开发最先进的paged attention内核，结合算法和系统级改进、参数自动调优，并集成到流行推理服务器中。

Result: 在NVIDIA和AMD GPU上实现最先进性能，将通用Triton注意力内核性能从19.7%提升到105.9%。

Conclusion: 开源领域特定语言可以用于解锁跨不同GPU厂商的模型可移植性。

Abstract: A long-standing goal in both industry and academia is to develop an LLM inference platform that is portable across hardware architectures, eliminates the need for low-level hand-tuning, and still delivers best-in-class efficiency. In this work, we demonstrate that portable, efficient cross-platform LLM inference is indeed possible and share our experience. We develop a state-of-the-art paged attention kernel, the core performance-critical component of many LLM deployments, that builds exclusively on the domain-specific just-in-time compiled language Triton to achieve state-of-the-art performance on both NVIDIA and AMD GPUs. We describe our high-level approach, the key algorithmic and system-level improvements, the parameter auto-tuning required to unlock efficiency, and the integrations into a popular inference server that are necessary to bring the performance of a generic Triton attention kernel from 19.7% of the state-of-the-art to 105.9%. Our results highlight how open-source domain-specific languages can be leveraged to unlock model portability across different GPU vendors.

</details>
