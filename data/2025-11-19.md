<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 2]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.DC](#cs.DC) [Total: 1]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Library Liberation: Competitive Performance Matmul Through Compiler-composed Nanokernels](https://arxiv.org/abs/2511.13764)
*Arun Thangamani,Md Asghar Ahmad Shahid,Adam Siemieniuk,Rolf Morel,Renato Golin,Alexander Heinecke*

Main category: cs.LG

TL;DR: 提出了一种基于MLIR的编译方案，自动生成可扩展的高性能微内核，无需依赖低层库即可实现接近最优的代码生成。


<details>
  <summary>Details</summary>
Motivation: AI和机器学习工作负载快速发展，导致高层领域操作与高效硬件利用之间存在差距。实现接近峰值性能仍需深度硬件专业知识，这增加了复杂性并限制了大多数ML从业者的可扩展性。

Method: 利用MLIR方言桥接领域级操作和处理器能力，通过从低层IR构造组合纳米内核，实现接近最优的寄存器利用率，形成针对每个目标的高效微内核。

Result: 实验表明生成的纳米内核具有生产质量，与最先进的微内核库具有竞争力。

Conclusion: 该编译方案能够自动生成高性能微内核，减少对低层库的依赖，为ML从业者提供更易用的高性能计算解决方案。

Abstract: The rapidly evolving landscape of AI and machine learning workloads has widened the gap between high-level domain operations and efficient hardware utilization. Achieving near-peak performance still demands deep hardware expertise-experts either handcraft target-specific kernels (e.g., DeepSeek) or rely on specialized libraries (e.g., CUTLASS)-both of which add complexity and limit scalability for most ML practitioners.
  This paper introduces a compilation scheme that automatically generates scalable, high-performance microkernels by leveraging the MLIR dialects to bridge domain-level operations and processor capabilities. Our approach removes dependence on low-level libraries by enabling the compiler to auto-generate near-optimal code directly. At its core is a mechanism for composing nanokernels from low-level IR constructs with near-optimal register utilization, forming efficient microkernels tailored to each target. We implement this technique in an MLIR-based compiler supporting both vector and tile based CPU instructions. Experiments show that the generated nanokernels are of production-quality, and competitive with state-of-the-art microkernel libraries.

</details>


### [2] [Compiling to linear neurons](https://arxiv.org/abs/2511.13769)
*Joey Velez-Ginorio,Nada Amin,Konrad Kording,Steve Zdancewic*

Main category: cs.LG

TL;DR: Cajal是一个类型化、高阶线性编程语言，用于直接编程神经网络，将离散算法编译为线性神经元，实现与基于梯度的学习兼容。


<details>
  <summary>Details</summary>
Motivation: 当前神经网络编程依赖间接学习算法（如梯度下降），缺乏离散结构，无法将离散算法编译到网络中。需要一种直接编程神经网络的方法。

Method: 开发Cajal编程语言，证明其程序可编译为线性神经元，使离散算法能以可微分形式表达，并与基于梯度的学习兼容。

Result: 实验表明，将Cajal生成的线性神经元与其他神经网络链接，可在学习前确定部分功能，使网络学习更快、数据效率更高、更易调试。

Conclusion: 线性编程语言为直接编程神经网络提供了路径，实现了学习与普通编程离散结构的丰富交互。

Abstract: We don't program neural networks directly. Instead, we rely on an indirect style where learning algorithms, like gradient descent, determine a neural network's function by learning from data. This indirect style is often a virtue; it empowers us to solve problems that were previously impossible. But it lacks discrete structure. We can't compile most algorithms into a neural network -- even if these algorithms could help the network learn. This limitation occurs because discrete algorithms are not obviously differentiable, making them incompatible with the gradient-based learning algorithms that determine a neural network's function. To address this, we introduce $\textsf{Cajal}$: a typed, higher-order and linear programming language intended to be a minimal vehicle for exploring a direct style of programming neural networks. We prove $\textsf{Cajal}$ programs compile to linear neurons, allowing discrete algorithms to be expressed in a differentiable form compatible with gradient-based learning. With our implementation of $\textsf{Cajal}$, we conduct several experiments where we link these linear neurons against other neural networks to determine part of their function prior to learning. Linking with these neurons allows networks to learn faster, with greater data-efficiency, and in a way that's easier to debug. A key lesson is that linear programming languages provide a path towards directly programming neural networks, enabling a rich interplay between learning and the discrete structures of ordinary programming.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [3] [FlakyGuard: Automatically Fixing Flaky Tests at Industry Scale](https://arxiv.org/abs/2511.14002)
*Chengpeng Li,Farnaz Behrang,August Shi,Peng Liu*

Main category: cs.SE

TL;DR: FlakyGuard通过将代码视为图结构并使用选择性图探索来找到最相关的上下文，有效修复工业环境中的不稳定测试，修复成功率比现有方法至少提高22%。


<details>
  <summary>Details</summary>
Motivation: 现有方法如FlakyDoctor在工业环境中失败，因为上下文问题：提供太少上下文（缺少关键生产代码）或太多上下文（用无关信息淹没LLM）。

Method: 将代码视为图结构，使用选择性图探索找到最相关的上下文，为LLM提供精确的修复信息。

Result: 修复了47.6%的可重现不稳定测试，其中51.8%的修复被开发者接受；修复成功率比最先进方法至少提高22%；100%的开发者认为根因解释有用。

Conclusion: FlakyGuard通过选择性图探索有效解决了工业环境中不稳定测试修复的上下文问题，显著提升了修复成功率和开发者接受度。

Abstract: Flaky tests that non-deterministically pass or fail waste developer time and slow release cycles. While large language models (LLMs) show promise for automatically repairing flaky tests, existing approaches like FlakyDoctor fail in industrial settings due to the context problem: providing either too little context (missing critical production code) or too much context (overwhelming the LLM with irrelevant information). We present FlakyGuard, which addresses this problem by treating code as a graph structure and using selective graph exploration to find only the most relevant context. Evaluation on real-world flaky tests from industrial repositories shows that FlakyGuard repairs 47.6 % of reproducible flaky tests with 51.8 % of the fixes accepted by developers. Besides it outperforms state-of-the-art approaches by at least 22 % in repair success rate. Developer surveys confirm that 100 % find FlakyGuard's root cause explanations useful.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [Inside VOLT: Designing an Open-Source GPU Compiler](https://arxiv.org/abs/2511.13751)
*Shinnung Jeong,Chihyo Ahn,Huanzhi Pu,Jisheng Zhao,Hyesoon Kim,Blaise Tine*

Main category: cs.DC

TL;DR: VOLT是一个针对开源GPU的轻量级编译器工具链，支持SIMT代码生成和优化，通过分层设计适应多种前端语言和硬件架构。


<details>
  <summary>Details</summary>
Motivation: 开源GPU架构需要编译器框架来执行现有GPU程序并优化性能，但现有方案技术复杂且开发成本高。

Method: 采用分层设计，在中端集中处理SIMT相关分析和优化，支持多种前端语言和硬件架构，确保可扩展性。

Result: 通过ISA扩展和主机运行时API两个案例研究，证明VOLT能够支持扩展。

Conclusion: VOLT提供了一个可扩展的编译器框架，能够适应不断发展的开源GPU架构，降低开发成本。

Abstract: Recent efforts in open-source GPU research are opening new avenues in a domain that has long been tightly coupled with a few commercial vendors. Emerging open GPU architectures define SIMT functionality through their own ISAs, but executing existing GPU programs and optimizing performance on these ISAs relies on a compiler framework that is technically complex and often undercounted in open hardware development costs.
  To address this challenge, the Vortex-Optimized Lightweight Toolchain (VOLT) has been proposed. This paper presents its design principles, overall structure, and the key compiler transformations required to support SIMT execution on Vortex. VOLT enables SIMT code generation and optimization across multiple levels of abstraction through a hierarchical design that accommodates diverse front-end languages and open GPU hardware. To ensure extensibility as GPU architectures evolve, VOLT centralizes fundamental SIMT-related analyses and optimizations in the middle-end, allowing them to be reused across front-ends and easily adapted to emerging open-GPU variants. Through two case studies on ISA extensions and host-runtime API, this paper also demonstrates how VOLT can support extensions

</details>
