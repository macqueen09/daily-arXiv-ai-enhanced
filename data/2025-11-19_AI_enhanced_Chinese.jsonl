{"id": "2511.13751", "pdf": "https://arxiv.org/pdf/2511.13751", "abs": "https://arxiv.org/abs/2511.13751", "authors": ["Shinnung Jeong", "Chihyo Ahn", "Huanzhi Pu", "Jisheng Zhao", "Hyesoon Kim", "Blaise Tine"], "title": "Inside VOLT: Designing an Open-Source GPU Compiler", "categories": ["cs.DC", "cs.AR", "cs.PL"], "comment": "11 pages, 10 figures, two tables, two algorithms", "summary": "Recent efforts in open-source GPU research are opening new avenues in a domain that has long been tightly coupled with a few commercial vendors. Emerging open GPU architectures define SIMT functionality through their own ISAs, but executing existing GPU programs and optimizing performance on these ISAs relies on a compiler framework that is technically complex and often undercounted in open hardware development costs.\n  To address this challenge, the Vortex-Optimized Lightweight Toolchain (VOLT) has been proposed. This paper presents its design principles, overall structure, and the key compiler transformations required to support SIMT execution on Vortex. VOLT enables SIMT code generation and optimization across multiple levels of abstraction through a hierarchical design that accommodates diverse front-end languages and open GPU hardware. To ensure extensibility as GPU architectures evolve, VOLT centralizes fundamental SIMT-related analyses and optimizations in the middle-end, allowing them to be reused across front-ends and easily adapted to emerging open-GPU variants. Through two case studies on ISA extensions and host-runtime API, this paper also demonstrates how VOLT can support extensions", "AI": {"tldr": "VOLT\u662f\u4e00\u4e2a\u9488\u5bf9\u5f00\u6e90GPU\u7684\u8f7b\u91cf\u7ea7\u7f16\u8bd1\u5668\u5de5\u5177\u94fe\uff0c\u652f\u6301SIMT\u4ee3\u7801\u751f\u6210\u548c\u4f18\u5316\uff0c\u901a\u8fc7\u5206\u5c42\u8bbe\u8ba1\u9002\u5e94\u591a\u79cd\u524d\u7aef\u8bed\u8a00\u548c\u786c\u4ef6\u67b6\u6784\u3002", "motivation": "\u5f00\u6e90GPU\u67b6\u6784\u9700\u8981\u7f16\u8bd1\u5668\u6846\u67b6\u6765\u6267\u884c\u73b0\u6709GPU\u7a0b\u5e8f\u5e76\u4f18\u5316\u6027\u80fd\uff0c\u4f46\u73b0\u6709\u65b9\u6848\u6280\u672f\u590d\u6742\u4e14\u5f00\u53d1\u6210\u672c\u9ad8\u3002", "method": "\u91c7\u7528\u5206\u5c42\u8bbe\u8ba1\uff0c\u5728\u4e2d\u7aef\u96c6\u4e2d\u5904\u7406SIMT\u76f8\u5173\u5206\u6790\u548c\u4f18\u5316\uff0c\u652f\u6301\u591a\u79cd\u524d\u7aef\u8bed\u8a00\u548c\u786c\u4ef6\u67b6\u6784\uff0c\u786e\u4fdd\u53ef\u6269\u5c55\u6027\u3002", "result": "\u901a\u8fc7ISA\u6269\u5c55\u548c\u4e3b\u673a\u8fd0\u884c\u65f6API\u4e24\u4e2a\u6848\u4f8b\u7814\u7a76\uff0c\u8bc1\u660eVOLT\u80fd\u591f\u652f\u6301\u6269\u5c55\u3002", "conclusion": "VOLT\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u7f16\u8bd1\u5668\u6846\u67b6\uff0c\u80fd\u591f\u9002\u5e94\u4e0d\u65ad\u53d1\u5c55\u7684\u5f00\u6e90GPU\u67b6\u6784\uff0c\u964d\u4f4e\u5f00\u53d1\u6210\u672c\u3002"}}
{"id": "2511.13764", "pdf": "https://arxiv.org/pdf/2511.13764", "abs": "https://arxiv.org/abs/2511.13764", "authors": ["Arun Thangamani", "Md Asghar Ahmad Shahid", "Adam Siemieniuk", "Rolf Morel", "Renato Golin", "Alexander Heinecke"], "title": "Library Liberation: Competitive Performance Matmul Through Compiler-composed Nanokernels", "categories": ["cs.LG", "cs.PF", "cs.PL", "cs.SE"], "comment": null, "summary": "The rapidly evolving landscape of AI and machine learning workloads has widened the gap between high-level domain operations and efficient hardware utilization. Achieving near-peak performance still demands deep hardware expertise-experts either handcraft target-specific kernels (e.g., DeepSeek) or rely on specialized libraries (e.g., CUTLASS)-both of which add complexity and limit scalability for most ML practitioners.\n  This paper introduces a compilation scheme that automatically generates scalable, high-performance microkernels by leveraging the MLIR dialects to bridge domain-level operations and processor capabilities. Our approach removes dependence on low-level libraries by enabling the compiler to auto-generate near-optimal code directly. At its core is a mechanism for composing nanokernels from low-level IR constructs with near-optimal register utilization, forming efficient microkernels tailored to each target. We implement this technique in an MLIR-based compiler supporting both vector and tile based CPU instructions. Experiments show that the generated nanokernels are of production-quality, and competitive with state-of-the-art microkernel libraries.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMLIR\u7684\u7f16\u8bd1\u65b9\u6848\uff0c\u81ea\u52a8\u751f\u6210\u53ef\u6269\u5c55\u7684\u9ad8\u6027\u80fd\u5fae\u5185\u6838\uff0c\u65e0\u9700\u4f9d\u8d56\u4f4e\u5c42\u5e93\u5373\u53ef\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u7684\u4ee3\u7801\u751f\u6210\u3002", "motivation": "AI\u548c\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u8d1f\u8f7d\u5feb\u901f\u53d1\u5c55\uff0c\u5bfc\u81f4\u9ad8\u5c42\u9886\u57df\u64cd\u4f5c\u4e0e\u9ad8\u6548\u786c\u4ef6\u5229\u7528\u4e4b\u95f4\u5b58\u5728\u5dee\u8ddd\u3002\u5b9e\u73b0\u63a5\u8fd1\u5cf0\u503c\u6027\u80fd\u4ecd\u9700\u6df1\u5ea6\u786c\u4ef6\u4e13\u4e1a\u77e5\u8bc6\uff0c\u8fd9\u589e\u52a0\u4e86\u590d\u6742\u6027\u5e76\u9650\u5236\u4e86\u5927\u591a\u6570ML\u4ece\u4e1a\u8005\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "\u5229\u7528MLIR\u65b9\u8a00\u6865\u63a5\u9886\u57df\u7ea7\u64cd\u4f5c\u548c\u5904\u7406\u5668\u80fd\u529b\uff0c\u901a\u8fc7\u4ece\u4f4e\u5c42IR\u6784\u9020\u7ec4\u5408\u7eb3\u7c73\u5185\u6838\uff0c\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u7684\u5bc4\u5b58\u5668\u5229\u7528\u7387\uff0c\u5f62\u6210\u9488\u5bf9\u6bcf\u4e2a\u76ee\u6807\u7684\u9ad8\u6548\u5fae\u5185\u6838\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u751f\u6210\u7684\u7eb3\u7c73\u5185\u6838\u5177\u6709\u751f\u4ea7\u8d28\u91cf\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u5fae\u5185\u6838\u5e93\u5177\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "\u8be5\u7f16\u8bd1\u65b9\u6848\u80fd\u591f\u81ea\u52a8\u751f\u6210\u9ad8\u6027\u80fd\u5fae\u5185\u6838\uff0c\u51cf\u5c11\u5bf9\u4f4e\u5c42\u5e93\u7684\u4f9d\u8d56\uff0c\u4e3aML\u4ece\u4e1a\u8005\u63d0\u4f9b\u66f4\u6613\u7528\u7684\u9ad8\u6027\u80fd\u8ba1\u7b97\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13769", "pdf": "https://arxiv.org/pdf/2511.13769", "abs": "https://arxiv.org/abs/2511.13769", "authors": ["Joey Velez-Ginorio", "Nada Amin", "Konrad Kording", "Steve Zdancewic"], "title": "Compiling to linear neurons", "categories": ["cs.LG", "cs.PL"], "comment": null, "summary": "We don't program neural networks directly. Instead, we rely on an indirect style where learning algorithms, like gradient descent, determine a neural network's function by learning from data. This indirect style is often a virtue; it empowers us to solve problems that were previously impossible. But it lacks discrete structure. We can't compile most algorithms into a neural network -- even if these algorithms could help the network learn. This limitation occurs because discrete algorithms are not obviously differentiable, making them incompatible with the gradient-based learning algorithms that determine a neural network's function. To address this, we introduce $\\textsf{Cajal}$: a typed, higher-order and linear programming language intended to be a minimal vehicle for exploring a direct style of programming neural networks. We prove $\\textsf{Cajal}$ programs compile to linear neurons, allowing discrete algorithms to be expressed in a differentiable form compatible with gradient-based learning. With our implementation of $\\textsf{Cajal}$, we conduct several experiments where we link these linear neurons against other neural networks to determine part of their function prior to learning. Linking with these neurons allows networks to learn faster, with greater data-efficiency, and in a way that's easier to debug. A key lesson is that linear programming languages provide a path towards directly programming neural networks, enabling a rich interplay between learning and the discrete structures of ordinary programming.", "AI": {"tldr": "Cajal\u662f\u4e00\u4e2a\u7c7b\u578b\u5316\u3001\u9ad8\u9636\u7ebf\u6027\u7f16\u7a0b\u8bed\u8a00\uff0c\u7528\u4e8e\u76f4\u63a5\u7f16\u7a0b\u795e\u7ecf\u7f51\u7edc\uff0c\u5c06\u79bb\u6563\u7b97\u6cd5\u7f16\u8bd1\u4e3a\u7ebf\u6027\u795e\u7ecf\u5143\uff0c\u5b9e\u73b0\u4e0e\u57fa\u4e8e\u68af\u5ea6\u7684\u5b66\u4e60\u517c\u5bb9\u3002", "motivation": "\u5f53\u524d\u795e\u7ecf\u7f51\u7edc\u7f16\u7a0b\u4f9d\u8d56\u95f4\u63a5\u5b66\u4e60\u7b97\u6cd5\uff08\u5982\u68af\u5ea6\u4e0b\u964d\uff09\uff0c\u7f3a\u4e4f\u79bb\u6563\u7ed3\u6784\uff0c\u65e0\u6cd5\u5c06\u79bb\u6563\u7b97\u6cd5\u7f16\u8bd1\u5230\u7f51\u7edc\u4e2d\u3002\u9700\u8981\u4e00\u79cd\u76f4\u63a5\u7f16\u7a0b\u795e\u7ecf\u7f51\u7edc\u7684\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1Cajal\u7f16\u7a0b\u8bed\u8a00\uff0c\u8bc1\u660e\u5176\u7a0b\u5e8f\u53ef\u7f16\u8bd1\u4e3a\u7ebf\u6027\u795e\u7ecf\u5143\uff0c\u4f7f\u79bb\u6563\u7b97\u6cd5\u80fd\u4ee5\u53ef\u5fae\u5206\u5f62\u5f0f\u8868\u8fbe\uff0c\u5e76\u4e0e\u57fa\u4e8e\u68af\u5ea6\u7684\u5b66\u4e60\u517c\u5bb9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5c06Cajal\u751f\u6210\u7684\u7ebf\u6027\u795e\u7ecf\u5143\u4e0e\u5176\u4ed6\u795e\u7ecf\u7f51\u7edc\u94fe\u63a5\uff0c\u53ef\u5728\u5b66\u4e60\u524d\u786e\u5b9a\u90e8\u5206\u529f\u80fd\uff0c\u4f7f\u7f51\u7edc\u5b66\u4e60\u66f4\u5feb\u3001\u6570\u636e\u6548\u7387\u66f4\u9ad8\u3001\u66f4\u6613\u8c03\u8bd5\u3002", "conclusion": "\u7ebf\u6027\u7f16\u7a0b\u8bed\u8a00\u4e3a\u76f4\u63a5\u7f16\u7a0b\u795e\u7ecf\u7f51\u7edc\u63d0\u4f9b\u4e86\u8def\u5f84\uff0c\u5b9e\u73b0\u4e86\u5b66\u4e60\u4e0e\u666e\u901a\u7f16\u7a0b\u79bb\u6563\u7ed3\u6784\u7684\u4e30\u5bcc\u4ea4\u4e92\u3002"}}
{"id": "2511.14002", "pdf": "https://arxiv.org/pdf/2511.14002", "abs": "https://arxiv.org/abs/2511.14002", "authors": ["Chengpeng Li", "Farnaz Behrang", "August Shi", "Peng Liu"], "title": "FlakyGuard: Automatically Fixing Flaky Tests at Industry Scale", "categories": ["cs.SE", "cs.AI", "cs.LG", "cs.PL"], "comment": "To appear in ASE 2025", "summary": "Flaky tests that non-deterministically pass or fail waste developer time and slow release cycles. While large language models (LLMs) show promise for automatically repairing flaky tests, existing approaches like FlakyDoctor fail in industrial settings due to the context problem: providing either too little context (missing critical production code) or too much context (overwhelming the LLM with irrelevant information). We present FlakyGuard, which addresses this problem by treating code as a graph structure and using selective graph exploration to find only the most relevant context. Evaluation on real-world flaky tests from industrial repositories shows that FlakyGuard repairs 47.6 % of reproducible flaky tests with 51.8 % of the fixes accepted by developers. Besides it outperforms state-of-the-art approaches by at least 22 % in repair success rate. Developer surveys confirm that 100 % find FlakyGuard's root cause explanations useful.", "AI": {"tldr": "FlakyGuard\u901a\u8fc7\u5c06\u4ee3\u7801\u89c6\u4e3a\u56fe\u7ed3\u6784\u5e76\u4f7f\u7528\u9009\u62e9\u6027\u56fe\u63a2\u7d22\u6765\u627e\u5230\u6700\u76f8\u5173\u7684\u4e0a\u4e0b\u6587\uff0c\u6709\u6548\u4fee\u590d\u5de5\u4e1a\u73af\u5883\u4e2d\u7684\u4e0d\u7a33\u5b9a\u6d4b\u8bd5\uff0c\u4fee\u590d\u6210\u529f\u7387\u6bd4\u73b0\u6709\u65b9\u6cd5\u81f3\u5c11\u63d0\u9ad822%\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5982FlakyDoctor\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u5931\u8d25\uff0c\u56e0\u4e3a\u4e0a\u4e0b\u6587\u95ee\u9898\uff1a\u63d0\u4f9b\u592a\u5c11\u4e0a\u4e0b\u6587\uff08\u7f3a\u5c11\u5173\u952e\u751f\u4ea7\u4ee3\u7801\uff09\u6216\u592a\u591a\u4e0a\u4e0b\u6587\uff08\u7528\u65e0\u5173\u4fe1\u606f\u6df9\u6ca1LLM\uff09\u3002", "method": "\u5c06\u4ee3\u7801\u89c6\u4e3a\u56fe\u7ed3\u6784\uff0c\u4f7f\u7528\u9009\u62e9\u6027\u56fe\u63a2\u7d22\u627e\u5230\u6700\u76f8\u5173\u7684\u4e0a\u4e0b\u6587\uff0c\u4e3aLLM\u63d0\u4f9b\u7cbe\u786e\u7684\u4fee\u590d\u4fe1\u606f\u3002", "result": "\u4fee\u590d\u4e8647.6%\u7684\u53ef\u91cd\u73b0\u4e0d\u7a33\u5b9a\u6d4b\u8bd5\uff0c\u5176\u4e2d51.8%\u7684\u4fee\u590d\u88ab\u5f00\u53d1\u8005\u63a5\u53d7\uff1b\u4fee\u590d\u6210\u529f\u7387\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u81f3\u5c11\u63d0\u9ad822%\uff1b100%\u7684\u5f00\u53d1\u8005\u8ba4\u4e3a\u6839\u56e0\u89e3\u91ca\u6709\u7528\u3002", "conclusion": "FlakyGuard\u901a\u8fc7\u9009\u62e9\u6027\u56fe\u63a2\u7d22\u6709\u6548\u89e3\u51b3\u4e86\u5de5\u4e1a\u73af\u5883\u4e2d\u4e0d\u7a33\u5b9a\u6d4b\u8bd5\u4fee\u590d\u7684\u4e0a\u4e0b\u6587\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4fee\u590d\u6210\u529f\u7387\u548c\u5f00\u53d1\u8005\u63a5\u53d7\u5ea6\u3002"}}
