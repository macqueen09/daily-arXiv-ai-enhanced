<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 3]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Evaluate the Stack Management in Effect Handlers using the libseff C Library](https://arxiv.org/abs/2512.03083)
*ZeHao Yu*

Main category: cs.PL

TL;DR: 论文探讨了在libseff C库中使用用户级超额提交进行栈管理的新方法，通过虚拟内存机制和基于保护的延迟分配结合信号驱动的内存提交，动态调整栈大小以提高内存利用率。


<details>
  <summary>Details</summary>
Motivation: 效应处理器在现代编程中日益重要，用于管理并发、异步操作和异常处理等复杂计算效应，但其引入的动态控制流变化使得高效的栈管理成为一个重大挑战。

Method: 提出了一种基于用户级超额提交的栈管理方法，在libseff C库中实现，利用虚拟内存机制和基于保护的延迟分配，结合信号驱动的内存提交，动态按需调整栈大小。

Result: 实验结果表明，内核级超额提交在性能和灵活性之间取得了有效平衡，而用户级实现虽然灵活但带来了额外开销，突显了优化空间。研究对各种栈管理策略进行了详细比较分析。

Conclusion: 研究为特定应用需求和操作约束提供了实用的栈管理策略建议。未来工作将专注于改进用户级超额提交机制，减轻非确定性行为，并扩展基准测试框架以包含真实场景。

Abstract: Effect handlers are increasingly prominent in modern programming for managing complex computational effects, including concurrency, asynchronous operations, and exception handling, in a modular and flexible manner. Efficient stack management remains a significant challenge for effect handlers due to the dynamic control flow changes they introduce. This paper explores a novel stack management approach using user-level overcommitting within the libseff C library, which leverages virtual memory mechanisms and protection-based lazy allocation combined with signal-driven memory commitment. Our user-level overcommitting implementation dynamically resizes stacks on-demand, improving memory utilization and reducing waste compared to traditional methods. We rigorously benchmark and evaluate this novel strategy against conventional fixed- size stacks, segmented stacks, and kernel-based overcommitting, using metrics such as context-switch latency, stack expansion efficiency, multi-threaded performance, and robustness under rapid stack growth conditions. Experimental results demonstrate that kernel-based overcommitting achieves an effective balance between performance and flexibility, whereas our user-level implementation, while flexible, incurs additional overheads, highlighting areas for optimization. This study provides a detailed comparative analysis of various stack management strate- gies, offering practical recommendations tailored to specific application requirements and operational constraints. Future work will focus on refining user-level overcommit- ting mechanisms, mitigating non-deterministic behaviors, and expanding benchmark frameworks to include real-world scenarios.

</details>


### [2] [Beyond Code Pairs: Dialogue-Based Data Generation for LLM Code Translation](https://arxiv.org/abs/2512.03086)
*Le Chen,Nuo Xu,Winson Chen,Bin Lei,Pei-Hung Lin,Dunzhi Zhou,Rajeev Thakur,Caiwen Ding,Ali Jannesari,Chunhua Liao*

Main category: cs.PL

TL;DR: 提出自动化数据集生成流水线，通过双LLM问答设计结合编译器反馈，为低资源编程语言翻译（如Fortran→C++、C++→CUDA）生成带单元测试验证和多轮对话的数据集，显著提升翻译功能正确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码翻译方面表现优异，但在Fortran、CUDA等低资源编程领域性能下降，主要原因是高质量并行数据稀缺。需要解决低资源编程语言翻译的数据不足问题。

Method: 设计自动化数据集生成流水线，采用双LLM问答者-解决者架构，结合编译器和运行时反馈。除了传统源-目标代码对，还生成：(1)带单元测试的验证翻译以确保功能一致性；(2)多轮对话捕捉翻译精炼的推理过程。应用于Fortran→C++和C++→CUDA翻译。

Result: 生成3.64k个Fortran→C++对话和3.93k个C++→CUDA对话。微调后功能正确性显著提升，在C++→CUDA任务中单元测试成功率提高超过56%。7B开源模型在编译成功率等关键指标上显著优于更大的专有系统。

Conclusion: 提出的自动化数据集生成方法有效解决了低资源编程语言翻译的数据稀缺问题，通过生成带验证和多轮推理的数据，显著提升了翻译模型的功能正确性和编译成功率，使小型开源模型能够超越大型专有系统。

Abstract: Large language models (LLMs) have shown remarkable capabilities in code translation, yet their performance deteriorates in low-resource programming domains such as Fortran and emerging frameworks like CUDA, where high-quality parallel data are scarce. We present an automated dataset generation pipeline featuring a dual-LLM Questioner-Solver design that incorporates external knowledge from compilers and runtime feedback. Beyond traditional source-target code pair datasets, our approach additionally generates (1) verified translations with unit tests for assessing functional consistency, and (2) multi-turn dialogues that capture the reasoning process behind translation refinement. Applied to Fortran -> C++ and C++ -> CUDA, the pipeline yields 3.64k and 3.93k dialogues, respectively. Fine-tuning on this data yields dramatic improvements in functional correctness, boosting unit test success rates by over 56% on the challenging C++-to-CUDA task. We show this data enables a 7B open-weight model to significantly outperform larger proprietary systems on key metrics like compilation success.

</details>


### [3] [OOPredictor: Predicting Object-Oriented Accesses using Static Analysis](https://arxiv.org/abs/2512.03972)
*Hassan Arafat,David Bremner,Kenneth B. Kent,Julian Wang*

Main category: cs.PL

TL;DR: 使用编译时静态分析预测Java程序运行时访问模式，通过马尔可夫链建模程序行为，指导垃圾回收器优化内存布局以改善缓存性能


<details>
  <summary>Details</summary>
Motivation: 面向对象编程中的指针追逐导致缓存局部性差，现代硬件预取器难以处理这种不可预测的访问模式，现有软件方法需要运行时分析带来显著开销

Method: 在OpenJ9 JVM的OMR优化器基础设施中实现编译时静态分析器，预测程序最常见的访问模式，输出马尔可夫链模型程序预期行为

Result: 预测器表现出良好准确性，可通过与插桩解释器测量的实际运行时行为对比验证，可用于指导最小侵入性的负载停顿缓解策略

Conclusion: 静态分析预测器能有效建模程序访问模式，为垃圾回收器提供更友好的局部性复制顺序，改善面向对象程序的缓存性能

Abstract: Object-oriented Programming has become one of the most dominant design paradigms as the separation of concerns and adaptability of design reduce development and maintenance costs. However, the convenience is not without cost. The added indirection inherent in such designs causes excessive pointer chasing, negatively affecting locality, which in turn degrades the performance of cache structures. Furthermore, modern hardware prefetchers are mostly stride prefetchers that are ill-equipped to handle the unpredictability of access patterns generated by pointer chasing. Most software approaches that seek to address this problem resort to profiling the program as it runs, which comes with a significant run-time overhead or requires data from previous runs. In this paper, we propose the use of compile-time static analysis to predict the most common access patterns displayed by a program during run time. Since Java is one of the most popular object-oriented languages, we implement our prototype within the OpenJ9 JVM, inside the OMR optimizer infrastructure. The outputs of our proposed predictor are Markov chains that model the expected behavior of the program. The effectiveness of the proposed predictor is evaluated by comparing the model with the actual run-time behavior of the program measured using an instrumented interpreter. Our experiments show that the proposed predictor exhibits good accuracy and can be used to inform minimally intrusive load stall mitigation strategies, e.g. informing copying GCs on more locality-friendly copying orders

</details>
