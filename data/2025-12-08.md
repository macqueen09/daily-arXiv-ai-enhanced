<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 4]
- [cs.SE](#cs.SE) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [NVLang: Unified Static Typing for Actor-Based Concurrency on the BEAM](https://arxiv.org/abs/2512.05224)
*Miguel de Oliveira Guerreiro*

Main category: cs.PL

TL;DR: NVLang 是一种静态类型函数式语言，为BEAM虚拟机提供全面的类型安全，通过代数数据类型编码actor消息协议，在编译时强制执行协议一致性，消除消息传递错误。


<details>
  <summary>Details</summary>
Motivation: Erlang/OTP等基于actor的系统虽然具有高可靠性和并发处理能力，但缺乏对消息协议的静态保证。当前系统通过运行时模式匹配处理消息，将协议违规推迟到生产环境中才被发现，这可能导致严重的生产故障。

Method: NVLang使用代数数据类型(ADTs)编码actor消息协议：每个actor声明表示其消息词汇表的和类型，类型系统在编译时强制执行协议一致性。引入类型化进程标识符(Pid[T])编码actor期望的协议，以及类型化future(Future[T])提供类型安全的请求-回复模式。扩展Hindley-Milner类型推断来跟踪消息协议，编译到Core Erlang以实现与现有Erlang生态系统的互操作性。

Result: NVLang消除了整个类别的消息传递错误，同时保持了与动态类型替代方案相媲美的简洁语法。形式化了类型系统并提供了类型健全性的证明草图，证明类型良好的NVLang程序不能发送违反actor协议的消息。

Conclusion: NVLang成功地将全面的类型安全引入BEAM虚拟机，同时保留了actor模型的简单性和强大功能。通过静态类型系统确保消息协议的正确性，为关键基础设施提供了更强的可靠性保证，同时保持了与现有Erlang生态系统的无缝互操作性。

Abstract: Actor-based systems like Erlang/OTP power critical infrastructure -- from telecommunications to messaging platforms -- handling millions of concurrent connections with legendary reliability. Yet these systems lack static guarantees about message protocols: processes communicate by sending arbitrary messages that pattern-matched at runtime, deferring protocol violations to production failures.
  We present NVLang, a statically typed functional language that brings comprehensive type safety to the BEAM virtual machine while preserving actor model's simplicity and power. NVLang's central contribution that algebraic data types (ADTs) naturally encode actor message protocols: each actor declares the sum type representing its message vocabulary, and the type system enforces protocol conformance at compile time. We introduce typed process identifiers (Pid[T]) that encode the protocol an actor expects, and typed futures (Future[T]) that provide type-safe request-reply patterns.
  By extending Hindley-Milner type inference to track message protocols, NVLang eliminates an entire class of message-passing errors while maintaining clean syntax that rivals dynamically typed alternatives. Our implementation compiles to Core Erlang, enabling seamless interoperability with the existing Erlang ecosystem. We formalize the type system and provide proof sketches for type soundness, demonstrating that well-typed NVLang programs cannot send messages that violate actor protocols.

</details>


### [2] [Verified VCG and Verified Compiler for Dafny](https://arxiv.org/abs/2512.05262)
*Daniel Nezamabadi,Magnus O. Myreen,Yong Kiam Tan*

Main category: cs.PL

TL;DR: 为Dafny语言开发了经过验证的验证条件生成器和编译器，确保从源代码到机器代码的完整正确性证明


<details>
  <summary>Details</summary>
Motivation: Dafny语言现有的编译器和验证器存在正确性问题，需要建立具有基础性正确保证的开发工具链

Method: 为Dafny子集建立函数式大步语义，基于此开发经过验证的验证条件生成器和编译器，并在HOL4定理证明器中实现

Result: 成功构建了从Dafny程序到CakeML再到机器代码的完整验证工具链，能够处理包括递归方法、循环和数组在内的复杂语言特性

Conclusion: 通过形式化验证方法，为Dafny工具链提供了基础性正确保证，实现了从源代码到可执行代码的完整正确性证明

Abstract: Dafny is a verification-aware programming language that comes with a compiler and static program verifier. However, neither the compiler nor the verifier is proved correct; in fact, soundness bugs have been found in both tools. This paper shows that the aforementioned Dafny tools can be developed with foundational correctness guarantees. We present a functional big-step semantics for an imperative subset of Dafny and, based on this semantics, a verified verification condition generator (VCG) and a verified compiler for Dafny. The subset of Dafny we have formalized includes mutually recursive method calls, while loops, and arrays -- these language features are significant enough to cover challenging examples such as McCarthy's 91 function and array-based programs that are used when teaching Dafny. The verified VCG allows one to prove functional correctness of annotated Dafny programs, while the verified compiler can be used to compile verified Dafny programs to CakeML programs. From there, one can obtain executable machine code via the (already verified) CakeML compiler, all while provably maintaining the functional correctness guarantees that were proved for the source-level Dafny programs. Our work has been mechanized in the HOL4 theorem prover.

</details>


### [3] [Compiler-supported reduced precision and AoS-SoA transformations for heterogeneous hardware](https://arxiv.org/abs/2512.05516)
*Pawel K. Radtke,Tobias Weinzierl*

Main category: cs.PL

TL;DR: 评估AoS到SoA转换在GPU平台上对粒子模拟代码的性能影响，结合精度降低技术，通过编译器注解实现转换控制


<details>
  <summary>Details</summary>
Motivation: 研究在GPU平台上对粒子模拟代码进行AoS到SoA转换的性能优化，同时考虑精度降低技术。主要动机是探索在CPU和GPU之间如何最佳地组织数据转换，特别是在现代超级芯片上CPU和GPU共享数据空间的情况下。

Method: 引入编译器注解来促进AoS到SoA的转换，并让程序员能够结合GPU卸载来编排这些转换。评估了在不同GPU平台（Nvidia G200和AMD MI300A）上对粒子模拟代码的性能影响。

Result: Nvidia G200平台获得了约2.6倍的加速，而AMD MI300A表现出更稳健的性能但收益较少。编译器基础的技术被认为适用于广泛的拉格朗日代码及其他应用。

Conclusion: AoS到SoA转换结合精度降低技术能够显著提升GPU上粒子模拟代码的性能，编译器注解提供了灵活的控制机制。不同GPU平台对转换策略的响应不同，需要针对具体硬件进行优化。

Abstract: This study evaluates AoS-to-SoA transformations over reduced-precision data layouts for a particle simulation code on several GPU platforms: We hypothesize that SoA fits particularly well to SIMT, while AoS is the preferred storage format for many Lagrangian codes. Reduced-precision (below IEEE accuracy) is an established tool to address bandwidth constraints, although it remains unclear whether AoS and precision conversions should execute on a CPU or be deployed to a GPU if the compute kernel itself must run on an accelerator. On modern superchips where CPUs and GPUs share (logically) one data space, it is also unclear whether it is advantageous to stream data to the accelerator prior to the calculation, or whether we should let the accelerator transform data on demand, i.e.~work in-place logically. We therefore introduce compiler annotations to facilitate such conversions and to give the programmer the option to orchestrate the conversions in combination with GPU offloading. For some of our compute kernels of interest, Nvidia's G200 platforms yield a speedup of around 2.6 while AMD's MI300A exhibits more robust performance yet profits less. We assume that our compiler-based techniques are applicable to a wide variety of Lagrangian codes and beyond.

</details>


### [4] [Compiling Away the Overhead of Race Detection](https://arxiv.org/abs/2512.05555)
*Alexey Paznikov,Andrey Kogutenko,Yaroslav Osipov,Michael Schwarz,Umang Mathur*

Main category: cs.PL

TL;DR: 通过静态分析消除动态数据竞争检测器中的冗余检测，减少运行时开销，实现1.34倍几何平均加速


<details>
  <summary>Details</summary>
Motivation: 动态数据竞争检测器因对内存访问的广泛插桩导致高运行时开销，限制了其实际应用。许多插桩是冗余的，需要优化。

Method: 提出一套过程间静态分析方法：1) 分析内存访问模式、同步和线程创建，消除可证明无竞争的访问插桩；2) 基于支配关系的消除分析，识别并消除报告等价竞争的冗余检查。在LLVM中实现5种静态分析，与ThreadSanitizer集成。

Result: 在真实应用测试中，显著降低竞争检测开销，实现1.34倍几何平均加速，峰值加速达2.5倍（高线程争用下）。编译时间增加可忽略，已被ThreadSanitizer维护者接受并正在上游化。

Conclusion: 通过静态分析消除冗余插桩能有效降低动态数据竞争检测器的运行时开销，提升性能而不增加开发者负担，具有实际应用价值。

Abstract: Dynamic data race detectors are indispensable for flagging concurrency errors in software, but their high runtime overhead limits their adoption. This overhead stems primarily from pervasive instrumentation of memory accesses - a significant fraction of which is redundant. We addresses this inefficiency through a static, compiler-integrated approach that identifies and eliminates redundant instrumentation, drastically reducing the runtime cost of dynamic data race detectors. We introduce a suite of interprocedural static analyses reasoning about memory access patterns, synchronization, and thread creation to eliminate instrumentation for provably race-free accesses and show that the completeness properties of the data race detector are preserved. We further observe that many inserted checks flag a race if and only if a preceding check has already flagged an equivalent race for the same memory location - albeit potentially at a different access. We characterize this notion of equivalence and show that, when limiting reporting to at least one representative for each equivalence class, a further class of redundant checks can be eliminated. We identify such accesses using a novel dominance-based elimination analysis. Based on these two insights, we have implemented five static analyses within the LLVM, integrated with the instrumentation pass of the race detector ThreadSanitizer. Our experimental evaluation on a diverse suite of real-world applications demonstrates that our approach significantly reduces race detection overhead, achieving a geomean speedup of 1.34x, with peak speedups reaching 2.5x under high thread contention. This performance is achieved with a negligible increase in compilation time and, being fully automatic, places no additional burden on developers. Our optimizations have been accepted by the ThreadSanitizer maintainers and are in the process of being upstreamed.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [5] [Stellis: A Strategy Language for Purifying Separation Logic Entailments](https://arxiv.org/abs/2512.05159)
*Zhiyi Wang,Xiwei Wu,Yi Fang,Chengtao Li,Hongyi Zhong,Lihan Xie,Qinxiang Cao,Zhenjiang Hu*

Main category: cs.SE

TL;DR: Stellis：一种用于纯化分离逻辑蕴含的策略语言，通过移除空间公式简化验证，支持灵活的策略编码和自动正确性证明。


<details>
  <summary>Details</summary>
Motivation: 自动证明分离逻辑蕴含是验证中的核心挑战。基于规则的方法依赖分离逻辑规则进行自动化，但这些规则语句不足以描述自动化策略，特别是在特定场景中对齐和消除内存布局的策略。

Method: 提出Stellis策略语言，具有强大的匹配机制和灵活的动作描述，能够直接编码多种策略。引入算法为每个策略生成正确性条件，将策略的正确性简化为其正确性条件的验证。基于机械化的归约正确性定理，原型实现生成整体自动化的正确性证明。

Result: 在包含229个蕴含的基准测试中（来自标准链表数据结构和微内核内存模块的验证），使用5个库和98个策略，系统自动纯化了95.6%（219/229）的蕴含。

Conclusion: Stellis在提供灵活性和便利性的同时，也保持了高效性，能够自动纯化绝大多数分离逻辑蕴含，并通过正确性条件确保策略的可靠性。

Abstract: Automatically proving separation logic entailments is a fundamental challenge in verification. While rule-based methods rely on separation logic rules (lemmas) for automation, these rule statements are insufficient for describing automation strategies, which usually involve the alignment and elimination of corresponding memory layouts in specific scenarios. To overcome this limitation, we propose Stellis, a strategy language for purifying separation logic entailments, i.e., removing all spatial formulas to reduce the entailment to a simpler pure entailment. Stellis features a powerful matching mechanism and a flexible action description, enabling the straightforward encoding of a wide range of strategies. To ensure strategy soundness, we introduce an algorithm that generates a soundness condition for each strategy, thereby reducing the soundness of each strategy to the correctness of its soundness condition. Furthermore, based on a mechanized reduction soundness theorem, our prototype implementation generates correctness proofs for the overall automation. We evaluate our system on a benchmark of 229 entailments collected from verification of standard linked data structures and the memory module of a microkernel, and the evaluation results demonstrate that, with such flexibility and convenience provided, our system is also highly effective, which automatically purifies 95.6% (219 out of 229) of the entailments using 5 libraries with 98 strategies.

</details>


### [6] [Bootstrapping Fuzzers for Compilers of Low-Resource Language Dialects Using Language Models](https://arxiv.org/abs/2512.05887)
*Sairam Vaidya,Marcel Böhme,Loris D'Antoni*

Main category: cs.SE

TL;DR: Germinator：基于语法和覆盖引导的模糊测试工具，用于可扩展编译器，自动从方言规范提取语法，结合LLM生成种子输入，提高测试覆盖率和发现bug


<details>
  <summary>Details</summary>
Motivation: 可扩展编译器框架（如MLIR）虽然支持快速创建领域特定语言方言，但扩展性也增加了确保正确性的难度。现有测试生成方法要么需要为每个方言手动构建种子语料库，要么无法有效针对方言特定特性发现bug

Method: 提出方言无关且方言有效的语法基覆盖引导模糊测试方法：1）从方言规范自动提取语法；2）结合预训练大语言模型自动生成代表性多样化种子输入；3）用这些种子引导覆盖引导模糊测试器

Result: 在6个MLIR项目的91个方言上评估，Germinator生成的种子相比语法基线提高10-120%行覆盖率，发现88个未知bug（40个已确认），其中23个在之前没有自动化测试生成器的方言中

Conclusion: Germinator实现了对可扩展编译器的高效可控测试，特别是对低资源方言的大规模测试，通过自动语法提取和LLM辅助种子生成，解决了方言无关性和有效性的平衡问题

Abstract: Modern extensible compiler frameworks-such as MLIR-enable rapid creation of domain-specific language dialects. This flexibility, however, makes correctness harder to ensure as the same extensibility that accelerates development also complicates maintaining the testing infrastructure. Extensible languages require automated test generation that is both dialect-agnostic (works across dialects without manual adaptation) and dialect-effective (targets dialect-specific features to find bugs). Existing approaches typically sacrifice one of these goals by either requiring manually constructed seed corpora for each dialect, or by failing to be effective. We present a dialect-agnostic and dialect-effective grammar-based and coverage-guided fuzzing approach for extensible compilers that combines two key insights from existing work: (i) the grammars of dialects, which already encode the structural and type constraints, can often be extracted automatically from the dialect specification; and (ii) these grammars can be used in combination with pre-trained large language models to automatically generate representative and diverse seed inputs from the full dialect space without requiring any manual input or training data. These seeds can then be used to bootstrap coverage-guided fuzzers. We built this approach into a tool, Germinator. When evaluated on six MLIR projects spanning 91 dialects, Germinator generated seeds improve line coverage by 10-120% over grammar-based baselines. We compare against grammar-based baselines because they are the only class of existing automatic seed generators that can be applied uniformly across MLIR's heterogeneous dialect ecosystem. Germinator discovers 88 previously unknown bugs (40 confirmed), including 23 in dialects with no prior automated test generators, demonstrating effective and controllable testing of low-resource dialects at scale.

</details>
