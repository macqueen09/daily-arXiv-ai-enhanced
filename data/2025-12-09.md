<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 3]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.SE](#cs.SE) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Nice to Meet You: Synthesizing Practical MLIR Abstract Transformers](https://arxiv.org/abs/2512.06442)
*Xuanyu Peng,Dominic Kennedy,Yuyou Fan,Ben Greenman,John Regehr,Loris D'Antoni*

Main category: cs.PL

TL;DR: NiceToMeetYou是一个用于抽象转换器的程序合成框架，专门针对现代生产编译器广泛使用的非关系整数抽象域，通过将合成问题分解为多个简单转换器的meet操作，实现无需草图的批量自动化合成。


<details>
  <summary>Details</summary>
Motivation: 静态分析在编译中至关重要，但实现正确、精确且高效的抽象转换器具有挑战性。LLVM和GCC都曾因不正确的抽象转换器导致错误编译，且LLVM经过20多年发展仍缺少数百条中间表示指令的抽象转换器。

Method: 采用程序合成框架，通过将每个转换器分解为多个简单、正确的转换器的meet操作，每个新部分填补最终转换器的精度空白。设计特点是批量自动化，无需草图，通过降低到先前创建的MLIR的SMT方言来验证转换器。

Result: 所有合成的转换器都被证明是正确的，其中17%比LLVM提供的转换器更精确。框架成功解决了抽象转换器实现中的挑战。

Conclusion: NiceToMeetYou框架为编译器抽象转换器的合成提供了一种有效方法，能够自动生成正确且精确的转换器，填补了现有编译器工具链中的重要空白。

Abstract: Static analyses play a fundamental role during compilation: they discover facts that are true in all executions of the code being compiled, and then these facts are used to justify optimizations and diagnostics. Each static analysis is based on a collection of abstract transformers that provide abstract semantics for the concrete instructions that make up a program. It can be challenging to implement abstract transformers that are sound, precise, and efficient, and in fact both LLVM and GCC have suffered from miscompilations caused by unsound abstract transformers. Moreover, even after more than 20 years of development, LLVM lacks abstract transformers for hundreds of instructions in its intermediate representation (IR). We developed NiceToMeetYou, a program synthesis framework for abstract transformers that are aimed at the kinds of non-relational integer abstract domains that are heavily used by today's production compilers. It exploits a simple but novel technique for breaking the synthesis problem into parts: each of our transformers is the meet of a collection of simpler, sound transformers that are synthesized such that each new piece fills a gap in the precision of the final transformer. Our design point is bulk automation: no sketches are required. Transformers are verified by lowering to a previously created SMT dialect of MLIR. Each of our synthesized transformers is provably sound and some (17 percent) are more precise than those provided by LLVM.

</details>


### [2] [PIP: Making Andersen's Points-to Analysis Sound and Practical for Incomplete C Programs](https://arxiv.org/abs/2512.07299)
*Håvard Rognebakke Krogstie,Helge Bahmann,Magnus Själander,Nico Reissmann*

Main category: cs.PL

TL;DR: 提出一种针对不完整C程序的Andersen式指针分析，通过隐式指针目标跟踪实现高效且可靠的分析，比现有技术快15倍，内存可扩展，适合实际编译器优化。


<details>
  <summary>Details</summary>
Motivation: 现代编译器通常单独编译文件以实现并行化，但这导致编译器需要在程序不完整的情况下工作。现有的指针分析技术只对完整程序保证可靠性，需要摘要函数来描述缺失的程序部分，而生产编译器通常没有摘要函数，同时可靠性和效率又是必须的。

Method: 提出一种Andersen式指针分析，通过隐式跟踪可从外部模块访问的内存位置和指针来实现可靠性。在约束图中隐式执行指针目标跟踪，并提出Prefer Implicit Pointees (PIP)技术进一步减少显式指针目标的使用。

Result: 隐式指针目标跟踪使约束求解器比使用显式跟踪的五种最先进技术组合快15倍。PIP技术提供额外1.9倍加速。在别名分析客户端评估中，比LLVM的BasicAA单独使用减少40%的MayAlias响应。分析在内存方面具有可扩展性。

Conclusion: 该分析为不完整C程序提供高效可靠的指针分析解决方案，通过隐式指针目标跟踪和PIP技术显著提升性能，适合实际编译器优化应用。

Abstract: Compiling files individually lends itself well to parallelization, but forces the compiler to operate on incomplete programs. State-of-the-art points-to analyses guarantee sound solutions only for complete programs, requiring summary functions to describe any missing program parts. Summary functions are rarely available in production compilers, however, where soundness and efficiency are non-negotiable. This paper presents an Andersen-style points-to analysis that efficiently produces sound solutions for incomplete C programs. The analysis accomplishes soundness by tracking memory locations and pointers that are accessible from external modules, and efficiency by performing this tracking implicitly in the constraint graph. We show that implicit pointee tracking makes the constraint solver 15$\times$ faster than any combination of five different state-of-the-art techniques using explicit pointee tracking. We also present the Prefer Implicit Pointees (PIP) technique that further reduces the use of explicit pointees. PIP gives an additional speedup of 1.9$\times$, compared to the fastest solver configuration not benefiting from PIP. The precision of the analysis is evaluated in terms of an alias-analysis client, where it reduces the number of MayAlias-responses by 40% compared to LLVM's BasicAA pass alone. Finally, we show that the analysis is scalable in terms of memory, making it suitable for optimizing compilers in practice.

</details>


### [3] [Canonical bidirectional typechecking](https://arxiv.org/abs/2512.07511)
*Zanzi Mihejevs,Jules Hedges*

Main category: cs.PL

TL;DR: 论文展示了双向类型检查中的可检查/可综合划分与极化System L中的对偶性一致，建立了极化类型系统、LNL演算和双向演算之间的三方对应关系。


<details>
  <summary>Details</summary>
Motivation: 探索双向类型检查中的可检查/可综合划分与极化System L（极化μμ̃-演算）中现有对偶性之间的联系，建立不同类型系统之间的统一理论框架。

Method: 将标准双向类型检查公式与Zeilberger的"共上下文"变体结合，扩展到普通"笛卡尔"System L（使用McBride的共德布鲁因范围公式），并以线性-非线性风格组合两者，其中线性类型为正，笛卡尔类型为负。

Result: 证明了极化System L的移位、LNL演算和双向演算之间存在显著的三方一致性，具体表现为正项和负余项可检查，负项和正余项可综合。

Conclusion: 双向类型检查中的可检查/可综合划分与极化System L中的对偶性完全对应，这为理解不同类型系统之间的关系提供了新的理论视角，并展示了极化、线性和双向类型系统之间的深刻联系。

Abstract: We demonstrate that the checkable/synthesisable split in bidirectional typechecking coincides with existing dualities in polarised System L, also known as polarised $μ\tildeμ$-calculus. Specifically, positive terms and negative coterms are checkable, and negative terms and positive coterms are synthesisable. This combines a standard formulation of bidirectional typechecking with Zeilberger's `cocontextual' variant. We extend this to ordinary `cartesian' System L using Mc Bride's co-de Bruijn formulation of scopes, and show that both can be combined in a linear-nonlinear style, where linear types are positive and cartesian types are negative. This yields a remarkable 3-way coincidence between the shifts of polarised System L, LNL calculi, and bidirectional calculi.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [4] [Reasoning about concurrent loops and recursion with rely-guarantee rules](https://arxiv.org/abs/2512.06242)
*Ian J. Hayes,Larissa A. Meinicke,Cliff B. Jones*

Main category: cs.LO

TL;DR: 提出用于并发环境下递归程序和while循环的通用、机械验证的精化规则，不假设表达式求值是原子的，采用rely-guarantee方法进行组合式推理


<details>
  <summary>Details</summary>
Motivation: 在并发环境中，需要形式化方法来推理递归程序和循环，但现有方法通常假设表达式求值是原子的，这在实际并发系统中不成立。需要开发不依赖原子性假设的通用验证规则

Method: 采用rely-guarantee并发推理方法，将递归程序定义为命令格上的不动点，开发不动点推理定律，然后将循环定义为不动点，将递归定律应用于循环推理

Result: 开发了一套通用的、机械验证的精化规则，能够在非原子表达式求值的假设下，对并发环境中的递归程序和while循环进行形式化推理

Conclusion: 通过将递归和循环统一为不动点问题，并采用rely-guarantee方法，成功建立了适用于并发环境的通用验证框架，为并发程序的形式化验证提供了理论基础

Abstract: The objective of this paper is to present general, mechanically verified, refinement rules for reasoning about recursive programs and while loops in the context of concurrency. Unlike many approaches to concurrency, we do not assume that expression evaluation is atomic. We make use of the rely-guarantee approach to concurrency that facilitates reasoning about interference from concurrent threads in a compositional manner. Recursive programs can be defined as fixed points over a lattice of commands and hence we develop laws for reasoning about fixed points. Loops can be defined in terms of fixed points and hence the laws for recursion can be applied to develop laws for loops.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [5] [From Description to Score: Can LLMs Quantify Vulnerabilities?](https://arxiv.org/abs/2512.06781)
*Sima Jafarikhah,Daniel Thompson,Eva Deans,Hossein Siadati,Yi Liu*

Main category: cs.CR

TL;DR: 本研究探索使用通用大语言模型（ChatGPT、Llama、Grok、DeepSeek、Gemini）自动化漏洞评分，分析了31,000多个CVE条目，发现LLMs在某些指标上表现优异，但受限于CVE描述质量。


<details>
  <summary>Details</summary>
Motivation: 手动漏洞评分（如CVSS评分）是资源密集型过程且受主观影响，需要自动化解决方案来应对日益增长的CVE积压问题。

Method: 使用五种通用大语言模型（ChatGPT、Llama、Grok、DeepSeek、Gemini）分析超过31,000个近期CVE条目，评估它们在自动化CVSS评分方面的性能。

Result: LLMs在某些指标（如可用性影响）上显著优于基线，但在其他指标（如攻击复杂度）上提升有限；ChatGPT-5获得最高精度；模型倾向于错误分类相同的CVE；集成元分类器仅略微提升性能；CVE描述缺乏关键上下文或存在模糊表述是主要问题。

Conclusion: 需要改进漏洞描述质量并纳入更丰富的上下文细节，以支持更可靠的自动化推理，减轻CVE积压压力。

Abstract: Manual vulnerability scoring, such as assigning Common Vulnerability Scoring System (CVSS) scores, is a resource-intensive process that is often influenced by subjective interpretation. This study investigates the potential of general-purpose large language models (LLMs), namely ChatGPT, Llama, Grok, DeepSeek, and Gemini, to automate this process by analyzing over 31{,}000 recent Common Vulnerabilities and Exposures (CVE) entries. The results show that LLMs substantially outperform the baseline on certain metrics (e.g., \textit{Availability Impact}), while offering more modest gains on others (e.g., \textit{Attack Complexity}). Moreover, model performance varies across both LLM families and individual CVSS metrics, with ChatGPT-5 attaining the highest precision. Our analysis reveals that LLMs tend to misclassify many of the same CVEs, and ensemble-based meta-classifiers only marginally improve performance. Further examination shows that CVE descriptions often lack critical context or contain ambiguous phrasing, which contributes to systematic misclassifications. These findings underscore the importance of enhancing vulnerability descriptions and incorporating richer contextual details to support more reliable automated reasoning and alleviate the growing backlog of CVEs awaiting triage.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [6] [Leveraging LLMs to support co-evolution between definitions and instances of textual DSLs](https://arxiv.org/abs/2512.06836)
*Weixing Zhang,Regina Hebig,Daniel Strüber*

Main category: cs.SE

TL;DR: 研究探索使用大型语言模型（LLM）实现文本DSL语法与实例的协同演化，重点评估其在处理文本实例时保留注释和格式等辅助信息的能力。


<details>
  <summary>Details</summary>
Motivation: 文本DSL语法演化时，现有MDE技术无法有效处理文本实例的协同演化，容易丢失注释和布局等对软件理解和维护有价值的信息。

Method: 应用Claude-3.5和GPT-4o两种先进语言模型，在七个案例语言上进行实验，评估LLM在直接处理文本实例时的可行性和局限性。

Result: LLM在小规模、有限实例大小的案例中表现出良好的迁移能力，但在扩展到更大实例时面临显著的可扩展性挑战。

Conclusion: LLM在文本DSL协同演化方面具有潜力，特别是在小规模场景中，但需要解决可扩展性问题，这为未来研究提供了重要见解。

Abstract: Software languages evolve over time for various reasons, such as the addition of new features. When the language's grammar definition evolves, textual instances that originally conformed to the grammar become outdated. For DSLs in a model-driven engineering context, there exists a plethora of techniques to co-evolve models with the evolving metamodel. However, these techniques are not geared to support DSLs with a textual syntax -- applying them to textual language definitions and instances may lead to the loss of information from the original instances, such as comments and layout information, which are valuable for software comprehension and maintenance. This study explores the potential of Large Language Model (LLM)-based solutions in achieving grammar and instance co-evolution, with attention to their ability to preserve auxiliary information when directly processing textual instances. By applying two advanced language models, Claude-3.5 and GPT-4o, and conducting experiments across seven case languages, we evaluated the feasibility and limitations of this approach. Our results indicate a good ability of the considered LLMs for migrating textual instances in small-scale cases with limited instance size, which are representative of a subset of cases encountered in practice. In addition, we observe significant challenges with the scalability of LLM-based solutions to larger instances, leading to insights that are useful for informing future research.

</details>
