<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 4]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [LOOPRAG: Enhancing Loop Transformation Optimization with Retrieval-Augmented Large Language Models](https://arxiv.org/abs/2512.15766)
*Yijie Zhi,Yayu Cao,Jianhua Dai,Xiaoyang Han,Jingwen Pu,Qingran Wu,Sheng Cheng,Ming Cai*

Main category: cs.PL

TL;DR: LOOPRAG：基于检索增强生成的循环优化框架，通过参数驱动方法生成多样化合法示例，结合循环感知检索算法和反馈迭代机制，显著提升LLMs在循环变换优化中的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管循环变换是广泛使用的语义保持优化技术，但找到最优的变换组合仍然具有挑战性。现有大型语言模型在循环优化中经常产生错误或次优结果，错失性能提升机会。

Method: 提出LOOPRAG框架：1）参数驱动方法利用循环属性触发各种变换，生成多样化合法示例；2）基于循环特征的感知检索算法平衡相似性和多样性；3）反馈迭代机制结合编译、测试和性能结果指导LLMs；4）通过变异、覆盖和差分测试进行等价性检查。

Result: 在PolyBench、TSVC和LORE基准测试中，相比基础编译器分别获得最高11.20×、14.34×和9.29×的加速比，相比基础LLMs分别获得最高11.97×、5.61×和11.59×的加速比。

Conclusion: LOOPRAG框架有效解决了LLMs在循环优化中的局限性，通过检索增强生成和反馈迭代机制显著提升了循环变换优化的效果和可靠性。

Abstract: Loop transformations are semantics-preserving optimization techniques, widely used to maximize objectives such as parallelism. Despite decades of research, applying the optimal composition of loop transformations remains challenging due to inherent complexities, including cost modeling for optimization objectives. Recent studies have explored the potential of Large Language Models (LLMs) for code optimization. However, our key observation is that LLMs often struggle with effective loop transformation optimization, frequently leading to errors or suboptimal optimization, thereby missing opportunities for performance improvements. To bridge this gap, we propose LOOPRAG, a novel retrieval-augmented generation framework designed to guide LLMs in performing effective loop optimization on Static Control Part. We introduce a parameter-driven method to harness loop properties, which trigger various loop transformations, and generate diverse yet legal example codes serving as a demonstration source. To effectively obtain the most informative demonstrations, we propose a loop-aware algorithm based on loop features, which balances similarity and diversity for code retrieval. To enhance correct and efficient code generation, we introduce a feedback-based iterative mechanism that incorporates compilation, testing and performance results as feedback to guide LLMs. Each optimized code undergoes mutation, coverage and differential testing for equivalence checking. We evaluate LOOPRAG on PolyBench, TSVC and LORE benchmark suites, and compare it against compilers (GCC-Graphite, Clang-Polly, Perspective and ICX) and representative LLMs (DeepSeek and GPT-4). The results demonstrate average speedups over base compilers of up to 11.20$\times$, 14.34$\times$, and 9.29$\times$ for PolyBench, TSVC, and LORE, respectively, and speedups over base LLMs of up to 11.97$\times$, 5.61$\times$, and 11.59$\times$.

</details>


### [2] [Automated Formalization of Probabilistic Requirements from Structured Natural Language](https://arxiv.org/abs/2512.15788)
*Anastasia Mavridou,Marie Farrell,Gricel Vázquez,Tom Pressburger,Timothy E. Wang,Radu Calinescu,Michael Fisher*

Main category: cs.PL

TL;DR: 扩展NASA的FRET工具，支持用结构化自然语言编写概率需求，并自动转换为概率时序逻辑公式，使自主自适应系统的形式化分析更实用


<details>
  <summary>Details</summary>
Motivation: 自主自适应系统开发面临重大挑战，特别是在安全和任务关键系统中，需要明确捕捉环境或决策过程中的不确定性。现有方法要求开发者直接使用复杂的概率逻辑形式化语言编写需求，这既不现实又容易出错

Method: 扩展NASA的FRET工具的结构化自然语言，支持概率需求的规范；开发形式化、组合式、自动化的方法，将结构化自然语言需求转换为概率时序逻辑公式；建立自动化验证框架和形式化证明来确保生成公式的正确性

Result: 开发了扩展的FRET工具，使开发者能够用结构化自然语言指定概率需求，并自动将其转换为概率时序逻辑公式，提高了自主自适应系统形式化分析的实用性和可靠性

Conclusion: 通过扩展FRET工具支持概率需求规范，并开发自动化转换和验证方法，显著降低了自主自适应系统形式化分析的复杂性和错误率，使这类系统的开发更加实用和可靠

Abstract: Integrating autonomous and adaptive behavior into software-intensive systems presents significant challenges for software development, as uncertainties in the environment or decision-making processes must be explicitly captured. These challenges are amplified in safety- and mission-critical systems, which must undergo rigorous scrutiny during design and development. Key among these challenges is the difficulty of specifying requirements that use probabilistic constructs to capture the uncertainty affecting these systems. To enable formal analysis, such requirements must be expressed in precise mathematical notations such as probabilistic logics. However, expecting developers to write requirements directly in complex formalisms is unrealistic and highly error-prone. We extend the structured natural language used by NASA's Formal Requirement Elicitation Tool (FRET) with support for the specification of unambiguous and correct probabilistic requirements, and develop an automated approach for translating these requirements into logical formulas. We propose and develop a formal, compositional, and automated approach for translating structured natural-language requirements into formulas in probabilistic temporal logic. To increase trust in our formalizations, we provide assurance that the generated formulas are well-formed and conform to the intended semantics through an automated validation framework and a formal proof. The extended FRET tool enables developers to specify probabilistic requirements in structured natural language, and to automatically translate them into probabilistic temporal logic, making the formal analysis of autonomous and adaptive systems more practical and less error-prone.

</details>


### [3] [A Neurosymbolic Approach to Loop Invariant Generation via Weakest Precondition Reasoning](https://arxiv.org/abs/2512.15816)
*Daragh King,Vasileios Koutavas,Laura Kovacs*

Main category: cs.PL

TL;DR: NeuroInv：一种结合神经网络和符号推理的神经符号方法，用于自动生成循环不变式，在150个Java程序上达到99.5%的成功率


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的循环不变式生成方法缺乏可靠的结构化方法论，且很少参考现有的程序验证理论。循环不变式生成仍然是自动化程序验证的关键瓶颈。

Method: NeuroInv包含两个关键模块：(1) 神经推理模块：利用LLMs和霍尔逻辑，通过向后链式最弱前置条件推理来推导和精化候选不变式；(2) 验证引导的符号模块：使用OpenJML的反例迭代修复不变式。

Result: 在包含单循环、多循环、多数组、随机分支和噪声代码段的150个Java程序基准测试中，NeuroInv达到99.5%的成功率，显著优于其他方法。在包含10个更大规模多循环程序（平均每个程序7个循环）的困难基准测试中也表现出色。

Conclusion: NeuroInv通过神经符号方法有效解决了循环不变式生成问题，能够扩展到更复杂的验证场景，为自动化程序验证提供了可靠且可扩展的解决方案。

Abstract: Loop invariant generation remains a critical bottleneck in automated program verification. Recent work has begun to explore the use of Large Language Models (LLMs) in this area, yet these approaches tend to lack a reliable and structured methodology, with little reference to existing program verification theory. This paper presents NeuroInv, a neurosymbolic approach to loop invariant generation. NeuroInv comprises two key modules: (1) a neural reasoning module that leverages LLMs and Hoare logic to derive and refine candidate invariants via backward-chaining weakest precondition reasoning, and (2) a verification-guided symbolic module that iteratively repairs invariants using counterexamples from OpenJML. We evaluate NeuroInv on a comprehensive benchmark of 150 Java programs, encompassing single and multiple (sequential) loops, multiple arrays, random branching, and noisy code segments. NeuroInv achieves a $99.5\%$ success rate, substantially outperforming the other evaluated approaches. Additionally, we introduce a hard benchmark of $10$ larger multi-loop programs (with an average of $7$ loops each); NeuroInv's performance in this setting demonstrates that it can scale to more complex verification scenarios.

</details>


### [4] [Optimizing Agentic Language Model Inference via Speculative Tool Calls](https://arxiv.org/abs/2512.15834)
*Daniel Nichols,Prajwal Singhania,Charles Jekel,Abhinav Bhatele,Harshitha Menon*

Main category: cs.PL

TL;DR: 论文提出针对语言模型工具调用性能瓶颈的系统优化方案，通过推测工具调用和保持序列驻留来提升推理吞吐量


<details>
  <summary>Details</summary>
Motivation: 语言模型越来越依赖外部工具（文件搜索、代码执行、API调用等），但工具调用会引入推理过程中的性能瓶颈，影响LM代理的推理效率

Method: 提出两种系统优化：1）推测工具调用以减少等待时间；2）强制序列保持在推理引擎中驻留以最小化开销。还提供了算法理论分析，并建议新的"工具缓存"API端点

Result: 优化方案使LM代理推理的吞吐量提升数百token/秒，理论分析为推测配置提供性能优化指导

Conclusion: 通过系统优化解决LM工具调用性能瓶颈是有效的，提出的"工具缓存"API可以帮助LM提供商轻松采用这些优化

Abstract: Language models (LMs) are becoming increasingly dependent on external tools. LM-based agentic frameworks frequently interact with their environment via such tools to search files, run code, call APIs, etc. Further, modern reasoning-based LMs use tools such as web search and Python code execution to enhance their reasoning capabilities. While tools greatly improve the capabilities of LMs, they also introduce performance bottlenecks during the inference process. In this paper, we introduce novel systems optimizations to address such performance bottlenecks by speculating tool calls and forcing sequences to remain resident in the inference engine to minimize overheads. Our optimizations lead to throughput improvements of several hundred tokens per second when hosting inference for LM agents. We provide a theoretical analysis of our algorithms to provide insights into speculation configurations that will yield the best performance. Further, we recommend a new "tool cache" API endpoint to enable LM providers to easily adopt these optimizations.

</details>
