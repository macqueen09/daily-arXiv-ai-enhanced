<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 4]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [LOOPRAG: Enhancing Loop Transformation Optimization with Retrieval-Augmented Large Language Models](https://arxiv.org/abs/2512.15766)
*Yijie Zhi,Yayu Cao,Jianhua Dai,Xiaoyang Han,Jingwen Pu,Qingran Wu,Sheng Cheng,Ming Cai*

Main category: cs.PL

TL;DR: LOOPRAG：基于检索增强生成的循环变换优化框架，通过参数驱动方法生成多样化合法示例，结合循环感知检索算法和反馈迭代机制，显著提升LLM在循环优化上的性能。


<details>
  <summary>Details</summary>
Motivation: 传统循环变换优化复杂且难以找到最优组合，现有LLM在循环优化方面表现不佳，经常产生错误或次优结果，错失性能提升机会。

Method: 提出LOOPRAG框架：1) 参数驱动方法利用循环属性触发各种变换，生成多样化合法示例作为演示源；2) 基于循环特征的循环感知检索算法平衡相似性和多样性；3) 反馈迭代机制结合编译、测试和性能结果指导LLM；4) 对优化代码进行变异、覆盖和差分测试进行等价性检查。

Result: 在PolyBench、TSVC和LORE基准测试中，相比基础编译器分别获得最高11.20×、14.34×和9.29×的加速比，相比基础LLM分别获得最高11.97×、5.61×和11.59×的加速比。

Conclusion: LOOPRAG通过检索增强生成框架有效解决了LLM在循环优化中的局限性，显著提升了循环变换优化的性能和可靠性。

Abstract: Loop transformations are semantics-preserving optimization techniques, widely used to maximize objectives such as parallelism. Despite decades of research, applying the optimal composition of loop transformations remains challenging due to inherent complexities, including cost modeling for optimization objectives. Recent studies have explored the potential of Large Language Models (LLMs) for code optimization. However, our key observation is that LLMs often struggle with effective loop transformation optimization, frequently leading to errors or suboptimal optimization, thereby missing opportunities for performance improvements. To bridge this gap, we propose LOOPRAG, a novel retrieval-augmented generation framework designed to guide LLMs in performing effective loop optimization on Static Control Part. We introduce a parameter-driven method to harness loop properties, which trigger various loop transformations, and generate diverse yet legal example codes serving as a demonstration source. To effectively obtain the most informative demonstrations, we propose a loop-aware algorithm based on loop features, which balances similarity and diversity for code retrieval. To enhance correct and efficient code generation, we introduce a feedback-based iterative mechanism that incorporates compilation, testing and performance results as feedback to guide LLMs. Each optimized code undergoes mutation, coverage and differential testing for equivalence checking. We evaluate LOOPRAG on PolyBench, TSVC and LORE benchmark suites, and compare it against compilers (GCC-Graphite, Clang-Polly, Perspective and ICX) and representative LLMs (DeepSeek and GPT-4). The results demonstrate average speedups over base compilers of up to 11.20$\times$, 14.34$\times$, and 9.29$\times$ for PolyBench, TSVC, and LORE, respectively, and speedups over base LLMs of up to 11.97$\times$, 5.61$\times$, and 11.59$\times$.

</details>


### [2] [Automated Formalization of Probabilistic Requirements from Structured Natural Language](https://arxiv.org/abs/2512.15788)
*Anastasia Mavridou,Marie Farrell,Gricel Vázquez,Tom Pressburger,Timothy E. Wang,Radu Calinescu,Michael Fisher*

Main category: cs.PL

TL;DR: 扩展NASA的FRET工具，支持用结构化自然语言指定概率需求，并自动转换为概率时序逻辑公式，使自主自适应系统的形式化分析更实用


<details>
  <summary>Details</summary>
Motivation: 自主自适应系统在软件开发中面临重大挑战，特别是需要明确捕捉环境或决策过程中的不确定性。在安全和任务关键系统中，这些挑战更加突出，因为需要进行严格审查。主要困难在于如何用概率构造来指定捕捉系统不确定性的需求，而期望开发者直接使用复杂的概率逻辑形式化表示是不现实的且容易出错。

Method: 扩展NASA的形式化需求获取工具(FRET)的结构化自然语言，支持概率需求的规范；开发一种形式化、组合式、自动化的方法，将结构化自然语言需求转换为概率时序逻辑公式；建立自动化验证框架和形式化证明，确保生成的公式格式良好且符合预期语义。

Result: 扩展后的FRET工具使开发者能够用结构化自然语言指定概率需求，并自动将其转换为概率时序逻辑公式，使自主自适应系统的形式化分析更加实用且减少错误。

Conclusion: 通过扩展FRET工具支持概率需求规范，并提供自动化转换和验证框架，显著提高了自主自适应系统形式化分析的可行性和可靠性，使开发者能够更轻松地处理不确定性需求。

Abstract: Integrating autonomous and adaptive behavior into software-intensive systems presents significant challenges for software development, as uncertainties in the environment or decision-making processes must be explicitly captured. These challenges are amplified in safety- and mission-critical systems, which must undergo rigorous scrutiny during design and development. Key among these challenges is the difficulty of specifying requirements that use probabilistic constructs to capture the uncertainty affecting these systems. To enable formal analysis, such requirements must be expressed in precise mathematical notations such as probabilistic logics. However, expecting developers to write requirements directly in complex formalisms is unrealistic and highly error-prone. We extend the structured natural language used by NASA's Formal Requirement Elicitation Tool (FRET) with support for the specification of unambiguous and correct probabilistic requirements, and develop an automated approach for translating these requirements into logical formulas. We propose and develop a formal, compositional, and automated approach for translating structured natural-language requirements into formulas in probabilistic temporal logic. To increase trust in our formalizations, we provide assurance that the generated formulas are well-formed and conform to the intended semantics through an automated validation framework and a formal proof. The extended FRET tool enables developers to specify probabilistic requirements in structured natural language, and to automatically translate them into probabilistic temporal logic, making the formal analysis of autonomous and adaptive systems more practical and less error-prone.

</details>


### [3] [A Neurosymbolic Approach to Loop Invariant Generation via Weakest Precondition Reasoning](https://arxiv.org/abs/2512.15816)
*Daragh King,Vasileios Koutavas,Laura Kovacs*

Main category: cs.PL

TL;DR: NeuroInv是一种神经符号方法，通过结合LLM的推理能力和符号验证来生成循环不变式，在150个Java程序上达到99.5%的成功率。


<details>
  <summary>Details</summary>
Motivation: 循环不变式生成是自动程序验证的关键瓶颈。现有基于LLM的方法缺乏可靠的结构化方法，且很少参考现有程序验证理论。

Method: NeuroInv包含两个模块：1) 神经推理模块，利用LLM和霍尔逻辑通过反向链最弱前置条件推理来推导和精化候选不变式；2) 验证引导的符号模块，使用OpenJML的反例迭代修复不变式。

Result: 在包含单循环、多循环、多数组、随机分支和噪声代码段的150个Java程序基准测试中，NeuroInv达到99.5%的成功率，显著优于其他方法。在包含10个更大规模多循环程序（平均每个程序7个循环）的困难基准测试中也表现出色。

Conclusion: NeuroInv通过神经符号方法有效解决了循环不变式生成问题，能够扩展到更复杂的验证场景，为自动化程序验证提供了可靠解决方案。

Abstract: Loop invariant generation remains a critical bottleneck in automated program verification. Recent work has begun to explore the use of Large Language Models (LLMs) in this area, yet these approaches tend to lack a reliable and structured methodology, with little reference to existing program verification theory. This paper presents NeuroInv, a neurosymbolic approach to loop invariant generation. NeuroInv comprises two key modules: (1) a neural reasoning module that leverages LLMs and Hoare logic to derive and refine candidate invariants via backward-chaining weakest precondition reasoning, and (2) a verification-guided symbolic module that iteratively repairs invariants using counterexamples from OpenJML. We evaluate NeuroInv on a comprehensive benchmark of 150 Java programs, encompassing single and multiple (sequential) loops, multiple arrays, random branching, and noisy code segments. NeuroInv achieves a $99.5\%$ success rate, substantially outperforming the other evaluated approaches. Additionally, we introduce a hard benchmark of $10$ larger multi-loop programs (with an average of $7$ loops each); NeuroInv's performance in this setting demonstrates that it can scale to more complex verification scenarios.

</details>


### [4] [Optimizing Agentic Language Model Inference via Speculative Tool Calls](https://arxiv.org/abs/2512.15834)
*Daniel Nichols,Prajwal Singhania,Charles Jekel,Abhinav Bhatele,Harshitha Menon*

Main category: cs.PL

TL;DR: 论文提出针对语言模型工具调用性能瓶颈的系统优化方案，通过推测工具调用和保持序列驻留来提升推理吞吐量，达到每秒数百token的改进。


<details>
  <summary>Details</summary>
Motivation: 语言模型越来越依赖外部工具（文件搜索、代码执行、API调用等），但工具调用在推理过程中引入了显著的性能瓶颈，影响LM代理框架的效率。

Method: 1. 推测工具调用：提前预测并执行可能的工具调用；2. 保持序列驻留：让相关序列保持在推理引擎中，减少开销；3. 理论分析算法配置；4. 提出"工具缓存"API端点便于LM提供商采用。

Result: 优化方案使LM代理推理的吞吐量提升达到每秒数百token的显著改进，并通过理论分析提供了最佳推测配置的指导。

Conclusion: 提出的系统优化有效解决了LM工具调用的性能瓶颈，建议的"工具缓存"API端点有助于LM提供商轻松采用这些优化，提升整体推理效率。

Abstract: Language models (LMs) are becoming increasingly dependent on external tools. LM-based agentic frameworks frequently interact with their environment via such tools to search files, run code, call APIs, etc. Further, modern reasoning-based LMs use tools such as web search and Python code execution to enhance their reasoning capabilities. While tools greatly improve the capabilities of LMs, they also introduce performance bottlenecks during the inference process. In this paper, we introduce novel systems optimizations to address such performance bottlenecks by speculating tool calls and forcing sequences to remain resident in the inference engine to minimize overheads. Our optimizations lead to throughput improvements of several hundred tokens per second when hosting inference for LM agents. We provide a theoretical analysis of our algorithms to provide insights into speculation configurations that will yield the best performance. Further, we recommend a new "tool cache" API endpoint to enable LM providers to easily adopt these optimizations.

</details>
