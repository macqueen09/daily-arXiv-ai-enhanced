{"id": "2512.15766", "pdf": "https://arxiv.org/pdf/2512.15766", "abs": "https://arxiv.org/abs/2512.15766", "authors": ["Yijie Zhi", "Yayu Cao", "Jianhua Dai", "Xiaoyang Han", "Jingwen Pu", "Qingran Wu", "Sheng Cheng", "Ming Cai"], "title": "LOOPRAG: Enhancing Loop Transformation Optimization with Retrieval-Augmented Large Language Models", "categories": ["cs.PL", "cs.AI", "cs.DC", "cs.PF"], "comment": "Accepted to ASPLOS 2026", "summary": "Loop transformations are semantics-preserving optimization techniques, widely used to maximize objectives such as parallelism. Despite decades of research, applying the optimal composition of loop transformations remains challenging due to inherent complexities, including cost modeling for optimization objectives. Recent studies have explored the potential of Large Language Models (LLMs) for code optimization. However, our key observation is that LLMs often struggle with effective loop transformation optimization, frequently leading to errors or suboptimal optimization, thereby missing opportunities for performance improvements. To bridge this gap, we propose LOOPRAG, a novel retrieval-augmented generation framework designed to guide LLMs in performing effective loop optimization on Static Control Part. We introduce a parameter-driven method to harness loop properties, which trigger various loop transformations, and generate diverse yet legal example codes serving as a demonstration source. To effectively obtain the most informative demonstrations, we propose a loop-aware algorithm based on loop features, which balances similarity and diversity for code retrieval. To enhance correct and efficient code generation, we introduce a feedback-based iterative mechanism that incorporates compilation, testing and performance results as feedback to guide LLMs. Each optimized code undergoes mutation, coverage and differential testing for equivalence checking. We evaluate LOOPRAG on PolyBench, TSVC and LORE benchmark suites, and compare it against compilers (GCC-Graphite, Clang-Polly, Perspective and ICX) and representative LLMs (DeepSeek and GPT-4). The results demonstrate average speedups over base compilers of up to 11.20$\\times$, 14.34$\\times$, and 9.29$\\times$ for PolyBench, TSVC, and LORE, respectively, and speedups over base LLMs of up to 11.97$\\times$, 5.61$\\times$, and 11.59$\\times$.", "AI": {"tldr": "LOOPRAG\uff1a\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u5faa\u73af\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u53c2\u6570\u9a71\u52a8\u65b9\u6cd5\u751f\u6210\u591a\u6837\u5316\u5408\u6cd5\u793a\u4f8b\uff0c\u7ed3\u5408\u5faa\u73af\u611f\u77e5\u68c0\u7d22\u548c\u53cd\u9988\u8fed\u4ee3\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347LLM\u5728\u5faa\u73af\u53d8\u6362\u4f18\u5316\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u5faa\u73af\u53d8\u6362\u662f\u5e7f\u6cdb\u4f7f\u7528\u7684\u8bed\u4e49\u4fdd\u6301\u4f18\u5316\u6280\u672f\uff0c\u4f46\u627e\u5230\u6700\u4f18\u53d8\u6362\u7ec4\u5408\u4ecd\u7136\u56f0\u96be\u3002\u73b0\u6709LLM\u5728\u5faa\u73af\u4f18\u5316\u4e2d\u5e38\u4ea7\u751f\u9519\u8bef\u6216\u6b21\u4f18\u7ed3\u679c\uff0c\u9519\u5931\u6027\u80fd\u63d0\u5347\u673a\u4f1a\u3002", "method": "\u63d0\u51faLOOPRAG\u6846\u67b6\uff1a1\uff09\u53c2\u6570\u9a71\u52a8\u65b9\u6cd5\u5229\u7528\u5faa\u73af\u5c5e\u6027\u89e6\u53d1\u5404\u79cd\u53d8\u6362\uff0c\u751f\u6210\u591a\u6837\u5316\u5408\u6cd5\u793a\u4f8b\uff1b2\uff09\u57fa\u4e8e\u5faa\u73af\u7279\u5f81\u7684\u68c0\u7d22\u7b97\u6cd5\u5e73\u8861\u76f8\u4f3c\u6027\u548c\u591a\u6837\u6027\uff1b3\uff09\u53cd\u9988\u8fed\u4ee3\u673a\u5236\u7ed3\u5408\u7f16\u8bd1\u3001\u6d4b\u8bd5\u548c\u6027\u80fd\u7ed3\u679c\u6307\u5bfcLLM\uff1b4\uff09\u901a\u8fc7\u53d8\u5f02\u3001\u8986\u76d6\u548c\u5dee\u5206\u6d4b\u8bd5\u8fdb\u884c\u7b49\u4ef7\u6027\u68c0\u67e5\u3002", "result": "\u5728PolyBench\u3001TSVC\u548cLORE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u57fa\u7840\u7f16\u8bd1\u5668\u5206\u522b\u83b7\u5f97\u6700\u9ad811.20\u00d7\u300114.34\u00d7\u548c9.29\u00d7\u52a0\u901f\uff1b\u76f8\u6bd4\u57fa\u7840LLM\u5206\u522b\u83b7\u5f97\u6700\u9ad811.97\u00d7\u30015.61\u00d7\u548c11.59\u00d7\u52a0\u901f\u3002", "conclusion": "LOOPRAG\u6709\u6548\u89e3\u51b3\u4e86LLM\u5728\u5faa\u73af\u4f18\u5316\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u53cd\u9988\u8fed\u4ee3\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u5faa\u73af\u53d8\u6362\u4f18\u5316\u7684\u6548\u679c\uff0c\u4e3a\u4ee3\u7801\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2512.15788", "pdf": "https://arxiv.org/pdf/2512.15788", "abs": "https://arxiv.org/abs/2512.15788", "authors": ["Anastasia Mavridou", "Marie Farrell", "Gricel V\u00e1zquez", "Tom Pressburger", "Timothy E. Wang", "Radu Calinescu", "Michael Fisher"], "title": "Automated Formalization of Probabilistic Requirements from Structured Natural Language", "categories": ["cs.PL", "cs.FL", "cs.SE"], "comment": "Official website https://github.com/NASA-SW-VnV/fret/releases/tag/v3.0.0", "summary": "Integrating autonomous and adaptive behavior into software-intensive systems presents significant challenges for software development, as uncertainties in the environment or decision-making processes must be explicitly captured. These challenges are amplified in safety- and mission-critical systems, which must undergo rigorous scrutiny during design and development. Key among these challenges is the difficulty of specifying requirements that use probabilistic constructs to capture the uncertainty affecting these systems. To enable formal analysis, such requirements must be expressed in precise mathematical notations such as probabilistic logics. However, expecting developers to write requirements directly in complex formalisms is unrealistic and highly error-prone. We extend the structured natural language used by NASA's Formal Requirement Elicitation Tool (FRET) with support for the specification of unambiguous and correct probabilistic requirements, and develop an automated approach for translating these requirements into logical formulas. We propose and develop a formal, compositional, and automated approach for translating structured natural-language requirements into formulas in probabilistic temporal logic. To increase trust in our formalizations, we provide assurance that the generated formulas are well-formed and conform to the intended semantics through an automated validation framework and a formal proof. The extended FRET tool enables developers to specify probabilistic requirements in structured natural language, and to automatically translate them into probabilistic temporal logic, making the formal analysis of autonomous and adaptive systems more practical and less error-prone.", "AI": {"tldr": "\u6269\u5c55NASA\u7684FRET\u5de5\u5177\uff0c\u652f\u6301\u7528\u7ed3\u6784\u5316\u81ea\u7136\u8bed\u8a00\u7f16\u5199\u6982\u7387\u9700\u6c42\uff0c\u5e76\u81ea\u52a8\u8f6c\u6362\u4e3a\u6982\u7387\u65f6\u5e8f\u903b\u8f91\u516c\u5f0f\uff0c\u4f7f\u81ea\u4e3b\u81ea\u9002\u5e94\u7cfb\u7edf\u7684\u5f62\u5f0f\u5316\u5206\u6790\u66f4\u5b9e\u7528", "motivation": "\u81ea\u4e3b\u81ea\u9002\u5e94\u7cfb\u7edf\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u7279\u522b\u662f\u9700\u8981\u660e\u786e\u6355\u6349\u73af\u5883\u6216\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u3002\u5728\u5b89\u5168\u548c\u4efb\u52a1\u5173\u952e\u7cfb\u7edf\u4e2d\uff0c\u8fd9\u4e9b\u6311\u6218\u66f4\u4e3a\u7a81\u51fa\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u7cfb\u7edf\u5fc5\u987b\u5728\u8bbe\u8ba1\u548c\u5f00\u53d1\u8fc7\u7a0b\u4e2d\u7ecf\u8fc7\u4e25\u683c\u5ba1\u67e5\u3002\u4e3b\u8981\u6311\u6218\u4e4b\u4e00\u662f\u96be\u4ee5\u4f7f\u7528\u6982\u7387\u6784\u9020\u6765\u6307\u5b9a\u9700\u6c42\u4ee5\u6355\u6349\u5f71\u54cd\u8fd9\u4e9b\u7cfb\u7edf\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u6269\u5c55NASA\u6b63\u5f0f\u9700\u6c42\u83b7\u53d6\u5de5\u5177(FRET)\u7684\u7ed3\u6784\u5316\u81ea\u7136\u8bed\u8a00\uff0c\u652f\u6301\u6982\u7387\u9700\u6c42\u7684\u89c4\u8303\u3002\u5f00\u53d1\u4e00\u79cd\u5f62\u5f0f\u5316\u3001\u7ec4\u5408\u5f0f\u3001\u81ea\u52a8\u5316\u7684\u65b9\u6cd5\uff0c\u5c06\u7ed3\u6784\u5316\u81ea\u7136\u8bed\u8a00\u9700\u6c42\u8f6c\u6362\u4e3a\u6982\u7387\u65f6\u5e8f\u903b\u8f91\u516c\u5f0f\u3002\u63d0\u4f9b\u81ea\u52a8\u5316\u9a8c\u8bc1\u6846\u67b6\u548c\u5f62\u5f0f\u5316\u8bc1\u660e\u6765\u786e\u4fdd\u751f\u6210\u7684\u516c\u5f0f\u7b26\u5408\u9884\u671f\u8bed\u4e49\u3002", "result": "\u6269\u5c55\u540e\u7684FRET\u5de5\u5177\u4f7f\u5f00\u53d1\u4eba\u5458\u80fd\u591f\u7528\u7ed3\u6784\u5316\u81ea\u7136\u8bed\u8a00\u6307\u5b9a\u6982\u7387\u9700\u6c42\uff0c\u5e76\u81ea\u52a8\u5c06\u5176\u8f6c\u6362\u4e3a\u6982\u7387\u65f6\u5e8f\u903b\u8f91\uff0c\u4f7f\u81ea\u4e3b\u81ea\u9002\u5e94\u7cfb\u7edf\u7684\u5f62\u5f0f\u5316\u5206\u6790\u66f4\u52a0\u5b9e\u7528\u4e14\u51cf\u5c11\u9519\u8bef\u3002", "conclusion": "\u901a\u8fc7\u6269\u5c55FRET\u5de5\u5177\u652f\u6301\u6982\u7387\u9700\u6c42\u89c4\u8303\uff0c\u5e76\u63d0\u4f9b\u81ea\u52a8\u5316\u8f6c\u6362\u548c\u9a8c\u8bc1\u6846\u67b6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u81ea\u4e3b\u81ea\u9002\u5e94\u7cfb\u7edf\u5f62\u5f0f\u5316\u5206\u6790\u7684\u5b9e\u7528\u6027\u548c\u53ef\u9760\u6027\uff0c\u89e3\u51b3\u4e86\u5f00\u53d1\u4eba\u5458\u76f4\u63a5\u4f7f\u7528\u590d\u6742\u5f62\u5f0f\u5316\u65b9\u6cd5\u7684\u4e0d\u73b0\u5b9e\u6027\u548c\u6613\u9519\u6027\u95ee\u9898\u3002"}}
{"id": "2512.15816", "pdf": "https://arxiv.org/pdf/2512.15816", "abs": "https://arxiv.org/abs/2512.15816", "authors": ["Daragh King", "Vasileios Koutavas", "Laura Kovacs"], "title": "A Neurosymbolic Approach to Loop Invariant Generation via Weakest Precondition Reasoning", "categories": ["cs.PL", "cs.AI", "cs.LO"], "comment": null, "summary": "Loop invariant generation remains a critical bottleneck in automated program verification. Recent work has begun to explore the use of Large Language Models (LLMs) in this area, yet these approaches tend to lack a reliable and structured methodology, with little reference to existing program verification theory. This paper presents NeuroInv, a neurosymbolic approach to loop invariant generation. NeuroInv comprises two key modules: (1) a neural reasoning module that leverages LLMs and Hoare logic to derive and refine candidate invariants via backward-chaining weakest precondition reasoning, and (2) a verification-guided symbolic module that iteratively repairs invariants using counterexamples from OpenJML. We evaluate NeuroInv on a comprehensive benchmark of 150 Java programs, encompassing single and multiple (sequential) loops, multiple arrays, random branching, and noisy code segments. NeuroInv achieves a $99.5\\%$ success rate, substantially outperforming the other evaluated approaches. Additionally, we introduce a hard benchmark of $10$ larger multi-loop programs (with an average of $7$ loops each); NeuroInv's performance in this setting demonstrates that it can scale to more complex verification scenarios.", "AI": {"tldr": "NeuroInv\u662f\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\uff0c\u7ed3\u5408LLM\u548cHoare\u903b\u8f91\u8fdb\u884c\u5faa\u73af\u4e0d\u53d8\u5f0f\u751f\u6210\uff0c\u5728150\u4e2aJava\u7a0b\u5e8f\u4e0a\u8fbe\u523099.5%\u6210\u529f\u7387", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u5faa\u73af\u4e0d\u53d8\u5f0f\u751f\u6210\u65b9\u6cd5\u7f3a\u4e4f\u53ef\u9760\u7684\u7ed3\u6784\u5316\u65b9\u6cd5\uff0c\u4e14\u5f88\u5c11\u53c2\u8003\u7a0b\u5e8f\u9a8c\u8bc1\u7406\u8bba\uff0c\u9700\u8981\u66f4\u7cfb\u7edf\u7684\u65b9\u6cd5", "method": "\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u5305\u542b\u4e24\u4e2a\u6a21\u5757\uff1a1) \u795e\u7ecf\u63a8\u7406\u6a21\u5757\u5229\u7528LLM\u548cHoare\u903b\u8f91\u901a\u8fc7\u540e\u5411\u94fe\u6700\u5f31\u524d\u7f6e\u6761\u4ef6\u63a8\u7406\u63a8\u5bfc\u548c\u7cbe\u5316\u5019\u9009\u4e0d\u53d8\u5f0f\uff1b2) \u9a8c\u8bc1\u5f15\u5bfc\u7684\u7b26\u53f7\u6a21\u5757\u4f7f\u7528OpenJML\u7684\u53cd\u4f8b\u8fed\u4ee3\u4fee\u590d\u4e0d\u53d8\u5f0f", "result": "\u5728150\u4e2aJava\u7a0b\u5e8f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523099.5%\u6210\u529f\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff1b\u572810\u4e2a\u5927\u578b\u591a\u5faa\u73af\u7a0b\u5e8f(\u5e73\u57477\u4e2a\u5faa\u73af)\u7684\u56f0\u96be\u57fa\u51c6\u4e0a\u4e5f\u8868\u73b0\u826f\u597d", "conclusion": "NeuroInv\u5c55\u793a\u4e86\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u5728\u5faa\u73af\u4e0d\u53d8\u5f0f\u751f\u6210\u4e2d\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u6269\u5c55\u5230\u66f4\u590d\u6742\u7684\u9a8c\u8bc1\u573a\u666f"}}
{"id": "2512.15834", "pdf": "https://arxiv.org/pdf/2512.15834", "abs": "https://arxiv.org/abs/2512.15834", "authors": ["Daniel Nichols", "Prajwal Singhania", "Charles Jekel", "Abhinav Bhatele", "Harshitha Menon"], "title": "Optimizing Agentic Language Model Inference via Speculative Tool Calls", "categories": ["cs.PL", "cs.AI", "cs.DC", "cs.PF", "cs.SE"], "comment": null, "summary": "Language models (LMs) are becoming increasingly dependent on external tools. LM-based agentic frameworks frequently interact with their environment via such tools to search files, run code, call APIs, etc. Further, modern reasoning-based LMs use tools such as web search and Python code execution to enhance their reasoning capabilities. While tools greatly improve the capabilities of LMs, they also introduce performance bottlenecks during the inference process. In this paper, we introduce novel systems optimizations to address such performance bottlenecks by speculating tool calls and forcing sequences to remain resident in the inference engine to minimize overheads. Our optimizations lead to throughput improvements of several hundred tokens per second when hosting inference for LM agents. We provide a theoretical analysis of our algorithms to provide insights into speculation configurations that will yield the best performance. Further, we recommend a new \"tool cache\" API endpoint to enable LM providers to easily adopt these optimizations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u9488\u5bf9\u8bed\u8a00\u6a21\u578b\u5de5\u5177\u8c03\u7528\u6027\u80fd\u74f6\u9888\u7684\u7cfb\u7edf\u4f18\u5316\u65b9\u6848\uff0c\u901a\u8fc7\u63a8\u6d4b\u5de5\u5177\u8c03\u7528\u548c\u4fdd\u6301\u5e8f\u5217\u9a7b\u7559\u6765\u63d0\u5347\u63a8\u7406\u541e\u5410\u91cf\uff0c\u5b9e\u73b0\u6bcf\u79d2\u6570\u767etoken\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u4f9d\u8d56\u5916\u90e8\u5de5\u5177\uff08\u5982\u6587\u4ef6\u641c\u7d22\u3001\u4ee3\u7801\u6267\u884c\u3001API\u8c03\u7528\u7b49\uff09\uff0c\u8fd9\u4e9b\u5de5\u5177\u867d\u7136\u589e\u5f3a\u4e86\u6a21\u578b\u80fd\u529b\uff0c\u4f46\u4e5f\u5f15\u5165\u4e86\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u6027\u80fd\u74f6\u9888\uff0c\u5f71\u54cd\u4e86LM\u4ee3\u7406\u6846\u67b6\u7684\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u7cfb\u7edf\u4f18\u5316\uff1a1) \u63a8\u6d4b\u5de5\u5177\u8c03\u7528\uff0c\u63d0\u524d\u9884\u6d4b\u5e76\u6267\u884c\u53ef\u80fd\u7684\u5de5\u5177\u8c03\u7528\uff1b2) \u5f3a\u5236\u5e8f\u5217\u5728\u63a8\u7406\u5f15\u64ce\u4e2d\u4fdd\u6301\u9a7b\u7559\uff0c\u51cf\u5c11\u5f00\u9500\u3002\u8fd8\u5efa\u8bae\u65b0\u7684\"\u5de5\u5177\u7f13\u5b58\"API\u7aef\u70b9\u3002", "result": "\u4f18\u5316\u65b9\u6848\u4f7fLM\u4ee3\u7406\u63a8\u7406\u7684\u541e\u5410\u91cf\u63d0\u5347\u4e86\u6bcf\u79d2\u6570\u767e\u4e2atoken\uff0c\u663e\u8457\u6539\u5584\u4e86\u6027\u80fd\u3002\u63d0\u4f9b\u4e86\u7406\u8bba\u5206\u6790\u6307\u5bfc\u6700\u4f18\u63a8\u6d4b\u914d\u7f6e\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u5c42\u9762\u7684\u4f18\u5316\u53ef\u4ee5\u6709\u6548\u89e3\u51b3LM\u5de5\u5177\u8c03\u7528\u5e26\u6765\u7684\u6027\u80fd\u74f6\u9888\uff0c\u63d0\u51fa\u7684\u63a8\u6d4b\u5de5\u5177\u8c03\u7528\u548c\u5e8f\u5217\u9a7b\u7559\u6280\u672f\u80fd\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\uff0c\u5efa\u8baeLM\u63d0\u4f9b\u5546\u91c7\u7528\u5de5\u5177\u7f13\u5b58API\u3002"}}
