{"id": "2512.18134", "pdf": "https://arxiv.org/pdf/2512.18134", "abs": "https://arxiv.org/abs/2512.18134", "authors": ["Rupanshu Soi", "Rohan Yadav", "Fredrik Kjolstad", "Alex Aiken", "Maryam Mehri Dehnavi", "Michael Garland", "Michael Bauer"], "title": "Optimal Software Pipelining and Warp Specialization for Tensor Core GPUs", "categories": ["cs.PL", "cs.AR", "cs.LG"], "comment": null, "summary": "GPU architectures have continued to grow in complexity, with recent incarnations introducing increasingly powerful fixed-function units for matrix multiplication and data movement to accompany highly parallel general-purpose cores. To fully leverage these machines, software must use sophisticated schedules that maximally utilize all hardware resources. Since realizing such schedules is complex, both programmers and compilers routinely employ program transformations, such as software pipelining (SWP) and warp specialization (WS), to do so in practice. However, determining how best to use SWP and WS in combination is a challenging problem that is currently handled through a mix of brittle compilation heuristics and fallible human intuition, with little insight into the space of solutions. To remedy this situation, we introduce a novel formulation of SWP and WS as a joint optimization problem that can be solved holistically by off-the-shelf constraint solvers. We reify our approach in Twill, the first system that automatically derives optimal SWP and WS schedules for a large class of iterative programs. Twill is heuristic-free, easily extensible to new GPU architectures, and guaranteed to produce optimal schedules. We show that Twill can rediscover, and thereby prove optimal, the SWP and WS schedules manually developed by experts for Flash Attention on both the NVIDIA Hopper and Blackwell GPU architectures."}
{"id": "2512.18842", "pdf": "https://arxiv.org/pdf/2512.18842", "abs": "https://arxiv.org/abs/2512.18842", "authors": ["Aleksandr Fedchin", "Antero Mejr", "Hari Sundar", "Jeffrey S. Foster"], "title": "DafnyMPI: A Dafny Library for Verifying Message-Passing Concurrent Programs", "categories": ["cs.PL"], "comment": "To appear in Proceedings of the ACM on Programming Languages (POPL)", "summary": "The Message Passing Interface (MPI) is widely used in parallel, high-performance programming, yet writing bug-free software that uses MPI remains difficult. We introduce DafnyMPI, a novel, scalable approach to formally verifying MPI software. DafnyMPI allows proving deadlock freedom, termination, and functional equivalence with simpler sequential implementations. In contrast to existing specialized frameworks, DafnyMPI avoids custom concurrency logics and instead relies on Dafny, a verification-ready programming language used for sequential programs, extending it with concurrent reasoning abilities. DafnyMPI is implemented as a library that enables safe MPI programming by requiring users to specify the communication topology upfront and to verify that calls to communication primitives such as MPI_ISEND and MPI_WAIT meet their preconditions. We formalize DafnyMPI using a core calculus and prove that the preconditions suffice to guarantee deadlock freedom. Functional equivalence is proved via rely-guarantee reasoning over message payloads and a system that guarantees safe use of read and write buffers. Termination and the absence of runtime errors are proved using standard Dafny techniques. To further demonstrate the applicability of DafnyMPI, we verify numerical solutions to three canonical partial differential equations. We believe DafnyMPI demonstrates how to make formal verification viable for a broader class of programs and provides proof engineers with additional tools for software verification of parallel and concurrent systems."}
{"id": "2512.19250", "pdf": "https://arxiv.org/pdf/2512.19250", "abs": "https://arxiv.org/abs/2512.19250", "authors": ["Prathamesh Devadiga"], "title": "Small Language Models as Compiler Experts: Auto-Parallelization for Heterogeneous Systems", "categories": ["cs.LG", "cs.PL"], "comment": "Accepted at NeurIPS 2025 ML for Systems Workshop", "summary": "Traditional auto-parallelizing compilers, reliant on rigid heuristics, struggle with the complexity of modern heterogeneous systems. This paper presents a comprehensive evaluation of small (approximately 1B parameter) language-model-driven compiler auto-parallelization. We evaluate three models: gemma3, llama3.2, and qwen2.5, using six reasoning strategies across 11 real-world kernels drawn from scientific computing, graph algorithms, and machine learning. Our system is benchmarked against strong compiler baselines, including LLVM Polly, TVM, and Triton. Across 376 total evaluations, the proposed approach achieves an average speedup of 6.81x and a peak performance of 43.25x on convolution operations. We analyze scalability, verify correctness using multiple sanitizers, and confirm robustness across diverse compilers and hardware platforms. Our results demonstrate that small, efficient language models can serve as powerful reasoning engines for complex compiler optimization tasks."}
