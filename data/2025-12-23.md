<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.LG](#cs.LG) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Optimal Software Pipelining and Warp Specialization for Tensor Core GPUs](https://arxiv.org/abs/2512.18134)
*Rupanshu Soi,Rohan Yadav,Fredrik Kjolstad,Alex Aiken,Maryam Mehri Dehnavi,Michael Garland,Michael Bauer*

Main category: cs.PL

TL;DR: Twill系统首次自动为GPU程序生成最优的软件流水线（SWP）和warp专业化（WS）调度方案，无需启发式方法，可证明最优性。


<details>
  <summary>Details</summary>
Motivation: 现代GPU架构日益复杂，包含专用矩阵乘法单元和数据移动单元，需要复杂调度才能充分利用所有硬件资源。当前实践中，程序员和编译器依赖脆弱的启发式方法和人类直觉来组合使用SWP和WS，缺乏对解决方案空间的深入理解。

Method: 提出将SWP和WS作为联合优化问题的新颖形式化方法，可使用现成的约束求解器整体求解。实现为Twill系统，能够自动为大量迭代程序推导最优的SWP和WS调度方案。

Result: Twill能够重新发现并证明专家为Flash Attention在NVIDIA Hopper和Blackwell GPU架构上手动开发的SWP和WS调度方案的最优性。系统无启发式、易于扩展到新GPU架构，并保证生成最优调度。

Conclusion: Twill首次实现了自动生成最优GPU调度方案，解决了当前依赖启发式方法和人类直觉的问题，为GPU编程提供了系统化的优化方法。

Abstract: GPU architectures have continued to grow in complexity, with recent incarnations introducing increasingly powerful fixed-function units for matrix multiplication and data movement to accompany highly parallel general-purpose cores. To fully leverage these machines, software must use sophisticated schedules that maximally utilize all hardware resources. Since realizing such schedules is complex, both programmers and compilers routinely employ program transformations, such as software pipelining (SWP) and warp specialization (WS), to do so in practice. However, determining how best to use SWP and WS in combination is a challenging problem that is currently handled through a mix of brittle compilation heuristics and fallible human intuition, with little insight into the space of solutions. To remedy this situation, we introduce a novel formulation of SWP and WS as a joint optimization problem that can be solved holistically by off-the-shelf constraint solvers. We reify our approach in Twill, the first system that automatically derives optimal SWP and WS schedules for a large class of iterative programs. Twill is heuristic-free, easily extensible to new GPU architectures, and guaranteed to produce optimal schedules. We show that Twill can rediscover, and thereby prove optimal, the SWP and WS schedules manually developed by experts for Flash Attention on both the NVIDIA Hopper and Blackwell GPU architectures.

</details>


### [2] [DafnyMPI: A Dafny Library for Verifying Message-Passing Concurrent Programs](https://arxiv.org/abs/2512.18842)
*Aleksandr Fedchin,Antero Mejr,Hari Sundar,Jeffrey S. Foster*

Main category: cs.PL

TL;DR: DafnyMPI：基于Dafny语言的可扩展MPI程序形式化验证框架，可证明死锁自由、终止性和功能等价性


<details>
  <summary>Details</summary>
Motivation: MPI广泛用于高性能并行编程，但编写无bug的MPI软件仍然困难，需要更实用的形式化验证方法

Method: 基于Dafny验证语言构建MPI库，要求用户预先指定通信拓扑并验证通信原语的前置条件，使用核心演算形式化并证明死锁自由，通过rely-guarantee推理证明功能等价

Result: 成功验证了三个典型偏微分方程的数值解，证明了DafnyMPI的适用性，为并行系统软件验证提供了新工具

Conclusion: DafnyMPI展示了如何使形式化验证对更广泛的程序类别可行，为并行和并发系统的软件验证提供了额外工具

Abstract: The Message Passing Interface (MPI) is widely used in parallel, high-performance programming, yet writing bug-free software that uses MPI remains difficult. We introduce DafnyMPI, a novel, scalable approach to formally verifying MPI software. DafnyMPI allows proving deadlock freedom, termination, and functional equivalence with simpler sequential implementations. In contrast to existing specialized frameworks, DafnyMPI avoids custom concurrency logics and instead relies on Dafny, a verification-ready programming language used for sequential programs, extending it with concurrent reasoning abilities. DafnyMPI is implemented as a library that enables safe MPI programming by requiring users to specify the communication topology upfront and to verify that calls to communication primitives such as MPI_ISEND and MPI_WAIT meet their preconditions. We formalize DafnyMPI using a core calculus and prove that the preconditions suffice to guarantee deadlock freedom. Functional equivalence is proved via rely-guarantee reasoning over message payloads and a system that guarantees safe use of read and write buffers. Termination and the absence of runtime errors are proved using standard Dafny techniques. To further demonstrate the applicability of DafnyMPI, we verify numerical solutions to three canonical partial differential equations. We believe DafnyMPI demonstrates how to make formal verification viable for a broader class of programs and provides proof engineers with additional tools for software verification of parallel and concurrent systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [3] [Small Language Models as Compiler Experts: Auto-Parallelization for Heterogeneous Systems](https://arxiv.org/abs/2512.19250)
*Prathamesh Devadiga*

Main category: cs.LG

TL;DR: 小型语言模型（约10亿参数）驱动的编译器自动并行化方法，在11个真实内核上平均加速6.81倍，最高达43.25倍，超越传统编译器基线。


<details>
  <summary>Details</summary>
Motivation: 传统基于启发式的自动并行化编译器难以应对现代异构系统的复杂性，需要更智能的优化方法。

Method: 使用三种小型语言模型（gemma3、llama3.2、qwen2.5）和六种推理策略，在科学计算、图算法和机器学习等11个真实内核上进行编译器自动并行化。

Result: 在376次评估中，平均加速6.81倍，卷积操作峰值性能达43.25倍。验证了正确性和跨编译器/硬件平台的鲁棒性。

Conclusion: 小型高效的语言模型可以作为强大的推理引擎，用于复杂的编译器优化任务，为编译器自动并行化提供了新方向。

Abstract: Traditional auto-parallelizing compilers, reliant on rigid heuristics, struggle with the complexity of modern heterogeneous systems. This paper presents a comprehensive evaluation of small (approximately 1B parameter) language-model-driven compiler auto-parallelization. We evaluate three models: gemma3, llama3.2, and qwen2.5, using six reasoning strategies across 11 real-world kernels drawn from scientific computing, graph algorithms, and machine learning. Our system is benchmarked against strong compiler baselines, including LLVM Polly, TVM, and Triton. Across 376 total evaluations, the proposed approach achieves an average speedup of 6.81x and a peak performance of 43.25x on convolution operations. We analyze scalability, verify correctness using multiple sanitizers, and confirm robustness across diverse compilers and hardware platforms. Our results demonstrate that small, efficient language models can serve as powerful reasoning engines for complex compiler optimization tasks.

</details>
