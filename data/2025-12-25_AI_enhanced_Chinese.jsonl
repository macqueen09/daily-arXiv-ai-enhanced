{"id": "2512.21132", "pdf": "https://arxiv.org/pdf/2512.21132", "abs": "https://arxiv.org/abs/2512.21132", "authors": ["Tobias von Arx", "Niels M\u00fcndler", "Mark Vero", "Maximilian Baader", "Martin Vechev"], "title": "AutoBaxBuilder: Bootstrapping Code Security Benchmarking", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.PL"], "comment": null, "summary": "As LLMs see wide adoption in software engineering, the reliable assessment of the correctness and security of LLM-generated code is crucial. Notably, prior work has demonstrated that security is often overlooked, exposing that LLMs are prone to generating code with security vulnerabilities. These insights were enabled by specialized benchmarks, crafted through significant manual effort by security experts. However, relying on manually-crafted benchmarks is insufficient in the long term, because benchmarks (i) naturally end up contaminating training data, (ii) must extend to new tasks to provide a more complete picture, and (iii) must increase in difficulty to challenge more capable LLMs. In this work, we address these challenges and present AutoBaxBuilder, a framework that generates tasks and tests for code security benchmarking from scratch. We introduce a robust pipeline with fine-grained plausibility checks, leveraging the code understanding capabilities of LLMs to construct functionality tests and end-to-end security-probing exploits. To confirm the quality of the generated benchmark, we conduct both a qualitative analysis and perform quantitative experiments, comparing it against tasks constructed by human experts. We use AutoBaxBuilder to construct entirely new tasks and release them to the public as AutoBaxBench, together with a thorough evaluation of the security capabilities of LLMs on these tasks. We find that a new task can be generated in under 2 hours, costing less than USD 10.", "AI": {"tldr": "AutoBaxBuilder\u662f\u4e00\u4e2a\u81ea\u52a8\u751f\u6210\u4ee3\u7801\u5b89\u5168\u57fa\u51c6\u6d4b\u8bd5\u4efb\u52a1\u7684\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u96f6\u5f00\u59cb\u521b\u5efa\u529f\u80fd\u548c\u5b89\u5168\u6027\u6d4b\u8bd5\uff0c\u6210\u672c\u4f4e\u4e14\u6548\u7387\u9ad8\u3002", "motivation": "\u968f\u7740LLM\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u9700\u8981\u53ef\u9760\u8bc4\u4f30LLM\u751f\u6210\u4ee3\u7801\u7684\u6b63\u786e\u6027\u548c\u5b89\u5168\u6027\u3002\u73b0\u6709\u624b\u52a8\u6784\u5efa\u7684\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u4e09\u4e2a\u95ee\u9898\uff1a(1) \u5bb9\u6613\u6c61\u67d3\u8bad\u7ec3\u6570\u636e\uff0c(2) \u9700\u8981\u6269\u5c55\u5230\u65b0\u4efb\u52a1\uff0c(3) \u9700\u8981\u589e\u52a0\u96be\u5ea6\u4ee5\u6311\u6218\u66f4\u5f3a\u5927\u7684LLM\u3002", "method": "\u63d0\u51faAutoBaxBuilder\u6846\u67b6\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7684\u5408\u7406\u6027\u68c0\u67e5\u7ba1\u9053\uff0c\u5229\u7528LLM\u7684\u4ee3\u7801\u7406\u89e3\u80fd\u529b\u6784\u5efa\u529f\u80fd\u6d4b\u8bd5\u548c\u7aef\u5230\u7aef\u5b89\u5168\u63a2\u6d4b\u5229\u7528\u3002\u6846\u67b6\u80fd\u591f\u4ece\u96f6\u5f00\u59cb\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4efb\u52a1\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86AutoBaxBench\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u6bcf\u4e2a\u65b0\u4efb\u52a1\u751f\u6210\u65f6\u95f4\u5c11\u4e8e2\u5c0f\u65f6\uff0c\u6210\u672c\u4f4e\u4e8e10\u7f8e\u5143\u3002\u4e0e\u4eba\u5de5\u4e13\u5bb6\u6784\u5efa\u7684\u4efb\u52a1\u76f8\u6bd4\uff0c\u8be5\u6846\u67b6\u751f\u6210\u7684\u57fa\u51c6\u6d4b\u8bd5\u8d28\u91cf\u5f97\u5230\u9a8c\u8bc1\u3002", "conclusion": "AutoBaxBuilder\u80fd\u591f\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u5730\u751f\u6210\u4ee3\u7801\u5b89\u5168\u57fa\u51c6\u6d4b\u8bd5\u4efb\u52a1\uff0c\u89e3\u51b3\u4e86\u624b\u52a8\u57fa\u51c6\u6d4b\u8bd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u8bc4\u4f30LLM\u4ee3\u7801\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
