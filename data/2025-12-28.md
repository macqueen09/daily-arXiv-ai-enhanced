<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 1]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [AutoBaxBuilder: Bootstrapping Code Security Benchmarking](https://arxiv.org/abs/2512.21132)
*Tobias von Arx,Niels Mündler,Mark Vero,Maximilian Baader,Martin Vechev*

Main category: cs.CR

TL;DR: AutoBaxBuilder是一个自动生成代码安全基准测试任务的框架，无需人工手动创建，可在2小时内以低于10美元的成本生成新任务。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在软件工程中的广泛应用，可靠评估LLM生成代码的正确性和安全性变得至关重要。现有手动创建的安全基准存在三个主要问题：(1) 容易污染训练数据，(2) 需要扩展到新任务以提供更全面的评估，(3) 需要提高难度以挑战更强大的LLM。

Method: 提出AutoBaxBuilder框架，通过LLM的代码理解能力，从零开始生成代码安全基准测试任务。该框架包含细粒度的合理性检查管道，能够构建功能测试和端到端的安全探测利用。

Result: 使用AutoBaxBuilder构建了全新的基准测试任务集AutoBaxBench，并通过定性和定量实验验证了生成任务的质量。新任务生成时间少于2小时，成本低于10美元，与人工专家构建的任务相比具有可比性。

Conclusion: AutoBaxBuilder能够高效、低成本地自动生成高质量的代码安全基准测试任务，解决了手动创建基准的局限性，为持续评估LLM代码安全性提供了可扩展的解决方案。

Abstract: As LLMs see wide adoption in software engineering, the reliable assessment of the correctness and security of LLM-generated code is crucial. Notably, prior work has demonstrated that security is often overlooked, exposing that LLMs are prone to generating code with security vulnerabilities. These insights were enabled by specialized benchmarks, crafted through significant manual effort by security experts. However, relying on manually-crafted benchmarks is insufficient in the long term, because benchmarks (i) naturally end up contaminating training data, (ii) must extend to new tasks to provide a more complete picture, and (iii) must increase in difficulty to challenge more capable LLMs. In this work, we address these challenges and present AutoBaxBuilder, a framework that generates tasks and tests for code security benchmarking from scratch. We introduce a robust pipeline with fine-grained plausibility checks, leveraging the code understanding capabilities of LLMs to construct functionality tests and end-to-end security-probing exploits. To confirm the quality of the generated benchmark, we conduct both a qualitative analysis and perform quantitative experiments, comparing it against tasks constructed by human experts. We use AutoBaxBuilder to construct entirely new tasks and release them to the public as AutoBaxBench, together with a thorough evaluation of the security capabilities of LLMs on these tasks. We find that a new task can be generated in under 2 hours, costing less than USD 10.

</details>
