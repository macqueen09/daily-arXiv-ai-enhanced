<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.SE](#cs.SE) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Quantitative Verification of Omega-regular Properties in Probabilistic Programming](https://arxiv.org/abs/2512.21596)
*Peixin Wang,Jianhao Bai,Min Zhang,C. -H. Luke Ong*

Main category: cs.PL

TL;DR: 提出TPI框架，将概率编程与时序逻辑结合，计算满足ω-正则规范的执行轨迹后验分布，并提供严格的概率上下界保证。


<details>
  <summary>Details</summary>
Motivation: 现有概率编程推理技术通常只计算程序终止时的后验分布，无法捕捉概率行为的时序演化，需要统一概率编程与时序逻辑的新框架。

Method: 开发TPI框架，将Rabin接受条件分解为持久性和递归性组件，构建随机屏障证书来严格限定每个组件的概率上下界。

Result: 实现了原型工具TPInfer，在多个基准测试中展示了有效且高效的推理能力，能够处理概率模型中丰富的时序属性。

Conclusion: TPI框架成功统一了概率编程与时序逻辑，为时序后验推理提供了严格的理论保证和实用的实现工具。

Abstract: Probabilistic programming provides a high-level framework for specifying statistical models as executable programs with built-in randomness and conditioning. Existing inference techniques, however, typically compute posterior distributions over program states at fixed time points, most often at termination, thereby failing to capture the temporal evolution of probabilistic behaviors. We introduce temporal posterior inference (TPI), a new framework that unifies probabilistic programming with temporal logic by computing posterior distributions over execution traces that satisfy omega-regular specifications, conditioned on possibly temporal observations. To obtain rigorous quantitative guarantees, we develop a new method for computing upper and lower bounds on the satisfaction probabilities of omega-regular properties. Our approach decomposes Rabin acceptance conditions into persistence and recurrence components and constructs stochastic barrier certificates that soundly bound each component. We implement our approach in a prototype tool, TPInfer, and evaluate it on a suite of benchmarks, demonstrating effective and efficient inference over rich temporal properties in probabilistic models.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [2] [AInsteinBench: Benchmarking Coding Agents on Scientific Repositories](https://arxiv.org/abs/2512.21373)
*Titouan Duston,Shuo Xin,Yang Sun,Daoguang Zan,Aoyan Li,Shulin Xin,Kai Shen,Yixiao Chen,Qiming Sun,Ge Zhang,Jiashuo Liu,Huan Zhou,Jingkai Liu,Zhichen Pu,Yuanheng Wang,Bo-Xuan Ge,Xin Tong,Fei Ye,Zhi-Chao Zhao,Wen-Biao Han,Zhoujian Cao,Yueran Zhao,Weiluo Ren,Qingshen Long,Yuxiao Liu,Anni Huang,Yidi Du,Yuanyuan Rong,Jiahao Peng*

Main category: cs.SE

TL;DR: AInsteinBench是一个用于评估LLM代理在真实科研软件生态系统中作为科学计算开发代理能力的大规模基准测试，基于生产级科学代码库的维护者提交的pull request任务。


<details>
  <summary>Details</summary>
Motivation: 现有科学推理基准主要关注概念知识，而软件工程基准强调通用功能实现和问题解决，缺乏评估模型在端到端科学开发环境中实际能力的基准。

Method: 从六个广泛使用的科学代码库（量子化学、量子计算、分子动力学、数值相对论、流体动力学、化学信息学）中提取维护者提交的pull request任务，通过多阶段筛选和专家评审确保科学挑战性、充分测试覆盖和适当难度。

Result: 创建了一个包含可执行环境评估、科学意义失败模式和测试驱动验证的基准测试，能够衡量模型超越表面代码生成、达到计算科学研究核心能力的能力。

Conclusion: AInsteinBench填补了现有基准的空白，为评估LLM代理在真实科研软件生态系统中的科学计算开发能力提供了标准化测试框架。

Abstract: We introduce AInsteinBench, a large-scale benchmark for evaluating whether large language model (LLM) agents can operate as scientific computing development agents within real research software ecosystems. Unlike existing scientific reasoning benchmarks which focus on conceptual knowledge, or software engineering benchmarks that emphasize generic feature implementation and issue resolving, AInsteinBench evaluates models in end-to-end scientific development settings grounded in production-grade scientific repositories. The benchmark consists of tasks derived from maintainer-authored pull requests across six widely used scientific codebases, spanning quantum chemistry, quantum computing, molecular dynamics, numerical relativity, fluid dynamics, and cheminformatics. All benchmark tasks are carefully curated through multi-stage filtering and expert review to ensure scientific challenge, adequate test coverage, and well-calibrated difficulty. By leveraging evaluation in executable environments, scientifically meaningful failure modes, and test-driven verification, AInsteinBench measures a model's ability to move beyond surface-level code generation toward the core competencies required for computational scientific research.

</details>
