<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 8]
- [cs.CL](#cs.CL) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.DC](#cs.DC) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Symbolic Specification and Reasoning for Quantum Data and Operations](https://arxiv.org/abs/2512.22383)
*Mingsheng Ying*

Main category: cs.PL

TL;DR: 提出了一种名为SOL的符号算子逻辑框架，用于量子数据和操作的符号化规范与推理，将经典一阶逻辑嵌入形式算子语言中，为量子计算的自动化验证提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 量子信息与计算研究中，符号方法已被广泛用于量子态和操作的人工规范与推理，同时也是确保量子算法和程序自动化推理与验证工具可扩展性和效率的关键。然而，目前缺乏关于量子数据和操作的符号化规范与推理的形式化理论，这严重限制了自动化验证技术在量子计算中的实际应用。

Method: 提出了一个通用的逻辑框架SOL（符号算子逻辑），将经典一阶逻辑语言嵌入到用于规范量子数据和操作的形式算子语言中，包括递归定义。这种嵌入允许在底层经典数据（如布尔代数或群论）的选定理论下推理量子操作的属性，从而利用为经典计算开发的现有自动化验证工具。

Result: 建立了一个统一的符号化规范与推理框架，能够处理量子数据和操作，包括递归定义。该框架为量子计算的形式化验证和自动化定理证明提供了概念基础。

Conclusion: SOL框架为量子计算的形式化验证和自动化定理证明（如在Lean、Coq等证明助手中）提供了概念基础，通过将经典一阶逻辑嵌入到符号算子逻辑中，实现了量子计算的符号化方法，有望促进量子算法和程序的自动化验证技术的发展。

Abstract: In quantum information and computation research, symbolic methods have been widely used for human specification and reasoning about quantum states and operations. At the same time, they are essential for ensuring the scalability and efficiency of automated reasoning and verification tools for quantum algorithms and programs. However, a formal theory for symbolic specification and reasoning about quantum data and operations is still lacking, which significantly limits the practical applicability of automated verification techniques in quantum computing.
  In this paper, we present a general logical framework, called Symbolic Operator Logic $\mathbf{SOL}$, which enables symbolic specification and reasoning about quantum data and operations. Within this framework, a classical first-order logical language is embedded into a language of formal operators used to specify quantum data and operations, including their recursive definitions. This embedding allows reasoning about their properties modulo a chosen theory of the underlying classical data (e.g., Boolean algebra or group theory), thereby leveraging existing automated verification tools developed for classical computing. It should be emphasised that this embedding of classical first-order logic into $\mathbf{SOL}$ is precisely what makes the symbolic method possible.
  We envision that this framework can provide a conceptual foundation for the formal verification and automated theorem proving of quantum computation and information in proof assistants such as Lean, Coq, and related systems.

</details>


### [2] [Eliminate Branches by Melding IR Instructions](https://arxiv.org/abs/2512.22390)
*Yuze Li,Srinivasan Ramachandra Sharma,Charitha Saumya,Ali R. Butt,Kirshanthan Sundararajah*

Main category: cs.PL

TL;DR: MERIT是一种编译器转换技术，通过在对齐和融合IR指令层面消除分支，解决数据依赖分支的性能问题，相比硬件分支预测器获得10.9%的几何平均加速。


<details>
  <summary>Details</summary>
Motivation: 现代处理器中分支预测失败会导致严重的性能损失。虽然存在硬件预测器和配置文件引导技术，但具有不规则模式的数据依赖分支仍然具有挑战性。传统的if-conversion通过软件谓词消除分支，但在x86等架构上存在限制，经常在包含内存指令的路径上失败，或者通过完全推测大型分支体产生过多的指令开销。

Method: MERIT是一种编译器转换，通过在对齐和融合相似操作来消除分支。它观察到不同路径通常执行结构相似但操作数不同的操作，采用序列对齐来发现合并机会，并使用安全的操作数级保护来确保语义正确性，无需硬件谓词。该方法实现为LLVM pass。

Result: 在来自四个基准测试套件的102个程序上评估，MERIT实现了10.9%的几何平均加速，相比硬件分支预测器峰值改进达到32倍，展示了在减少静态指令开销方面的有效性。

Conclusion: MERIT通过IR指令级别的对齐和融合，有效消除了分支，解决了传统if-conversion的限制，显著提升了具有数据依赖分支的程序的性能。

Abstract: Branch mispredictions cause catastrophic performance penalties in modern processors, leading to performance loss. While hardware predictors and profile-guided techniques exist, data-dependent branches with irregular patterns remain challenging. Traditional if-conversion eliminates branches via software predication but faces limitations on architectures like x86. It often fails on paths containing memory instructions or incurs excessive instruction overhead by fully speculating large branch bodies.
  This paper presents Melding IR Instructions (MERIT), a compiler transformation that eliminates branches by aligning and melding similar operations from divergent paths at the IR instruction level. By observing that divergent paths often perform structurally similar operations with different operands, MERIT adapts sequence alignment to discover merging opportunities and employs safe operand-level guarding to ensure semantic correctness without hardware predication. Implemented as an LLVM pass and evaluated on 102 programs from four benchmark suites, MERIT achieves a geometric mean speedup of 10.9% with peak improvements of 32x compared to hardware branch predictor, demonstrating the effectiveness with reduced static instruction overhead.

</details>


### [3] [A Bounded Game Semantics Checker for Precise Smart Contract Analysis](https://arxiv.org/abs/2512.22417)
*Vasileios Koutavas,Yu-Yang Lin,Nikos Tzevelekos*

Main category: cs.PL

TL;DR: 基于博弈语义的智能合约漏洞检测方法，通过YulToolkit工具实现，能精确检测重入等漏洞，无假阳性，在真实案例中验证有效。


<details>
  <summary>Details</summary>
Motivation: 智能合约漏洞检测需要高精度（无假阳性）且能扩展到真实合约的方法。现有方法要么精度不足（产生假阳性），要么无法处理真实规模合约。

Method: 基于博弈语义，将计算建模为合约与环境间的交互，将未知或恶意外部合约的推理简化为迹枚举。实现为YulToolkit工具，支持Solidity编写的检测代码传播到Yul中间语言。

Result: 在The DAO、PredyPool、Lendf.Me等真实漏洞案例和基准合约上测试，YulToolkit均能检测到已知漏洞（产生触发违规的迹），修复后无违规报告。

Conclusion: 有界博弈语义探索是智能合约分析工具箱中有效且精确的补充，特别适用于重入等在真实代码中难以精确检测的漏洞类型。

Abstract: We present a new approach to finding smart contract vulnerabilities that is precise (no false positives up to our EVM-Yul interpreter), bounded-complete, and, when instrumented with domain knowledge, scales to real-world contracts. Our method is based on game semantics, modelling computation as an interaction between a contract and its environment, reducing reasoning about unknown or malicious external contracts to trace enumeration. We implement this in a tool we refer to as YulToolkit, a bounded game-semantics checker for Yul, the intermediate language of Solidity. By exploring only feasible interactions, YulToolkit avoids over-approximation, and by relying on the theory of game semantics it achieves bounded completeness. To make exploration tractable, YulToolkit supports instrumentation written in Solidity and propagated to Yul, comparable in effort to creating a test harness. Unlike tests, however, our technique explores all admissible traces within the chosen parameters and bounds. We evaluate YulToolkit on three real-world incidents: The DAO, PredyPool, and Lendf.Me, as well as benchmark contracts. In all cases, YulToolkit detects the known vulnerabilities (producing a violation-triggering trace), and after applying fixes, reports no further violations within bounds. These results show that bounded game semantics exploration is an effective and precise addition to the smart contract analysis toolbox, particularly for vulnerabilities such as reentrancy that are hard to detect precisely in real code.

</details>


### [4] [Compiling Gradual Types with Evidence](https://arxiv.org/abs/2512.22684)
*José Luis Romero,Cristóbal Isla,Matías Toro,Éric Tanter*

Main category: cs.PL

TL;DR: 本文设计并实现了基于证据的编译器GrEv，证明证据语义可以实现高效的类型渐进化实现，性能甚至优于基于强制转换的编译器。


<details>
  <summary>Details</summary>
Motivation: 虽然抽象渐进化类型（AGT）方法在语言设计和语义方面取得了成功，但其基于证据的运行时语义是否能实现高效的渐进化类型实现尚不明确。本文旨在探索证据语义是否可行的高效实现路径。

Method: 设计、实现并评估了基于证据的编译器GrEv，弥合了形式语义与编译器实现之间的差距，并识别了新颖的单调语义。使用Grift基准测试套件进行性能评估。

Result: GrEv编译器在性能上可以与基于强制转换的编译器竞争，甚至更快，在静态到动态的配置谱系中表现出更好的稳定性。

Conclusion: 证据语义可以实现高效的渐进化类型编译器实现，为AGT文献中许多高级渐进化类型系统的高效实现打开了直接通道。

Abstract: Efficiently supporting sound gradual typing in a language with structural types is challenging. To date, the Grift compiler is the only close-to-the-metal implementation of gradual typing in this setting, exploiting coercions for runtime checks, and further extended with monotonic references for efficient access to statically-typed data structures. On the language design and semantics side, the Abstracting Gradual Typing (AGT) methodology has proven fruitful to elucidate existing designs and to innovate by deriving gradualizations of a wide variety of typing disciplines and language features. Grounded in abstract interpretation, the Curry-Howard inspired runtime semantics of AGT is based on the notion of evidence for consistent judgments that evolve during reduction, monitoring the plausibility of well-typedness. While expressive and versatile, it is unclear whether such evidence-based semantics are a viable route to realize an efficient implementation of gradual typing.
  In this work, we explore this question by designing, implementing, and evaluating an evidence-based compiler, called GrEv. We explain how to bridge the gap between the formal semantics and the GrEv compiler implementation, and identify novel monotonic semantics. We empirically evaluate the performance of GrEv on the Grift benchmark suite. The results show that an evidence-based compiler can be competitive with, and even faster than, a coercion-based compiler, exhibiting more stability across configurations on the static-to-dynamic spectrum. In addition to enriching the space of gradual typing compilers, this work opens a direct door to exploring efficient implementations of the many advanced gradual typing disciplines formally derived with AGT in the literature.

</details>


### [5] [Fancy Some Chips for Your TeaStore? Modeling the Control of an Adaptable Discrete System](https://arxiv.org/abs/2512.23496)
*Anna Gallone,Simon Bliudze,Sophie Cerf,Olga Kouchnarenko*

Main category: cs.PL

TL;DR: Chips是一种用于设计复杂系统模型的语言，通过功能块描述应用程序，结合控制理论和通用编程语言概念，生成鲁棒的基于组件的模型。


<details>
  <summary>Details</summary>
Motivation: 开发者在设计新Web应用时需要处理各种资源约束（软件、硬件、网络、在线微服务等），这些实体形成复杂的通信相互依赖系统。确保系统鲁棒性以提供良好服务质量非常重要。

Method: 引入Chips语言，采用功能块形式描述应用程序，混合控制理论和通用编程语言概念，支持系统化设计、建模和分析复杂系统项目。

Result: 使用Adaptable TeaStore应用作为运行示例，展示了如何使用Chips系统化地设计、建模和分析复杂系统项目。

Conclusion: Chips语言有助于简化由各种交织组件组成的模型设计，通过结合控制理论和编程语言概念，能够生成鲁棒的基于组件的系统模型。

Abstract: When designing new web applications, developers must cope with different kinds of constraints relative to the resources they rely on: software, hardware, network, online micro-services, or any combination of the mentioned entities. Together, these entities form a complex system of communicating interdependent processes, physical or logical. It is very desirable that such system ensures its robustness to provide a good quality of service. In this paper we introduce Chips, a language that aims at facilitating the design of models made of various entwined components. It allows the description of applications in the form of functional blocks. Chips mixes notions  from control theory and general purpose programming languages to generate robust component-based models. This paper presents how to use Chips to systematically design, model and analyse a complex system project, using a variation of the Adaptable TeaStore application as running example.

</details>


### [6] [Adaptable TeaStore: A Choreographic Approach](https://arxiv.org/abs/2512.23497)
*Giuseppe De Palma,Saverio Giallorenzo,Ivan Lanese,Gianluigi Zavattaro*

Main category: cs.PL

TL;DR: 使用AIOCJ编排语言实现Adaptable TeaStore，展示动态自适应微服务架构的方法，确保通信正确性


<details>
  <summary>Details</summary>
Motivation: Adaptable TeaStore作为可适应微服务架构的参考模型，需要实现不同配置间的动态转换，同时确保通信的正确性

Method: 基于AIOCJ编排语言实现Adaptable TeaStore，利用编排编程确保通信正确性，支持运行时动态适应

Result: 展示了AIOCJ方法的优势，包括构造性通信正确性保证和运行时适应能力，但也指出了当前局限性

Conclusion: AIOCJ为可适应微服务架构提供了有前景的方法，但需要进一步改进以更好地适应实际云架构需求

Abstract: The Adaptable TeaStore has recently been proposed as a reference model for adaptable microservice architectures. It includes different configurations, as well as scenarios requiring to transition between them. We describe an implementation of the Adaptable TeaStore based on AIOCJ, a choreographic language that allows one to program multiparty systems that can adapt at runtime to different conditions. Following the choreographic tradition, AIOCJ ensures by-construction correctness of communications (e.g., no deadlocks) before, during, and after adaptation. Adaptation is dynamic, and the adaptation scenarios need to be fully specified only at runtime. Using AIOCJ to model the Adaptable TeaStore, we showcase the strengths of the approach and its current limitations, providing suggestions for future directions for refining the paradigm (and the AIOCJ language, in particular), to better align it with real-world Cloud architectures.

</details>


### [7] [Beyond Per-Thread Lock Sets: Multi-Thread Critical Sections and Dynamic Deadlock Prediction](https://arxiv.org/abs/2512.23552)
*Martin Sulzmann*

Main category: cs.PL

TL;DR: 论文提出了一种改进的锁集构造方法，通过跨线程的临界区定义来减少死锁分析中的误报和漏报。


<details>
  <summary>Details</summary>
Motivation: 传统的基于线程的锁集构造只考虑同一线程内获取的锁，忽略了其他线程的锁获取事件，这导致死锁分析中出现误报和漏报。问题的根源在于传统临界区定义忽略了其他线程的事件。

Method: 1. 提出基于迹的临界区表征方法，允许临界区跨越多个线程；2. 通过偏序关系对迹基表征进行可靠近似；3. 开发改进的锁集构造算法，可高效计算；4. 将各种提高精度的锁集构造集成到SPDOffline扩展中。

Result: 1. 改进的锁集构造能够消除DIRK死锁预测器的误报；2. 扩展SPDOffline死锁预测器以减少漏报；3. 扩展版本保持无误报（可靠性），同时减少漏报（更完整）；4. 在标准基准测试套件上性能不受影响。

Conclusion: 提出的跨线程临界区定义是自然且正确的，基于此的改进锁集构造方法能够有效提高死锁分析的准确性，同时保持计算效率和性能。

Abstract: Lock sets are commonly used for dynamic analysis of deadlocks. The standard per-thread lock set construction only considers locks acquired in the same thread, but is unaware of locks acquired in another thread. This leads to false positives and false negatives. The underlying issue is that the commonly used notion of a critical section on which the lock set construction relies ignores events from other threads. We give a trace-based characterization of critical sections that drops this restriction. Critical sections are no longer restricted to a single thread and can cover multiple threads. Such forms of critical sections exist, are natural, and correct the standard formulation.
  We show how to soundly approximate the trace-based characterization via partial order relations. Thus, we obtain an improved lock set construction that can still be efficiently computed and allows us to remove false positives reported by the DIRK deadlock predictor and remove false negatives by extending the SPDOffline deadlock predictor. We integrate various lock set constructions with increased precision in an extension of SPDOffline. Our extensions remain sound (no false positives) but are more complete (fewer false negatives) w.r.t. SPDOffline. For an extensive standard benchmark suite we can also show that the performance is not affected.

</details>


### [8] [Automating the Analysis of Parsing Algorithms (and other Dynamic Programs)](https://arxiv.org/abs/2512.23665)
*Tim Vieira,Ryan Cotterell,Jason Eisner*

Main category: cs.PL

TL;DR: 开发一个帮助程序员分析NLP算法复杂度的系统，能推断类型、死代码、冗余代码以及参数化运行时和空间复杂度边界


<details>
  <summary>Details</summary>
Motivation: NLP算法研究通常需要高效操作复杂的形式结构，算法设计者需要为其算法提供保证（如运行时间或空间复杂度的上界），并确定算法导出量的必要属性以合成高效数据结构和验证类型错误

Method: 开发一个系统来帮助程序员执行这些类型的分析，具体方法未在摘要中详细说明

Result: 将该系统应用于多个NLP算法，成功推断出类型、死代码、冗余代码以及参数化运行时和空间复杂度边界

Conclusion: 该系统能有效帮助程序员分析NLP算法的复杂度特性，为算法设计提供支持

Abstract: Much algorithmic research in NLP aims to efficiently manipulate rich formal structures. An algorithm designer typically seeks to provide guarantees about their proposed algorithm -- for example, that its running time or space complexity is upper-bounded as a certain function of its input size. They may also wish to determine the necessary properties of the quantities derived by the algorithm to synthesize efficient data structures and verify type errors. In this paper, we develop a system for helping programmers to perform these types of analyses. We apply our system to a number of NLP algorithms and find that it successfully infers types, dead and redundant code, and parametric runtime and space complexity bounds.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [9] [Anka: A Domain-Specific Language for Reliable LLM Code Generation](https://arxiv.org/abs/2512.23214)
*Saif Khalfan Saif Al Mazrouei*

Main category: cs.CL

TL;DR: LLMs在复杂编程任务中常出错，作者假设是通用语言的灵活性导致。他们设计了Anka DSL，约束语法减少歧义，使LLM在未训练的情况下达到高准确率，尤其在多步骤任务上显著优于Python。


<details>
  <summary>Details</summary>
Motivation: LLMs在代码生成方面表现出色，但在复杂、多步骤编程任务中会出现系统性错误。作者假设这些错误源于通用语言的灵活性，允许多种有效方法且需要隐式状态管理，导致歧义和错误。

Method: 引入Anka，一种用于数据转换管道的领域特定语言（DSL），设计具有明确、约束的语法以减少代码生成中的歧义。在100个基准问题上测试Claude 3.5 Haiku和GPT-4o-mini，尽管LLMs之前从未接触过Anka。

Result: Claude 3.5 Haiku在Anka上达到99.9%解析成功率和95.8%总体任务准确率。在多步骤管道任务上，Anka比Python有40个百分点的准确率优势（100% vs. 60%）。GPT-4o-mini也确认了这一优势（多步骤任务上+26.7个百分点）。

Conclusion: LLMs可以从上下文提示中完全学习新的DSL，达到接近原生的准确率；约束语法显著减少复杂任务中的错误；为LLM生成专门设计的领域特定语言可以优于LLM有广泛训练的通用语言。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in code generation, yet they exhibit systematic errors on complex, multi-step programming tasks. We hypothesize that these errors stem from the flexibility of general-purpose languages, which permits multiple valid approaches and requires implicit state management. To test this hypothesis, we introduce Anka, a domain-specific language (DSL) for data transformation pipelines designed with explicit, constrained syntax that reduces ambiguity in code generation. Despite having zero prior training exposure to Anka, Claude 3.5 Haiku achieves 99.9% parse success and 95.8% overall task accuracy across 100 benchmark problems. Critically, Anka demonstrates a 40 percentage point accuracy advantage over Python on multi-step pipeline tasks (100% vs. 60%), where Python's flexible syntax leads to frequent errors in operation sequencing and variable management. Cross-model validation with GPT-4o-mini confirms this advantage (+26.7 percentage points on multi-step tasks). Our results demonstrate that: (1) LLMs can learn novel DSLs entirely from in-context prompts, achieving near-native accuracy; (2) constrained syntax significantly reduces errors on complex tasks; and (3) domain-specific languages purposefully designed for LLM generation can outperform general-purpose languages on which the LLM has extensive training. We release the complete language implementation, benchmark suite, and evaluation framework to facilitate further research.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [10] [Verifying Asynchronous Hyperproperties in Reactive Systems](https://arxiv.org/abs/2512.23344)
*Raven Beutner,Bernd Finkbeiner*

Main category: cs.LO

TL;DR: 提出基于博弈的方法验证异步HyperLTL（A-HLTL）的∀*∃*公式，解决异步超属性模型检测问题


<details>
  <summary>Details</summary>
Motivation: 现有HyperLTL等逻辑同步比较执行轨迹，无法表达异步超属性（如观测确定性、非推断等），需要能处理异步比较的验证方法

Method: 将验证问题建模为验证者与反驳者之间的博弈，双方分别控制轨迹和停顿部分，验证者获胜策略对应存在量词轨迹的具体见证和异步对齐

Result: 识别出博弈解释完备的片段，从而构成有限状态决策过程，能够处理任意∀*∃* A-HLTL公式在反应式系统中的验证

Conclusion: 提出的博弈方法为异步超属性验证提供了新途径，特别适用于处理存在量词停顿的异步比较问题

Abstract: Hyperproperties are system properties that relate multiple execution traces and commonly occur when specifying information-flow and security policies. Logics like HyperLTL utilize explicit quantification over execution traces to express temporal hyperproperties in reactive systems, i.e., hyperproperties that reason about the temporal behavior along infinite executions. An often unwanted side-effect of such logics is that they compare the quantified traces synchronously. This prohibits the logics from expressing properties that compare multiple traces asynchronously, such as Zdancewic and Myers's observational determinism, McLean's non-inference, or stuttering refinement. We study the model-checking problem for a variant of asynchronous HyperLTL (A-HLTL), a temporal logic that can express hyperproperties where multiple traces are compared across timesteps. In addition to quantifying over system traces, A-HLTL features secondary quantification over stutterings of these traces. Consequently, A-HLTL allows for a succinct specification of many widely used asynchronous hyperproperties. Model-checking A-HLTL requires finding suitable stutterings, which, thus far, has been only possible for very restricted fragments or terminating systems. In this paper, we propose a novel game-based approach for the verification of arbitrary $\forall^*\exists^*$ A-HLTL formulas in reactive systems. In our method, we consider the verification as a game played between a verifier and a refuter, who challenge each other by controlling parts of the underlying traces and stutterings. A winning strategy for the verifier then corresponds to concrete witnesses for existentially quantified traces and asynchronous alignments for existentially quantified stutterings. We identify fragments for which our game-based interpretation is complete and thus constitutes a finite-state decision procedure.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [11] [TL: Automatic End-to-End Compiler of Tile-Based Languages for Spatial Dataflow Architectures](https://arxiv.org/abs/2512.22168)
*Wei Li,Zhenyu Bai,Heru Wang,Pranav Dangi,Zhiqiang Zhang,Cheng Tan,Huiying Lan,Weng-Fai Wong,Tulika Mitra*

Main category: cs.DC

TL;DR: TL是一个端到端框架，可将基于tile的程序（如Triton内核）编译到空间数据流架构上，解决tile实例在分布式核心间的分配问题，利用片上网络和分布式内存提高数据重用和减少通信。


<details>
  <summary>Details</summary>
Motivation: 空间数据流加速器通过显式的编译器管理数据移动和片上网络组织计算，减少对高延迟、带宽受限的全局共享内存的依赖，提供更高的吞吐量和效率。然而，其端到端性能严重依赖于工作负载到硬件的映射方式，而现有空间数据流加速器的有限可编程性阻碍了其更广泛采用。

Method: TL提出了一种硬件表示方法，捕捉互连拓扑、内存层次结构和计算能力，支持特定架构优化和多样化空间数据流目标。它基于MLIR生态系统构建，定义了不同前端的通用入口点和不同后端的终点，专注于在分布式核心间分配tile实例，并利用片上网络和分布式内存来增加数据重用和减少通信。

Result: 论文没有提供具体的实验结果，但从描述来看，TL框架能够解决空间数据流加速器的可编程性问题，通过编译器优化实现更好的工作负载映射，从而提高性能和效率。

Conclusion: TL框架通过端到端的编译方法解决了空间数据流加速器的可编程性挑战，使开发者能够更轻松地将基于tile的程序映射到空间数据流架构上，有望促进这类加速器的更广泛采用。

Abstract: Spatial dataflow accelerators are a promising direction for next-generation computer systems because they can reduce the memory bottlenecks of traditional von Neumann machines such as CPUs and GPUs. They do so by organizing computation around explicit, compiler-managed data movement over the on-chip network, allowing operands to be directly forwarded between processing elements and reducing reliance on high-latency, bandwidth-limited global shared memory. Such localized communications can provide higher throughput and efficiency compared to repeated off-chip memory accesses. However, their end-to-end performance depends strongly on how workloads are mapped to the hardware. Naive mappings can perform very poorly, and most users rely on hand-tuned vendor libraries. In practice, although existing spatial-dataflow accelerators have strong potential for high performance, energy- and cost-efficiency, their limited programmability remains a major barrier to their wider adoption. This paper presents TL, an end-to-end framework that compiles tile-based programs (such as Triton kernels) onto spatial dataflow architectures. Unlike most existing compiler frameworks that focus on optimizing code generation within a single tile, TL addresses the central challenge of distributing tile instances across spatially distributed cores and exploiting the on-chip network and distributed memories to increase data reuse and reduce communications. TL proposes a hardware representation that captures interconnect topology, memory hierarchy, and compute capabilities, enabling both specialized architecture-specific optimizations and support for diverse spatial dataflow targets. TL is built on the MLIR ecosystem and defines a generic entry point for different front-ends and an end point for different back-ends.

</details>


### [12] [Mirage Persistent Kernel: A Compiler and Runtime for Mega-Kernelizing Tensor Programs](https://arxiv.org/abs/2512.22219)
*Xinhao Cheng,Zhihao Zhang,Yu Zhou,Jianan Ji,Jinchen Jiang,Zepeng Zhao,Ziruo Xiao,Zihao Ye,Yingyi Huang,Ruihang Lai,Hongyi Jin,Bohan Hou,Mengdi Wu,Yixin Dong,Anthony Yip,Zihao Ye,Songting Wang,Wenqin Yang,Xupeng Miao,Tianqi Chen,Zhihao Jia*

Main category: cs.DC

TL;DR: MPK是首个自动将多GPU模型推理转换为单个高性能megakernel的编译器和运行时系统，通过SM级图表示实现跨算子软件流水线等优化，显著提升LLM推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理系统通常采用算子级内核执行，存在内核启动开销大、SM利用率低、跨算子优化困难等问题，无法充分利用GPU硬件潜力。

Method: 提出SM级图表示捕获数据依赖，MPK编译器将张量程序转换为SM级任务图并生成优化CUDA实现，MPK内核内并行运行时在单个megakernel内通过去中心化调度执行任务。

Result: MPK显著优于现有算子级LLM服务系统，端到端推理延迟降低高达1.7倍，将LLM推理性能推向硬件极限。

Conclusion: MPK实现了端到端内核融合，以最小开发工作量提供高性能LLM推理，同时保持现有编程模型的灵活性，是首个自动生成megakernel的编译器和运行时系统。

Abstract: We introduce Mirage Persistent Kernel (MPK), the first compiler and runtime system that automatically transforms multi-GPU model inference into a single high-performance megakernel. MPK introduces an SM-level graph representation that captures data dependencies at the granularity of individual streaming multiprocessors (SMs), enabling cross-operator software pipelining, fine-grained kernel overlap, and other previously infeasible GPU optimizations. The MPK compiler lowers tensor programs into highly optimized SM-level task graphs and generates optimized CUDA implementations for all tasks, while the MPK in-kernel parallel runtime executes these tasks within a single mega-kernel using decentralized scheduling across SMs. Together, these components provide end-to-end kernel fusion with minimal developer effort, while preserving the flexibility of existing programming models. Our evaluation shows that MPK significantly outperforms existing kernel-per-operator LLM serving systems by reducing end-to-end inference latency by up to 1.7x, pushing LLM inference performance close to hardware limits. MPK is publicly available at https://github.com/mirage-project/mirage.

</details>
