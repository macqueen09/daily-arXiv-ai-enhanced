{"id": "2512.22383", "pdf": "https://arxiv.org/pdf/2512.22383", "abs": "https://arxiv.org/abs/2512.22383", "authors": ["Mingsheng Ying"], "title": "Symbolic Specification and Reasoning for Quantum Data and Operations", "categories": ["cs.PL", "cs.LO", "quant-ph"], "comment": null, "summary": "In quantum information and computation research, symbolic methods have been widely used for human specification and reasoning about quantum states and operations. At the same time, they are essential for ensuring the scalability and efficiency of automated reasoning and verification tools for quantum algorithms and programs. However, a formal theory for symbolic specification and reasoning about quantum data and operations is still lacking, which significantly limits the practical applicability of automated verification techniques in quantum computing.\n  In this paper, we present a general logical framework, called Symbolic Operator Logic $\\mathbf{SOL}$, which enables symbolic specification and reasoning about quantum data and operations. Within this framework, a classical first-order logical language is embedded into a language of formal operators used to specify quantum data and operations, including their recursive definitions. This embedding allows reasoning about their properties modulo a chosen theory of the underlying classical data (e.g., Boolean algebra or group theory), thereby leveraging existing automated verification tools developed for classical computing. It should be emphasised that this embedding of classical first-order logic into $\\mathbf{SOL}$ is precisely what makes the symbolic method possible.\n  We envision that this framework can provide a conceptual foundation for the formal verification and automated theorem proving of quantum computation and information in proof assistants such as Lean, Coq, and related systems."}
{"id": "2512.22390", "pdf": "https://arxiv.org/pdf/2512.22390", "abs": "https://arxiv.org/abs/2512.22390", "authors": ["Yuze Li", "Srinivasan Ramachandra Sharma", "Charitha Saumya", "Ali R. Butt", "Kirshanthan Sundararajah"], "title": "Eliminate Branches by Melding IR Instructions", "categories": ["cs.PL"], "comment": null, "summary": "Branch mispredictions cause catastrophic performance penalties in modern processors, leading to performance loss. While hardware predictors and profile-guided techniques exist, data-dependent branches with irregular patterns remain challenging. Traditional if-conversion eliminates branches via software predication but faces limitations on architectures like x86. It often fails on paths containing memory instructions or incurs excessive instruction overhead by fully speculating large branch bodies.\n  This paper presents Melding IR Instructions (MERIT), a compiler transformation that eliminates branches by aligning and melding similar operations from divergent paths at the IR instruction level. By observing that divergent paths often perform structurally similar operations with different operands, MERIT adapts sequence alignment to discover merging opportunities and employs safe operand-level guarding to ensure semantic correctness without hardware predication. Implemented as an LLVM pass and evaluated on 102 programs from four benchmark suites, MERIT achieves a geometric mean speedup of 10.9% with peak improvements of 32x compared to hardware branch predictor, demonstrating the effectiveness with reduced static instruction overhead."}
{"id": "2512.22417", "pdf": "https://arxiv.org/pdf/2512.22417", "abs": "https://arxiv.org/abs/2512.22417", "authors": ["Vasileios Koutavas", "Yu-Yang Lin", "Nikos Tzevelekos"], "title": "A Bounded Game Semantics Checker for Precise Smart Contract Analysis", "categories": ["cs.PL"], "comment": "21 pages, 2 figures, 4 tables", "summary": "We present a new approach to finding smart contract vulnerabilities that is precise (no false positives up to our EVM-Yul interpreter), bounded-complete, and, when instrumented with domain knowledge, scales to real-world contracts. Our method is based on game semantics, modelling computation as an interaction between a contract and its environment, reducing reasoning about unknown or malicious external contracts to trace enumeration. We implement this in a tool we refer to as YulToolkit, a bounded game-semantics checker for Yul, the intermediate language of Solidity. By exploring only feasible interactions, YulToolkit avoids over-approximation, and by relying on the theory of game semantics it achieves bounded completeness. To make exploration tractable, YulToolkit supports instrumentation written in Solidity and propagated to Yul, comparable in effort to creating a test harness. Unlike tests, however, our technique explores all admissible traces within the chosen parameters and bounds. We evaluate YulToolkit on three real-world incidents: The DAO, PredyPool, and Lendf.Me, as well as benchmark contracts. In all cases, YulToolkit detects the known vulnerabilities (producing a violation-triggering trace), and after applying fixes, reports no further violations within bounds. These results show that bounded game semantics exploration is an effective and precise addition to the smart contract analysis toolbox, particularly for vulnerabilities such as reentrancy that are hard to detect precisely in real code."}
{"id": "2512.22684", "pdf": "https://arxiv.org/pdf/2512.22684", "abs": "https://arxiv.org/abs/2512.22684", "authors": ["José Luis Romero", "Cristóbal Isla", "Matías Toro", "Éric Tanter"], "title": "Compiling Gradual Types with Evidence", "categories": ["cs.PL"], "comment": "Submitted to TOPLAS", "summary": "Efficiently supporting sound gradual typing in a language with structural types is challenging. To date, the Grift compiler is the only close-to-the-metal implementation of gradual typing in this setting, exploiting coercions for runtime checks, and further extended with monotonic references for efficient access to statically-typed data structures. On the language design and semantics side, the Abstracting Gradual Typing (AGT) methodology has proven fruitful to elucidate existing designs and to innovate by deriving gradualizations of a wide variety of typing disciplines and language features. Grounded in abstract interpretation, the Curry-Howard inspired runtime semantics of AGT is based on the notion of evidence for consistent judgments that evolve during reduction, monitoring the plausibility of well-typedness. While expressive and versatile, it is unclear whether such evidence-based semantics are a viable route to realize an efficient implementation of gradual typing.\n  In this work, we explore this question by designing, implementing, and evaluating an evidence-based compiler, called GrEv. We explain how to bridge the gap between the formal semantics and the GrEv compiler implementation, and identify novel monotonic semantics. We empirically evaluate the performance of GrEv on the Grift benchmark suite. The results show that an evidence-based compiler can be competitive with, and even faster than, a coercion-based compiler, exhibiting more stability across configurations on the static-to-dynamic spectrum. In addition to enriching the space of gradual typing compilers, this work opens a direct door to exploring efficient implementations of the many advanced gradual typing disciplines formally derived with AGT in the literature."}
{"id": "2512.23496", "pdf": "https://arxiv.org/pdf/2512.23496", "abs": "https://arxiv.org/abs/2512.23496", "authors": ["Anna Gallone", "Simon Bliudze", "Sophie Cerf", "Olga Kouchnarenko"], "title": "Fancy Some Chips for Your TeaStore? Modeling the Control of an Adaptable Discrete System", "categories": ["cs.PL", "cs.DC"], "comment": "In Proceedings WACA 2025, arXiv:2512.22054. This work was supported by the ANR grant ANR-23-CE25-0004 (ADAPT). O. Kouchnarenko was supported by the EIPHI Graduate School (grant number ANR-17-EURE-0002)", "summary": "When designing new web applications, developers must cope with different kinds of constraints relative to the resources they rely on: software, hardware, network, online micro-services, or any combination of the mentioned entities. Together, these entities form a complex system of communicating interdependent processes, physical or logical. It is very desirable that such system ensures its robustness to provide a good quality of service. In this paper we introduce Chips, a language that aims at facilitating the design of models made of various entwined components. It allows the description of applications in the form of functional blocks. Chips mixes notions  from control theory and general purpose programming languages to generate robust component-based models. This paper presents how to use Chips to systematically design, model and analyse a complex system project, using a variation of the Adaptable TeaStore application as running example."}
{"id": "2512.23497", "pdf": "https://arxiv.org/pdf/2512.23497", "abs": "https://arxiv.org/abs/2512.23497", "authors": ["Giuseppe De Palma", "Saverio Giallorenzo", "Ivan Lanese", "Gianluigi Zavattaro"], "title": "Adaptable TeaStore: A Choreographic Approach", "categories": ["cs.PL", "cs.SE"], "comment": "In Proceedings WACA 2025, arXiv:2512.22054", "summary": "The Adaptable TeaStore has recently been proposed as a reference model for adaptable microservice architectures. It includes different configurations, as well as scenarios requiring to transition between them. We describe an implementation of the Adaptable TeaStore based on AIOCJ, a choreographic language that allows one to program multiparty systems that can adapt at runtime to different conditions. Following the choreographic tradition, AIOCJ ensures by-construction correctness of communications (e.g., no deadlocks) before, during, and after adaptation. Adaptation is dynamic, and the adaptation scenarios need to be fully specified only at runtime. Using AIOCJ to model the Adaptable TeaStore, we showcase the strengths of the approach and its current limitations, providing suggestions for future directions for refining the paradigm (and the AIOCJ language, in particular), to better align it with real-world Cloud architectures."}
{"id": "2512.23552", "pdf": "https://arxiv.org/pdf/2512.23552", "abs": "https://arxiv.org/abs/2512.23552", "authors": ["Martin Sulzmann"], "title": "Beyond Per-Thread Lock Sets: Multi-Thread Critical Sections and Dynamic Deadlock Prediction", "categories": ["cs.PL", "cs.SE"], "comment": null, "summary": "Lock sets are commonly used for dynamic analysis of deadlocks. The standard per-thread lock set construction only considers locks acquired in the same thread, but is unaware of locks acquired in another thread. This leads to false positives and false negatives. The underlying issue is that the commonly used notion of a critical section on which the lock set construction relies ignores events from other threads. We give a trace-based characterization of critical sections that drops this restriction. Critical sections are no longer restricted to a single thread and can cover multiple threads. Such forms of critical sections exist, are natural, and correct the standard formulation.\n  We show how to soundly approximate the trace-based characterization via partial order relations. Thus, we obtain an improved lock set construction that can still be efficiently computed and allows us to remove false positives reported by the DIRK deadlock predictor and remove false negatives by extending the SPDOffline deadlock predictor. We integrate various lock set constructions with increased precision in an extension of SPDOffline. Our extensions remain sound (no false positives) but are more complete (fewer false negatives) w.r.t. SPDOffline. For an extensive standard benchmark suite we can also show that the performance is not affected."}
{"id": "2512.23665", "pdf": "https://arxiv.org/pdf/2512.23665", "abs": "https://arxiv.org/abs/2512.23665", "authors": ["Tim Vieira", "Ryan Cotterell", "Jason Eisner"], "title": "Automating the Analysis of Parsing Algorithms (and other Dynamic Programs)", "categories": ["cs.PL"], "comment": "2021 manuscript; accepted by TACL but not revised for publication", "summary": "Much algorithmic research in NLP aims to efficiently manipulate rich formal structures. An algorithm designer typically seeks to provide guarantees about their proposed algorithm -- for example, that its running time or space complexity is upper-bounded as a certain function of its input size. They may also wish to determine the necessary properties of the quantities derived by the algorithm to synthesize efficient data structures and verify type errors. In this paper, we develop a system for helping programmers to perform these types of analyses. We apply our system to a number of NLP algorithms and find that it successfully infers types, dead and redundant code, and parametric runtime and space complexity bounds."}
{"id": "2512.22168", "pdf": "https://arxiv.org/pdf/2512.22168", "abs": "https://arxiv.org/abs/2512.22168", "authors": ["Wei Li", "Zhenyu Bai", "Heru Wang", "Pranav Dangi", "Zhiqiang Zhang", "Cheng Tan", "Huiying Lan", "Weng-Fai Wong", "Tulika Mitra"], "title": "TL: Automatic End-to-End Compiler of Tile-Based Languages for Spatial Dataflow Architectures", "categories": ["cs.DC", "cs.PL"], "comment": null, "summary": "Spatial dataflow accelerators are a promising direction for next-generation computer systems because they can reduce the memory bottlenecks of traditional von Neumann machines such as CPUs and GPUs. They do so by organizing computation around explicit, compiler-managed data movement over the on-chip network, allowing operands to be directly forwarded between processing elements and reducing reliance on high-latency, bandwidth-limited global shared memory. Such localized communications can provide higher throughput and efficiency compared to repeated off-chip memory accesses. However, their end-to-end performance depends strongly on how workloads are mapped to the hardware. Naive mappings can perform very poorly, and most users rely on hand-tuned vendor libraries. In practice, although existing spatial-dataflow accelerators have strong potential for high performance, energy- and cost-efficiency, their limited programmability remains a major barrier to their wider adoption. This paper presents TL, an end-to-end framework that compiles tile-based programs (such as Triton kernels) onto spatial dataflow architectures. Unlike most existing compiler frameworks that focus on optimizing code generation within a single tile, TL addresses the central challenge of distributing tile instances across spatially distributed cores and exploiting the on-chip network and distributed memories to increase data reuse and reduce communications. TL proposes a hardware representation that captures interconnect topology, memory hierarchy, and compute capabilities, enabling both specialized architecture-specific optimizations and support for diverse spatial dataflow targets. TL is built on the MLIR ecosystem and defines a generic entry point for different front-ends and an end point for different back-ends."}
{"id": "2512.22219", "pdf": "https://arxiv.org/pdf/2512.22219", "abs": "https://arxiv.org/abs/2512.22219", "authors": ["Xinhao Cheng", "Zhihao Zhang", "Yu Zhou", "Jianan Ji", "Jinchen Jiang", "Zepeng Zhao", "Ziruo Xiao", "Zihao Ye", "Yingyi Huang", "Ruihang Lai", "Hongyi Jin", "Bohan Hou", "Mengdi Wu", "Yixin Dong", "Anthony Yip", "Zihao Ye", "Songting Wang", "Wenqin Yang", "Xupeng Miao", "Tianqi Chen", "Zhihao Jia"], "title": "Mirage Persistent Kernel: A Compiler and Runtime for Mega-Kernelizing Tensor Programs", "categories": ["cs.DC", "cs.LG", "cs.PL"], "comment": null, "summary": "We introduce Mirage Persistent Kernel (MPK), the first compiler and runtime system that automatically transforms multi-GPU model inference into a single high-performance megakernel. MPK introduces an SM-level graph representation that captures data dependencies at the granularity of individual streaming multiprocessors (SMs), enabling cross-operator software pipelining, fine-grained kernel overlap, and other previously infeasible GPU optimizations. The MPK compiler lowers tensor programs into highly optimized SM-level task graphs and generates optimized CUDA implementations for all tasks, while the MPK in-kernel parallel runtime executes these tasks within a single mega-kernel using decentralized scheduling across SMs. Together, these components provide end-to-end kernel fusion with minimal developer effort, while preserving the flexibility of existing programming models. Our evaluation shows that MPK significantly outperforms existing kernel-per-operator LLM serving systems by reducing end-to-end inference latency by up to 1.7x, pushing LLM inference performance close to hardware limits. MPK is publicly available at https://github.com/mirage-project/mirage."}
{"id": "2512.23214", "pdf": "https://arxiv.org/pdf/2512.23214", "abs": "https://arxiv.org/abs/2512.23214", "authors": ["Saif Khalfan Saif Al Mazrouei"], "title": "Anka: A Domain-Specific Language for Reliable LLM Code Generation", "categories": ["cs.CL", "cs.LG", "cs.PL", "cs.SE"], "comment": "11 pages, 1 figure, 4 tables. Code and benchmarks available at https://github.com/BleBlo/Anka", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in code generation, yet they exhibit systematic errors on complex, multi-step programming tasks. We hypothesize that these errors stem from the flexibility of general-purpose languages, which permits multiple valid approaches and requires implicit state management. To test this hypothesis, we introduce Anka, a domain-specific language (DSL) for data transformation pipelines designed with explicit, constrained syntax that reduces ambiguity in code generation. Despite having zero prior training exposure to Anka, Claude 3.5 Haiku achieves 99.9% parse success and 95.8% overall task accuracy across 100 benchmark problems. Critically, Anka demonstrates a 40 percentage point accuracy advantage over Python on multi-step pipeline tasks (100% vs. 60%), where Python's flexible syntax leads to frequent errors in operation sequencing and variable management. Cross-model validation with GPT-4o-mini confirms this advantage (+26.7 percentage points on multi-step tasks). Our results demonstrate that: (1) LLMs can learn novel DSLs entirely from in-context prompts, achieving near-native accuracy; (2) constrained syntax significantly reduces errors on complex tasks; and (3) domain-specific languages purposefully designed for LLM generation can outperform general-purpose languages on which the LLM has extensive training. We release the complete language implementation, benchmark suite, and evaluation framework to facilitate further research."}
{"id": "2512.23344", "pdf": "https://arxiv.org/pdf/2512.23344", "abs": "https://arxiv.org/abs/2512.23344", "authors": ["Raven Beutner", "Bernd Finkbeiner"], "title": "Verifying Asynchronous Hyperproperties in Reactive Systems", "categories": ["cs.LO", "cs.FL", "cs.PL"], "comment": "OOPSLA 2025", "summary": "Hyperproperties are system properties that relate multiple execution traces and commonly occur when specifying information-flow and security policies. Logics like HyperLTL utilize explicit quantification over execution traces to express temporal hyperproperties in reactive systems, i.e., hyperproperties that reason about the temporal behavior along infinite executions. An often unwanted side-effect of such logics is that they compare the quantified traces synchronously. This prohibits the logics from expressing properties that compare multiple traces asynchronously, such as Zdancewic and Myers's observational determinism, McLean's non-inference, or stuttering refinement. We study the model-checking problem for a variant of asynchronous HyperLTL (A-HLTL), a temporal logic that can express hyperproperties where multiple traces are compared across timesteps. In addition to quantifying over system traces, A-HLTL features secondary quantification over stutterings of these traces. Consequently, A-HLTL allows for a succinct specification of many widely used asynchronous hyperproperties. Model-checking A-HLTL requires finding suitable stutterings, which, thus far, has been only possible for very restricted fragments or terminating systems. In this paper, we propose a novel game-based approach for the verification of arbitrary $\\forall^*\\exists^*$ A-HLTL formulas in reactive systems. In our method, we consider the verification as a game played between a verifier and a refuter, who challenge each other by controlling parts of the underlying traces and stutterings. A winning strategy for the verifier then corresponds to concrete witnesses for existentially quantified traces and asynchronous alignments for existentially quantified stutterings. We identify fragments for which our game-based interpretation is complete and thus constitutes a finite-state decision procedure."}
