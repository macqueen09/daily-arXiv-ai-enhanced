<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 8]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.CL](#cs.CL) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Symbolic Specification and Reasoning for Quantum Data and Operations](https://arxiv.org/abs/2512.22383)
*Mingsheng Ying*

Main category: cs.PL

TL;DR: 提出了一种称为符号算子逻辑（SOL）的通用逻辑框架，用于量子数据和操作的符号化规范与推理，将经典一阶逻辑嵌入到形式算子语言中，为量子计算的自动化验证提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 量子信息与计算研究中，符号方法已被广泛用于量子态和操作的人工规范与推理，同时也是确保量子算法和程序自动化推理与验证工具可扩展性和效率的关键。然而，目前缺乏关于量子数据和操作的符号化规范与推理的形式化理论，这严重限制了自动化验证技术在量子计算中的实际应用。

Method: 提出了符号算子逻辑（SOL）这一通用逻辑框架，将经典一阶逻辑语言嵌入到用于规范量子数据和操作的形式算子语言中，包括递归定义。这种嵌入允许在底层经典数据（如布尔代数或群论）的选定理论下推理其性质，从而利用为经典计算开发的现有自动化验证工具。

Result: 建立了一个能够支持量子数据和操作符号化规范与推理的逻辑框架，为量子计算的形式化验证和自动化定理证明提供了概念基础。

Conclusion: SOL框架为量子计算和信息的形式化验证提供了理论基础，有望在Lean、Coq等证明助手中支持量子计算的自动化定理证明，填补了量子计算领域符号化规范与推理的形式化理论空白。

Abstract: In quantum information and computation research, symbolic methods have been widely used for human specification and reasoning about quantum states and operations. At the same time, they are essential for ensuring the scalability and efficiency of automated reasoning and verification tools for quantum algorithms and programs. However, a formal theory for symbolic specification and reasoning about quantum data and operations is still lacking, which significantly limits the practical applicability of automated verification techniques in quantum computing.
  In this paper, we present a general logical framework, called Symbolic Operator Logic $\mathbf{SOL}$, which enables symbolic specification and reasoning about quantum data and operations. Within this framework, a classical first-order logical language is embedded into a language of formal operators used to specify quantum data and operations, including their recursive definitions. This embedding allows reasoning about their properties modulo a chosen theory of the underlying classical data (e.g., Boolean algebra or group theory), thereby leveraging existing automated verification tools developed for classical computing. It should be emphasised that this embedding of classical first-order logic into $\mathbf{SOL}$ is precisely what makes the symbolic method possible.
  We envision that this framework can provide a conceptual foundation for the formal verification and automated theorem proving of quantum computation and information in proof assistants such as Lean, Coq, and related systems.

</details>


### [2] [Eliminate Branches by Melding IR Instructions](https://arxiv.org/abs/2512.22390)
*Yuze Li,Srinivasan Ramachandra Sharma,Charitha Saumya,Ali R. Butt,Kirshanthan Sundararajah*

Main category: cs.PL

TL;DR: MERIT是一种编译器转换技术，通过对齐和融合不同分支路径中的相似操作来消除分支，避免分支预测错误带来的性能损失。


<details>
  <summary>Details</summary>
Motivation: 现代处理器中分支预测错误会导致严重的性能损失。虽然存在硬件预测器和配置文件引导技术，但具有不规则模式的数据依赖分支仍然具有挑战性。传统的if-conversion通过软件谓词消除分支，但在x86等架构上存在局限性，经常在包含内存指令的路径上失败，或者因完全推测大型分支体而产生过多的指令开销。

Method: MERIT是一种编译器转换，通过在IR指令级别对齐和融合不同路径中的相似操作来消除分支。它观察到不同路径通常执行结构相似但操作数不同的操作，采用序列对齐来发现融合机会，并使用安全的操作数级别保护来确保语义正确性，无需硬件谓词。该方法作为LLVM pass实现。

Result: 在来自四个基准测试套件的102个程序上评估，MERIT实现了10.9%的几何平均加速比，与硬件分支预测器相比峰值改进达到32倍，证明了其有效性且减少了静态指令开销。

Conclusion: MERIT通过编译器级别的分支消除技术，有效解决了数据依赖分支的预测问题，相比传统方法具有更好的性能和更低的指令开销，为处理不规则分支模式提供了有效的解决方案。

Abstract: Branch mispredictions cause catastrophic performance penalties in modern processors, leading to performance loss. While hardware predictors and profile-guided techniques exist, data-dependent branches with irregular patterns remain challenging. Traditional if-conversion eliminates branches via software predication but faces limitations on architectures like x86. It often fails on paths containing memory instructions or incurs excessive instruction overhead by fully speculating large branch bodies.
  This paper presents Melding IR Instructions (MERIT), a compiler transformation that eliminates branches by aligning and melding similar operations from divergent paths at the IR instruction level. By observing that divergent paths often perform structurally similar operations with different operands, MERIT adapts sequence alignment to discover merging opportunities and employs safe operand-level guarding to ensure semantic correctness without hardware predication. Implemented as an LLVM pass and evaluated on 102 programs from four benchmark suites, MERIT achieves a geometric mean speedup of 10.9% with peak improvements of 32x compared to hardware branch predictor, demonstrating the effectiveness with reduced static instruction overhead.

</details>


### [3] [A Bounded Game Semantics Checker for Precise Smart Contract Analysis](https://arxiv.org/abs/2512.22417)
*Vasileios Koutavas,Yu-Yang Lin,Nikos Tzevelekos*

Main category: cs.PL

TL;DR: 提出基于博弈语义的智能合约漏洞检测方法YulToolkit，实现无假阳性、边界完备的精确分析，可扩展到真实合约


<details>
  <summary>Details</summary>
Motivation: 智能合约漏洞检测需要精确性（无假阳性）和可扩展性，现有方法难以同时满足这两个要求，特别是对于重入等复杂漏洞

Method: 基于博弈语义建模合约与环境交互，将未知或恶意外部合约简化为轨迹枚举，实现YulToolkit工具，支持Solidity编写的检测器，探索所有可行交互轨迹

Result: YulToolkit成功检测DAO、PredyPool、Lendf.Me等真实漏洞，生成触发轨迹；修复后无违规报告，证明其精确性和有效性

Conclusion: 边界博弈语义探索是智能合约分析工具箱的有效补充，特别适用于重入等难以精确检测的漏洞类型

Abstract: We present a new approach to finding smart contract vulnerabilities that is precise (no false positives up to our EVM-Yul interpreter), bounded-complete, and, when instrumented with domain knowledge, scales to real-world contracts. Our method is based on game semantics, modelling computation as an interaction between a contract and its environment, reducing reasoning about unknown or malicious external contracts to trace enumeration. We implement this in a tool we refer to as YulToolkit, a bounded game-semantics checker for Yul, the intermediate language of Solidity. By exploring only feasible interactions, YulToolkit avoids over-approximation, and by relying on the theory of game semantics it achieves bounded completeness. To make exploration tractable, YulToolkit supports instrumentation written in Solidity and propagated to Yul, comparable in effort to creating a test harness. Unlike tests, however, our technique explores all admissible traces within the chosen parameters and bounds. We evaluate YulToolkit on three real-world incidents: The DAO, PredyPool, and Lendf.Me, as well as benchmark contracts. In all cases, YulToolkit detects the known vulnerabilities (producing a violation-triggering trace), and after applying fixes, reports no further violations within bounds. These results show that bounded game semantics exploration is an effective and precise addition to the smart contract analysis toolbox, particularly for vulnerabilities such as reentrancy that are hard to detect precisely in real code.

</details>


### [4] [Compiling Gradual Types with Evidence](https://arxiv.org/abs/2512.22684)
*José Luis Romero,Cristóbal Isla,Matías Toro,Éric Tanter*

Main category: cs.PL

TL;DR: 本文设计并实现了基于证据的编译器GrEv，证明证据语义可以实现高效的渐进类型实现，性能可与基于强制转换的编译器竞争甚至更快。


<details>
  <summary>Details</summary>
Motivation: 在结构类型语言中高效支持渐进类型具有挑战性。虽然AGT方法在语言设计和语义方面富有成效，但基于证据的语义是否能够实现高效的渐进类型实现尚不明确。

Method: 设计、实现和评估基于证据的编译器GrEv，弥合形式语义与编译器实现之间的差距，识别新颖的单调语义，并在Grift基准测试套件上进行实证评估。

Result: GrEv编译器性能可与基于强制转换的编译器竞争甚至更快，在静态到动态谱系配置中表现出更好的稳定性。

Conclusion: 基于证据的编译器是可行的高效渐进类型实现方案，为探索AGT文献中许多高级渐进类型系统的高效实现打开了大门。

Abstract: Efficiently supporting sound gradual typing in a language with structural types is challenging. To date, the Grift compiler is the only close-to-the-metal implementation of gradual typing in this setting, exploiting coercions for runtime checks, and further extended with monotonic references for efficient access to statically-typed data structures. On the language design and semantics side, the Abstracting Gradual Typing (AGT) methodology has proven fruitful to elucidate existing designs and to innovate by deriving gradualizations of a wide variety of typing disciplines and language features. Grounded in abstract interpretation, the Curry-Howard inspired runtime semantics of AGT is based on the notion of evidence for consistent judgments that evolve during reduction, monitoring the plausibility of well-typedness. While expressive and versatile, it is unclear whether such evidence-based semantics are a viable route to realize an efficient implementation of gradual typing.
  In this work, we explore this question by designing, implementing, and evaluating an evidence-based compiler, called GrEv. We explain how to bridge the gap between the formal semantics and the GrEv compiler implementation, and identify novel monotonic semantics. We empirically evaluate the performance of GrEv on the Grift benchmark suite. The results show that an evidence-based compiler can be competitive with, and even faster than, a coercion-based compiler, exhibiting more stability across configurations on the static-to-dynamic spectrum. In addition to enriching the space of gradual typing compilers, this work opens a direct door to exploring efficient implementations of the many advanced gradual typing disciplines formally derived with AGT in the literature.

</details>


### [5] [Fancy Some Chips for Your TeaStore? Modeling the Control of an Adaptable Discrete System](https://arxiv.org/abs/2512.23496)
*Anna Gallone,Simon Bliudze,Sophie Cerf,Olga Kouchnarenko*

Main category: cs.PL

TL;DR: Chips语言：用于设计包含各种交织组件的复杂系统模型，结合控制理论和通用编程语言概念，通过功能块描述应用，提高系统鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 开发者在设计新web应用时需要应对软件、硬件、网络、在线微服务等多种资源的约束，这些实体形成复杂的通信依赖系统。确保系统鲁棒性以提供良好服务质量非常重要。

Method: 提出Chips语言，允许以功能块形式描述应用，混合控制理论和通用编程语言概念，生成鲁棒的基于组件的模型。使用Adaptable TeaStore应用变体作为运行示例。

Result: Chips语言能够系统化地设计、建模和分析复杂系统项目，通过功能块描述和混合控制理论的方法，提高系统设计的鲁棒性和质量。

Conclusion: Chips语言为复杂系统的建模提供了一种系统化方法，通过结合控制理论和编程语言概念，有助于设计更鲁棒的web应用架构。

Abstract: When designing new web applications, developers must cope with different kinds of constraints relative to the resources they rely on: software, hardware, network, online micro-services, or any combination of the mentioned entities. Together, these entities form a complex system of communicating interdependent processes, physical or logical. It is very desirable that such system ensures its robustness to provide a good quality of service. In this paper we introduce Chips, a language that aims at facilitating the design of models made of various entwined components. It allows the description of applications in the form of functional blocks. Chips mixes notions  from control theory and general purpose programming languages to generate robust component-based models. This paper presents how to use Chips to systematically design, model and analyse a complex system project, using a variation of the Adaptable TeaStore application as running example.

</details>


### [6] [Adaptable TeaStore: A Choreographic Approach](https://arxiv.org/abs/2512.23497)
*Giuseppe De Palma,Saverio Giallorenzo,Ivan Lanese,Gianluigi Zavattaro*

Main category: cs.PL

TL;DR: 使用AIOCJ编排语言实现Adaptable TeaStore，展示动态自适应微服务架构的方法，确保通信正确性


<details>
  <summary>Details</summary>
Motivation: Adaptable TeaStore作为自适应微服务架构的参考模型，需要一种能够确保通信正确性并在运行时动态适应的实现方法

Method: 基于AIOCJ编排语言实现Adaptable TeaStore，该语言支持运行时动态适应，并保证通信的正确性（无死锁等）

Result: 成功展示了AIOCJ方法的优势，同时识别了当前限制，为未来改进编排范式和AIOCJ语言提供了建议

Conclusion: AIOCJ为自适应微服务架构提供了有前景的实现方法，但需要进一步改进以更好地适应实际云架构需求

Abstract: The Adaptable TeaStore has recently been proposed as a reference model for adaptable microservice architectures. It includes different configurations, as well as scenarios requiring to transition between them. We describe an implementation of the Adaptable TeaStore based on AIOCJ, a choreographic language that allows one to program multiparty systems that can adapt at runtime to different conditions. Following the choreographic tradition, AIOCJ ensures by-construction correctness of communications (e.g., no deadlocks) before, during, and after adaptation. Adaptation is dynamic, and the adaptation scenarios need to be fully specified only at runtime. Using AIOCJ to model the Adaptable TeaStore, we showcase the strengths of the approach and its current limitations, providing suggestions for future directions for refining the paradigm (and the AIOCJ language, in particular), to better align it with real-world Cloud architectures.

</details>


### [7] [Beyond Per-Thread Lock Sets: Multi-Thread Critical Sections and Dynamic Deadlock Prediction](https://arxiv.org/abs/2512.23552)
*Martin Sulzmann*

Main category: cs.PL

TL;DR: 提出一种改进的锁集构造方法，通过多线程临界区概念减少死锁预测中的误报和漏报


<details>
  <summary>Details</summary>
Motivation: 传统基于线程的锁集构造只考虑同一线程内获取的锁，忽略了其他线程的锁获取，导致死锁预测出现误报和漏报

Method: 1) 提出基于踪迹的多线程临界区概念，突破单线程限制；2) 通过偏序关系对踪迹特征进行可靠近似；3) 集成改进的锁集构造到SPDOffline扩展中

Result: 改进的锁集构造能消除DIRK死锁预测器的误报，并减少SPDOffline的漏报；在标准基准测试中性能不受影响

Conclusion: 多线程临界区概念是自然且正确的，改进的锁集构造方法在保持高效计算的同时提高了死锁预测的准确性

Abstract: Lock sets are commonly used for dynamic analysis of deadlocks. The standard per-thread lock set construction only considers locks acquired in the same thread, but is unaware of locks acquired in another thread. This leads to false positives and false negatives. The underlying issue is that the commonly used notion of a critical section on which the lock set construction relies ignores events from other threads. We give a trace-based characterization of critical sections that drops this restriction. Critical sections are no longer restricted to a single thread and can cover multiple threads. Such forms of critical sections exist, are natural, and correct the standard formulation.
  We show how to soundly approximate the trace-based characterization via partial order relations. Thus, we obtain an improved lock set construction that can still be efficiently computed and allows us to remove false positives reported by the DIRK deadlock predictor and remove false negatives by extending the SPDOffline deadlock predictor. We integrate various lock set constructions with increased precision in an extension of SPDOffline. Our extensions remain sound (no false positives) but are more complete (fewer false negatives) w.r.t. SPDOffline. For an extensive standard benchmark suite we can also show that the performance is not affected.

</details>


### [8] [Automating the Analysis of Parsing Algorithms (and other Dynamic Programs)](https://arxiv.org/abs/2512.23665)
*Tim Vieira,Ryan Cotterell,Jason Eisner*

Main category: cs.PL

TL;DR: 开发了一个帮助程序员分析NLP算法复杂度、类型和代码优化的系统


<details>
  <summary>Details</summary>
Motivation: NLP算法研究需要高效处理复杂形式结构，算法设计者需要保证算法性能（如时间复杂度、空间复杂度），并确定算法推导量的必要属性以合成高效数据结构和验证类型错误

Method: 开发了一个系统来帮助程序员进行这些类型的分析

Result: 将系统应用于多个NLP算法，成功推断出类型、死代码和冗余代码，以及参数化运行时和空间复杂度边界

Conclusion: 该系统能够有效帮助程序员分析NLP算法的复杂度、类型和代码优化问题

Abstract: Much algorithmic research in NLP aims to efficiently manipulate rich formal structures. An algorithm designer typically seeks to provide guarantees about their proposed algorithm -- for example, that its running time or space complexity is upper-bounded as a certain function of its input size. They may also wish to determine the necessary properties of the quantities derived by the algorithm to synthesize efficient data structures and verify type errors. In this paper, we develop a system for helping programmers to perform these types of analyses. We apply our system to a number of NLP algorithms and find that it successfully infers types, dead and redundant code, and parametric runtime and space complexity bounds.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [9] [Verifying Asynchronous Hyperproperties in Reactive Systems](https://arxiv.org/abs/2512.23344)
*Raven Beutner,Bernd Finkbeiner*

Main category: cs.LO

TL;DR: 提出基于博弈的方法来验证异步HyperLTL（A-HLTL）的∀*∃*公式，解决异步超属性模型检测问题


<details>
  <summary>Details</summary>
Motivation: 现有HyperLTL等逻辑在比较多个执行轨迹时采用同步方式，无法表达异步超属性（如观测确定性、非推断等）。A-HLTL虽然支持异步比较，但其模型检测问题仅限于受限片段或终止系统。

Method: 提出基于博弈的验证方法：将验证视为验证者与反驳者之间的博弈，双方分别控制底层轨迹和停顿的部分。验证者的获胜策略对应于存在量化轨迹的具体见证和存在量化停顿的异步对齐。

Result: 识别出该方法具有完备性的片段，从而构成有限状态决策过程。为任意∀*∃* A-HLTL公式在反应式系统中的验证提供了新方法。

Conclusion: 提出的博弈方法能够处理A-HLTL中更广泛的异步超属性验证问题，超越了先前受限片段或终止系统的限制。

Abstract: Hyperproperties are system properties that relate multiple execution traces and commonly occur when specifying information-flow and security policies. Logics like HyperLTL utilize explicit quantification over execution traces to express temporal hyperproperties in reactive systems, i.e., hyperproperties that reason about the temporal behavior along infinite executions. An often unwanted side-effect of such logics is that they compare the quantified traces synchronously. This prohibits the logics from expressing properties that compare multiple traces asynchronously, such as Zdancewic and Myers's observational determinism, McLean's non-inference, or stuttering refinement. We study the model-checking problem for a variant of asynchronous HyperLTL (A-HLTL), a temporal logic that can express hyperproperties where multiple traces are compared across timesteps. In addition to quantifying over system traces, A-HLTL features secondary quantification over stutterings of these traces. Consequently, A-HLTL allows for a succinct specification of many widely used asynchronous hyperproperties. Model-checking A-HLTL requires finding suitable stutterings, which, thus far, has been only possible for very restricted fragments or terminating systems. In this paper, we propose a novel game-based approach for the verification of arbitrary $\forall^*\exists^*$ A-HLTL formulas in reactive systems. In our method, we consider the verification as a game played between a verifier and a refuter, who challenge each other by controlling parts of the underlying traces and stutterings. A winning strategy for the verifier then corresponds to concrete witnesses for existentially quantified traces and asynchronous alignments for existentially quantified stutterings. We identify fragments for which our game-based interpretation is complete and thus constitutes a finite-state decision procedure.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [10] [TL: Automatic End-to-End Compiler of Tile-Based Languages for Spatial Dataflow Architectures](https://arxiv.org/abs/2512.22168)
*Wei Li,Zhenyu Bai,Heru Wang,Pranav Dangi,Zhiqiang Zhang,Cheng Tan,Huiying Lan,Weng-Fai Wong,Tulika Mitra*

Main category: cs.DC

TL;DR: TL是一个端到端框架，可将基于tile的程序编译到空间数据流架构上，解决tile实例在分布式核心间的映射问题，提升数据重用并减少通信。


<details>
  <summary>Details</summary>
Motivation: 空间数据流加速器能减少传统冯·诺依曼架构的内存瓶颈，但性能严重依赖于工作负载到硬件的映射。现有编译器主要优化单个tile内的代码生成，而缺乏在分布式核心间有效分配tile实例的能力，限制了空间数据流加速器的可编程性和广泛应用。

Method: TL提出一个硬件表示方法，捕捉互连拓扑、内存层次和计算能力，支持架构特定的优化和多样化的空间数据流目标。基于MLIR生态系统构建，定义了不同前端的通用入口点和不同后端的终点，专注于在空间分布式核心间分配tile实例，利用片上网络和分布式内存增加数据重用并减少通信。

Result: 论文没有提供具体的实验结果数据，但描述了TL框架的技术实现和设计理念，表明该框架能够解决空间数据流加速器的可编程性挑战。

Conclusion: TL框架通过解决tile实例在空间分布式核心间的映射问题，利用片上网络和分布式内存优化数据重用和通信，有望提升空间数据流加速器的可编程性和性能，促进其更广泛的应用。

Abstract: Spatial dataflow accelerators are a promising direction for next-generation computer systems because they can reduce the memory bottlenecks of traditional von Neumann machines such as CPUs and GPUs. They do so by organizing computation around explicit, compiler-managed data movement over the on-chip network, allowing operands to be directly forwarded between processing elements and reducing reliance on high-latency, bandwidth-limited global shared memory. Such localized communications can provide higher throughput and efficiency compared to repeated off-chip memory accesses. However, their end-to-end performance depends strongly on how workloads are mapped to the hardware. Naive mappings can perform very poorly, and most users rely on hand-tuned vendor libraries. In practice, although existing spatial-dataflow accelerators have strong potential for high performance, energy- and cost-efficiency, their limited programmability remains a major barrier to their wider adoption. This paper presents TL, an end-to-end framework that compiles tile-based programs (such as Triton kernels) onto spatial dataflow architectures. Unlike most existing compiler frameworks that focus on optimizing code generation within a single tile, TL addresses the central challenge of distributing tile instances across spatially distributed cores and exploiting the on-chip network and distributed memories to increase data reuse and reduce communications. TL proposes a hardware representation that captures interconnect topology, memory hierarchy, and compute capabilities, enabling both specialized architecture-specific optimizations and support for diverse spatial dataflow targets. TL is built on the MLIR ecosystem and defines a generic entry point for different front-ends and an end point for different back-ends.

</details>


### [11] [Mirage Persistent Kernel: A Compiler and Runtime for Mega-Kernelizing Tensor Programs](https://arxiv.org/abs/2512.22219)
*Xinhao Cheng,Zhihao Zhang,Yu Zhou,Jianan Ji,Jinchen Jiang,Zepeng Zhao,Ziruo Xiao,Zihao Ye,Yingyi Huang,Ruihang Lai,Hongyi Jin,Bohan Hou,Mengdi Wu,Yixin Dong,Anthony Yip,Zihao Ye,Songting Wang,Wenqin Yang,Xupeng Miao,Tianqi Chen,Zhihao Jia*

Main category: cs.DC

TL;DR: MPK是首个将多GPU模型推理自动转换为单个高性能megakernel的编译器和运行时系统，通过SM级图表示实现跨算子软件流水线等优化，显著提升LLM推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理系统通常采用算子级内核调度，无法充分利用GPU硬件潜力，存在内核启动开销、SM资源利用不足等问题，需要更细粒度的优化方案。

Method: 提出SM级图表示捕获数据依赖，编译器将张量程序转换为优化后的SM级任务图并生成CUDA实现，运行时在单个megakernel内通过去中心化调度执行任务。

Result: MPK相比现有算子级LLM服务系统，端到端推理延迟降低高达1.7倍，将LLM推理性能推向硬件极限，且保持现有编程模型的灵活性。

Conclusion: MPK通过自动化的端到端内核融合和细粒度优化，显著提升了多GPU模型推理性能，为高效LLM服务提供了创新解决方案。

Abstract: We introduce Mirage Persistent Kernel (MPK), the first compiler and runtime system that automatically transforms multi-GPU model inference into a single high-performance megakernel. MPK introduces an SM-level graph representation that captures data dependencies at the granularity of individual streaming multiprocessors (SMs), enabling cross-operator software pipelining, fine-grained kernel overlap, and other previously infeasible GPU optimizations. The MPK compiler lowers tensor programs into highly optimized SM-level task graphs and generates optimized CUDA implementations for all tasks, while the MPK in-kernel parallel runtime executes these tasks within a single mega-kernel using decentralized scheduling across SMs. Together, these components provide end-to-end kernel fusion with minimal developer effort, while preserving the flexibility of existing programming models. Our evaluation shows that MPK significantly outperforms existing kernel-per-operator LLM serving systems by reducing end-to-end inference latency by up to 1.7x, pushing LLM inference performance close to hardware limits. MPK is publicly available at https://github.com/mirage-project/mirage.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [12] [Anka: A Domain-Specific Language for Reliable LLM Code Generation](https://arxiv.org/abs/2512.23214)
*Saif Khalfan Saif Al Mazrouei*

Main category: cs.CL

TL;DR: LLMs在复杂编程任务中常出错，作者假设是通用语言的灵活性导致。他们创建了Anka DSL（领域特定语言），通过约束语法减少歧义。尽管LLMs从未训练过Anka，但Claude 3.5 Haiku在100个基准问题上达到99.9%解析成功率和95.8%总体准确率，在多步管道任务上比Python准确率高40个百分点。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码生成方面表现出色，但在复杂、多步骤的编程任务中仍存在系统性错误。作者假设这些错误源于通用编程语言的灵活性，这种灵活性允许多种有效方法但需要隐式的状态管理。为了验证这一假设，需要设计一种能减少歧义的领域特定语言。

Method: 作者引入了Anka，一种专门为数据转换管道设计的领域特定语言（DSL）。Anka具有明确、约束的语法，旨在减少代码生成中的歧义。他们创建了包含100个基准问题的测试套件，评估LLMs（Claude 3.5 Haiku和GPT-4o-mini）在零训练暴露的情况下使用Anka的表现，并与Python进行对比。

Result: 尽管LLMs从未训练过Anka，但Claude 3.5 Haiku在100个基准问题上达到99.9%解析成功率和95.8%总体任务准确率。在多步管道任务中，Anka比Python准确率高40个百分点（100% vs. 60%）。GPT-4o-mini的跨模型验证也确认了这一优势（多步任务上+26.7个百分点）。

Conclusion: 研究结果表明：(1) LLMs可以通过上下文提示完全学习新的DSL，达到接近原生的准确率；(2) 约束语法能显著减少复杂任务中的错误；(3) 专门为LLM生成设计的领域特定语言可以超越LLMs有大量训练数据的通用语言。作者发布了完整的语言实现、基准套件和评估框架以促进进一步研究。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in code generation, yet they exhibit systematic errors on complex, multi-step programming tasks. We hypothesize that these errors stem from the flexibility of general-purpose languages, which permits multiple valid approaches and requires implicit state management. To test this hypothesis, we introduce Anka, a domain-specific language (DSL) for data transformation pipelines designed with explicit, constrained syntax that reduces ambiguity in code generation. Despite having zero prior training exposure to Anka, Claude 3.5 Haiku achieves 99.9% parse success and 95.8% overall task accuracy across 100 benchmark problems. Critically, Anka demonstrates a 40 percentage point accuracy advantage over Python on multi-step pipeline tasks (100% vs. 60%), where Python's flexible syntax leads to frequent errors in operation sequencing and variable management. Cross-model validation with GPT-4o-mini confirms this advantage (+26.7 percentage points on multi-step tasks). Our results demonstrate that: (1) LLMs can learn novel DSLs entirely from in-context prompts, achieving near-native accuracy; (2) constrained syntax significantly reduces errors on complex tasks; and (3) domain-specific languages purposefully designed for LLM generation can outperform general-purpose languages on which the LLM has extensive training. We release the complete language implementation, benchmark suite, and evaluation framework to facilitate further research.

</details>
