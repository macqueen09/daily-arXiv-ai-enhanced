{"id": "2601.00882", "pdf": "https://arxiv.org/pdf/2601.00882", "abs": "https://arxiv.org/abs/2601.00882", "authors": ["Mingxiu Wang", "Jiawei Wang", "Xiao Cheng"], "title": "BALI: Branch-Aware Loop Invariant Inference with Large Language Models", "categories": ["cs.PL"], "comment": "4 pages, 1 figure, AAAI-26 Bridge Program B10: Making Embodied AI Reliable with Testing and Formal Verification", "summary": "Loop invariants are fundamental for reasoning about the correctness of iterative algorithms. However, deriving suitable invariants remains a challenging and often manual task, particularly for complex programs. In this paper, we introduce BALI, a branch-aware framework that integrates large language models (LLMs) to enhance the inference and verification of loop invariants. Our approach combines automated reasoning with branch-aware static program analysis to improve both precision and scalability. Specifically, unlike prior LLM-only guess-and-check methods, BALI first verifies branch-sequence-level (path-level) clauses with SMT and then composes them into program-level invariants. We outline its key components, present preliminary results, and discuss future directions toward fully automated invariant discovery."}
{"id": "2601.02045", "pdf": "https://arxiv.org/pdf/2601.02045", "abs": "https://arxiv.org/abs/2601.02045", "authors": ["Shuoming Zhang", "Jiacheng Zhao", "Qiuchu Yu", "Chunwei Xia", "Zheng Wang", "Xiaobing Feng", "Huimin Cui"], "title": "The New Compiler Stack: A Survey on the Synergy of LLMs and Compilers", "categories": ["cs.PL", "cs.AI", "cs.SE"], "comment": "Accepted by CCF Transactions on High Performance Computing", "summary": "This survey has provided a systematic overview of the emerging field of LLM-enabled compilation by addressing several key research questions. We first answered how LLMs are being integrated by proposing a comprehensive, multi-dimensional taxonomy that categorizes works based on their Design Philosophy (Selector, Translator, Generator), LLM Methodology, their operational Level of Code Abstraction, and the specific Task Type they address. In answering what advancements these approaches offer, we identified three primary benefits: the democratization of compiler development, the discovery of novel optimization strategies, and the broadening of the compiler's traditional scope. Finally, in addressing the field's challenges and opportunities, we highlighted the critical hurdles of ensuring correctness and achieving scalability, while identifying the development of hybrid systems as the most promising path forward. By providing these answers, this survey serves as a foundational roadmap for researchers and practitioners, charting the course for a new generation of LLM-powered, intelligent, adaptive and synergistic compilation tools."}
{"id": "2601.02060", "pdf": "https://arxiv.org/pdf/2601.02060", "abs": "https://arxiv.org/abs/2601.02060", "authors": ["Nguyet-Anh H. Lang", "Eric Lang", "Thanh Le-Cong", "Bach Le", "Quyet-Thang Huynh"], "title": "Perish or Flourish? A Holistic Evaluation of Large Language Models for Code Generation in Functional Programming", "categories": ["cs.PL", "cs.AI", "cs.SE"], "comment": null, "summary": "Functional programming provides strong foundations for developing reliable and secure software systems, yet its adoption remains not widespread due to the steep learning curve. Recent advances in Large Language Models (LLMs) for code generation present new opportunities to lower these barriers. However, extensive evaluations of LLMs largely focus on imperative programming languages, and their capabilities in functional programming languages (FP) remain underexplored. To address this gap, we introduce FPEval, a holistic evaluation framework built on FPBench, a new benchmark of 721 programming tasks across three difficulty levels on three mainstream FP languages: Haskell, Ocaml and Scala. FPEval provides compehensive evaluation infrastructures with both test validations with comprehensive test suites and static analysis tools to assess both functional correctness and code style and maintainability. Using this framework, we evaluate state-of-the-art LLMs, including GPT-3.5, GPT-4o, and GPT-5, for code generation in functional programming languages and Java as an imperative baseline. Our results demonstrate that LLM performance in functional programming improves substantially with model advancement; however, error rates remain significantly higher in purely functional languages (Haskell and OCaml) than in hybrid (Scala) or imperative (Java) languages. Moreover, LLMs frequently generate non-idiomatic functional code that follows imperative patterns, raising concerns about code style and long-term maintainability. Finally, we show that LLMs can partially self-repair both correctness and quality issues when provided with static analysis feedback and hand-crafted instructions for common types of issues."}
{"id": "2601.02218", "pdf": "https://arxiv.org/pdf/2601.02218", "abs": "https://arxiv.org/abs/2601.02218", "authors": ["Berke Ates", "Filip Dobrosavljević", "Theodoros Theodoridis", "Zhendong Su"], "title": "MLIR-Smith: A Novel Random Program Generator for Evaluating Compiler Pipelines", "categories": ["cs.PL", "cs.SE"], "comment": null, "summary": "Compilers are essential for the performance and correct execution of software and hold universal relevance across various scientific disciplines. Despite this, there is a notable lack of tools for testing and evaluating them, especially within the adaptable Multi-Level Intermediate Representation (MLIR) context. This paper addresses the need for a tool that can accommodate MLIR's extensibility, a feature not provided by previous methods such as Csmith. Here we introduce MLIR-Smith, a novel random program generator specifically designed to test and evaluate MLIR-based compiler optimizations. We demonstrate the utility of MLIR-Smith by conducting differential testing on MLIR, LLVM, DaCe, and DCIR, which led to the discovery of multiple bugs in these compiler pipelines. The introduction of MLIR-Smith not only fills a void in the realm of compiler testing but also emphasizes the importance of comprehensive testing within these systems. By providing a tool that can generate random MLIR programs, this paper enhances our ability to evaluate and improve compilers and paves the way for future tools, potentially shaping the wider landscape of software testing and quality assurance."}
{"id": "2601.00880", "pdf": "https://arxiv.org/pdf/2601.00880", "abs": "https://arxiv.org/abs/2601.00880", "authors": ["Anthony Mikinka"], "title": "Universal Conditional Logic: A Formal Language for Prompt Engineering", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.PL", "cs.SE"], "comment": "25 pages, 15 figures, 5 tables. Includes appendices with variable reference, pattern library, and O_s calculation examples. Supplementary materials: V1-V4.1 prompt source code and 305 model responses available at GitHub repositories", "summary": "We present Universal Conditional Logic (UCL), a mathematical framework for prompt optimization that transforms prompt engineering from heuristic practice into systematic optimization. Through systematic evaluation (N=305, 11 models, 4 iterations), we demonstrate significant token reduction (29.8%, t(10)=6.36, p < 0.001, Cohen's d = 2.01) with corresponding cost savings. UCL's structural overhead function O_s(A) explains version-specific performance differences through the Over-Specification Paradox: beyond threshold S* = 0.509, additional specification degrades performance quadratically. Core mechanisms -- indicator functions (I_i in {0,1}), structural overhead (O_s = gamma * sum(ln C_k)), early binding -- are validated. Notably, optimal UCL configuration varies by model architecture -- certain models (e.g., Llama 4 Scout) require version-specific adaptations (V4.1). This work establishes UCL as a calibratable framework for efficient LLM interaction, with model-family-specific optimization as a key research direction."}
{"id": "2601.01436", "pdf": "https://arxiv.org/pdf/2601.01436", "abs": "https://arxiv.org/abs/2601.01436", "authors": ["Hyunhum Cho", "Ik Rae Jeong"], "title": "Bithoven: Formal Safety for Expressive Bitcoin Smart Contracts", "categories": ["cs.CR", "cs.PL"], "comment": "15 pages, 3 figures, 4 tables. Submitted to IEEE Transactions on Dependable and Secure Computing", "summary": "The rigorous security model of Bitcoin's UTXO architecture often comes at the cost of developer usability, forcing a reliance on manual stack manipulation that leads to critical financial vulnerabilities like signature malleability, unspendable states and unconstrained execution paths. Industry standards such as Miniscript provide necessary abstractions for policy verification but do not model the full imperative logic required for complex contracts, leaving gaps in state management and resource liveness. This paper introduces Bithoven, a high-level language designed to bridge the gap between expressiveness and formal safety. By integrating a strict type checker and a resource liveness analyzer with a semantic control-flow analyzer, Bithoven eliminates major categories of consensus and logic defects defined in our fault model prior to deployment. Our results indicate that this safety comes at modest cost: Bithoven compiles to Bitcoin Script with efficiency comparable to hand-optimized code, demonstrating that type-safe, developer-friendly abstractions are viable even within the strict byte-size constraints of the Bitcoin blockchain."}
{"id": "2601.01638", "pdf": "https://arxiv.org/pdf/2601.01638", "abs": "https://arxiv.org/abs/2601.01638", "authors": ["Adrienne Lancelot", "Giulio Manzonetto", "Guy McCusker", "Gabriele Vanoni"], "title": "Interaction Improvement", "categories": ["cs.LO", "cs.PL"], "comment": null, "summary": "The relational semantics of linear logic is a powerful framework for defining resource-aware models of the $λ$-calculus. However, its quantitative aspects are not reflected in the preorders and equational theories induced by these models. Indeed, they can be characterized in terms of (in)equalities between Böhm trees up to extensionality, which are qualitative in nature. We employ the recently introduced checkers calculus to provide a quantitative and contextual interpretation of the preorder associated to the relational semantics. This way, we show that the relational semantics refines the contextual preorder constraining the number of interactions between the related terms and the context."}
{"id": "2601.01944", "pdf": "https://arxiv.org/pdf/2601.01944", "abs": "https://arxiv.org/abs/2601.01944", "authors": ["Matteo Esposito", "Andrea Janes", "Valentina Lenarduzzi", "Davide Taibi"], "title": "The Invisible Hand of AI Libraries Shaping Open Source Projects and Communities", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.IR", "cs.PL"], "comment": "ACCEPTED REGISTERED REPORT AT SANER (CORE A*) 2026", "summary": "In the early 1980s, Open Source Software emerged as a revolutionary concept amidst the dominance of proprietary software. What began as a revolutionary idea has now become the cornerstone of computer science. Amidst OSS projects, AI is increasing its presence and relevance. However, despite the growing popularity of AI, its adoption and impacts on OSS projects remain underexplored.\n  We aim to assess the adoption of AI libraries in Python and Java OSS projects and examine how they shape development, including the technical ecosystem and community engagement. To this end, we will perform a large-scale analysis on 157.7k potential OSS repositories, employing repository metrics and software metrics to compare projects adopting AI libraries against those that do not. We expect to identify measurable differences in development activity, community engagement, and code complexity between OSS projects that adopt AI libraries and those that do not, offering evidence-based insights into how AI integration reshapes software development practices."}
{"id": "2601.02251", "pdf": "https://arxiv.org/pdf/2601.02251", "abs": "https://arxiv.org/abs/2601.02251", "authors": ["Guy Amir", "Mark Barbone", "Nicolas Amat", "Jules Jacobs"], "title": "Deciding Serializability in Network Systems", "categories": ["cs.FL", "cs.DC", "cs.LO", "cs.PL"], "comment": "To appear in TACAS 2026", "summary": "We present the SER modeling language for automatically verifying serializability of concurrent programs, i.e., whether every concurrent execution of the program is equivalent to some serial execution.\n  SER programs are suitably restricted to make this problem decidable, while still allowing for an unbounded number of concurrent threads of execution, each potentially running for an unbounded number of steps.\n  Building on prior theoretical results, we give the first automated end-to-end decision procedure that either proves serializability by producing a checkable certificate, or refutes it by producing a counterexample trace.\n  We also present a network-system abstraction to which SER programs compile. Our decision procedure then reduces serializability in this setting to a Petri net reachability query.\n  Furthermore, in order to scale, we curtail the search space via multiple optimizations, including Petri net slicing, semilinear-set compression, and Presburger-formula manipulation.\n  We extensively evaluate our framework and show that, despite the theoretical hardness of the problem, it can successfully handle various models of real-world programs, including stateful firewalls, BGP routers, and more."}
