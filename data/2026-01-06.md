<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 4]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.FL](#cs.FL) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [BALI: Branch-Aware Loop Invariant Inference with Large Language Models](https://arxiv.org/abs/2601.00882)
*Mingxiu Wang,Jiawei Wang,Xiao Cheng*

Main category: cs.PL

TL;DR: BALI：一个结合大语言模型和分支感知静态分析的框架，用于改进循环不变式的推断和验证


<details>
  <summary>Details</summary>
Motivation: 循环不变式对于验证迭代算法的正确性至关重要，但推导合适的不变式仍然是一项具有挑战性且通常需要手动完成的任务，特别是对于复杂程序。

Method: BALI框架结合大语言模型和分支感知静态程序分析，首先使用SMT验证分支序列级（路径级）子句，然后将它们组合成程序级不变式。

Result: 该方法相比之前的LLM-only猜测-检查方法，在精度和可扩展性方面都有所改进，并展示了初步结果。

Conclusion: BALI为完全自动化的不变式发现提供了有前景的方向，结合了自动化推理和分支感知分析的优势。

Abstract: Loop invariants are fundamental for reasoning about the correctness of iterative algorithms. However, deriving suitable invariants remains a challenging and often manual task, particularly for complex programs. In this paper, we introduce BALI, a branch-aware framework that integrates large language models (LLMs) to enhance the inference and verification of loop invariants. Our approach combines automated reasoning with branch-aware static program analysis to improve both precision and scalability. Specifically, unlike prior LLM-only guess-and-check methods, BALI first verifies branch-sequence-level (path-level) clauses with SMT and then composes them into program-level invariants. We outline its key components, present preliminary results, and discuss future directions toward fully automated invariant discovery.

</details>


### [2] [The New Compiler Stack: A Survey on the Synergy of LLMs and Compilers](https://arxiv.org/abs/2601.02045)
*Shuoming Zhang,Jiacheng Zhao,Qiuchu Yu,Chunwei Xia,Zheng Wang,Xiaobing Feng,Huimin Cui*

Main category: cs.PL

TL;DR: 本文对LLM赋能编译领域进行了系统性综述，提出了多维分类法，总结了三大优势，并指出了该领域的挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）在代码生成和理解方面展现出强大能力，如何将其有效应用于编译领域成为一个新兴研究方向。本文旨在系统性地梳理LLM赋能编译的研究现状，为研究者和从业者提供清晰的路线图。

Method: 通过提出一个多维分类法来系统分析现有研究：1）设计哲学（选择器、翻译器、生成器）；2）LLM方法论；3）代码抽象层级；4）任务类型。基于此分类法对现有工作进行系统梳理和分析。

Result: 识别了LLM赋能编译的三大主要优势：1）编译开发的民主化；2）新颖优化策略的发现；3）编译器传统范围的扩展。同时指出了确保正确性和实现可扩展性等关键挑战。

Conclusion: 混合系统的发展是最有前景的前进方向。本综述为研究者和从业者提供了基础路线图，指引新一代LLM驱动的智能、自适应和协同编译工具的发展。

Abstract: This survey has provided a systematic overview of the emerging field of LLM-enabled compilation by addressing several key research questions. We first answered how LLMs are being integrated by proposing a comprehensive, multi-dimensional taxonomy that categorizes works based on their Design Philosophy (Selector, Translator, Generator), LLM Methodology, their operational Level of Code Abstraction, and the specific Task Type they address. In answering what advancements these approaches offer, we identified three primary benefits: the democratization of compiler development, the discovery of novel optimization strategies, and the broadening of the compiler's traditional scope. Finally, in addressing the field's challenges and opportunities, we highlighted the critical hurdles of ensuring correctness and achieving scalability, while identifying the development of hybrid systems as the most promising path forward. By providing these answers, this survey serves as a foundational roadmap for researchers and practitioners, charting the course for a new generation of LLM-powered, intelligent, adaptive and synergistic compilation tools.

</details>


### [3] [Perish or Flourish? A Holistic Evaluation of Large Language Models for Code Generation in Functional Programming](https://arxiv.org/abs/2601.02060)
*Nguyet-Anh H. Lang,Eric Lang,Thanh Le-Cong,Bach Le,Quyet-Thang Huynh*

Main category: cs.PL

TL;DR: FPEval是一个评估大语言模型在函数式编程语言中代码生成能力的框架，基于包含721个任务的FPBench基准测试，涵盖Haskell、OCaml和Scala三种语言。评估发现LLM在函数式编程中表现随模型进步而提升，但在纯函数式语言中错误率仍较高，且常生成非惯用的命令式风格代码。


<details>
  <summary>Details</summary>
Motivation: 函数式编程为开发可靠软件提供了坚实基础，但由于学习曲线陡峭，采用率不高。虽然大语言模型在代码生成方面取得进展，但现有评估主要关注命令式编程语言，对函数式编程语言的能力评估不足。需要填补这一研究空白。

Method: 提出FPEval评估框架，基于FPBench基准测试（包含721个编程任务，分为三个难度级别，涵盖Haskell、OCaml和Scala三种主流函数式语言）。框架提供全面的评估基础设施，包括测试验证套件和静态分析工具，用于评估功能正确性、代码风格和可维护性。使用该框架评估GPT-3.5、GPT-4o和GPT-5等最先进LLM，并以Java作为命令式基线。

Result: LLM在函数式编程中的性能随模型进步显著提升；但在纯函数式语言（Haskell和OCaml）中的错误率明显高于混合式（Scala）或命令式（Java）语言。LLM经常生成遵循命令式模式的非惯用函数式代码，引发对代码风格和长期可维护性的担忧。当提供静态分析反馈和针对常见问题的手工指令时，LLM能够部分自我修复正确性和质量问题。

Conclusion: 虽然大语言模型在函数式编程代码生成方面有进步，但在纯函数式语言中仍面临挑战，且生成的代码风格问题值得关注。静态分析反馈有助于改善LLM输出，但需要进一步研究来提高函数式编程的代码生成质量和惯用性。

Abstract: Functional programming provides strong foundations for developing reliable and secure software systems, yet its adoption remains not widespread due to the steep learning curve. Recent advances in Large Language Models (LLMs) for code generation present new opportunities to lower these barriers. However, extensive evaluations of LLMs largely focus on imperative programming languages, and their capabilities in functional programming languages (FP) remain underexplored. To address this gap, we introduce FPEval, a holistic evaluation framework built on FPBench, a new benchmark of 721 programming tasks across three difficulty levels on three mainstream FP languages: Haskell, Ocaml and Scala. FPEval provides compehensive evaluation infrastructures with both test validations with comprehensive test suites and static analysis tools to assess both functional correctness and code style and maintainability. Using this framework, we evaluate state-of-the-art LLMs, including GPT-3.5, GPT-4o, and GPT-5, for code generation in functional programming languages and Java as an imperative baseline. Our results demonstrate that LLM performance in functional programming improves substantially with model advancement; however, error rates remain significantly higher in purely functional languages (Haskell and OCaml) than in hybrid (Scala) or imperative (Java) languages. Moreover, LLMs frequently generate non-idiomatic functional code that follows imperative patterns, raising concerns about code style and long-term maintainability. Finally, we show that LLMs can partially self-repair both correctness and quality issues when provided with static analysis feedback and hand-crafted instructions for common types of issues.

</details>


### [4] [MLIR-Smith: A Novel Random Program Generator for Evaluating Compiler Pipelines](https://arxiv.org/abs/2601.02218)
*Berke Ates,Filip Dobrosavljević,Theodoros Theodoridis,Zhendong Su*

Main category: cs.PL

TL;DR: MLIR-Smith：专门为MLIR编译器优化设计的随机程序生成器，用于测试和评估MLIR-based编译器，填补了编译器测试领域的空白。


<details>
  <summary>Details</summary>
Motivation: 尽管编译器对软件性能和正确执行至关重要，但在MLIR这种可扩展的中间表示框架中，缺乏专门的测试工具。现有方法如Csmith无法适应MLIR的可扩展性特点，因此需要开发专门针对MLIR的测试工具。

Method: 开发了MLIR-Smith，一种专门为MLIR设计的随机程序生成器。该工具能够生成随机的MLIR程序，用于对MLIR-based编译器优化进行差分测试。

Result: 通过对MLIR、LLVM、DaCe和DCIR进行差分测试，发现了多个编译器流水线中的bug。MLIR-Smith不仅填补了编译器测试领域的空白，还证明了其在实际应用中的有效性。

Conclusion: MLIR-Smith为MLIR编译器提供了有效的测试工具，增强了评估和改进编译器的能力，为未来软件测试和质量保证工具的发展奠定了基础。

Abstract: Compilers are essential for the performance and correct execution of software and hold universal relevance across various scientific disciplines. Despite this, there is a notable lack of tools for testing and evaluating them, especially within the adaptable Multi-Level Intermediate Representation (MLIR) context. This paper addresses the need for a tool that can accommodate MLIR's extensibility, a feature not provided by previous methods such as Csmith. Here we introduce MLIR-Smith, a novel random program generator specifically designed to test and evaluate MLIR-based compiler optimizations. We demonstrate the utility of MLIR-Smith by conducting differential testing on MLIR, LLVM, DaCe, and DCIR, which led to the discovery of multiple bugs in these compiler pipelines. The introduction of MLIR-Smith not only fills a void in the realm of compiler testing but also emphasizes the importance of comprehensive testing within these systems. By providing a tool that can generate random MLIR programs, this paper enhances our ability to evaluate and improve compilers and paves the way for future tools, potentially shaping the wider landscape of software testing and quality assurance.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [5] [The Invisible Hand of AI Libraries Shaping Open Source Projects and Communities](https://arxiv.org/abs/2601.01944)
*Matteo Esposito,Andrea Janes,Valentina Lenarduzzi,Davide Taibi*

Main category: cs.SE

TL;DR: 该研究通过大规模分析15.7万个开源项目，比较了采用AI库与未采用AI库的Python和Java项目在开发活动、社区参与和代码复杂性方面的差异。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在开源软件中的存在和相关性日益增加，但其在开源项目中的采用情况和影响仍未得到充分探索。研究旨在填补这一空白，了解AI集成如何重塑软件开发实践。

Method: 对157.7万个潜在开源仓库进行大规模分析，使用仓库指标和软件指标，比较采用AI库的项目与未采用AI库的项目。

Result: 预期发现采用AI库与未采用AI库的开源项目在开发活动、社区参与和代码复杂性方面存在可测量的差异。

Conclusion: 研究将为AI集成如何重塑软件开发实践提供基于证据的见解，帮助理解AI在开源生态系统中的实际影响。

Abstract: In the early 1980s, Open Source Software emerged as a revolutionary concept amidst the dominance of proprietary software. What began as a revolutionary idea has now become the cornerstone of computer science. Amidst OSS projects, AI is increasing its presence and relevance. However, despite the growing popularity of AI, its adoption and impacts on OSS projects remain underexplored.
  We aim to assess the adoption of AI libraries in Python and Java OSS projects and examine how they shape development, including the technical ecosystem and community engagement. To this end, we will perform a large-scale analysis on 157.7k potential OSS repositories, employing repository metrics and software metrics to compare projects adopting AI libraries against those that do not. We expect to identify measurable differences in development activity, community engagement, and code complexity between OSS projects that adopt AI libraries and those that do not, offering evidence-based insights into how AI integration reshapes software development practices.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [6] [Bithoven: Formal Safety for Expressive Bitcoin Smart Contracts](https://arxiv.org/abs/2601.01436)
*Hyunhum Cho,Ik Rae Jeong*

Main category: cs.CR

TL;DR: Bithoven是一个为比特币UTXO架构设计的高级语言，通过类型检查、资源活性分析和语义控制流分析，在保持高效编译的同时消除共识和逻辑缺陷。


<details>
  <summary>Details</summary>
Motivation: 比特币UTXO架构的严格安全模型牺牲了开发者可用性，导致手动堆栈操作容易引发签名可塑性、不可花费状态和无约束执行路径等关键金融漏洞。现有标准如Miniscript虽然提供了策略验证抽象，但无法建模复杂合约所需的完整命令式逻辑，在状态管理和资源活性方面存在不足。

Method: 开发Bithoven高级语言，集成严格类型检查器、资源活性分析器和语义控制流分析器，通过形式化安全分析在部署前消除共识和逻辑缺陷。该语言编译为比特币脚本，保持高效性。

Result: Bithoven能够消除主要类别的共识和逻辑缺陷，同时编译效率与手工优化代码相当，证明类型安全、开发者友好的抽象在比特币区块链的严格字节大小限制下是可行的。

Conclusion: Bithoven成功在比特币UTXO架构的表达能力和形式化安全之间架起桥梁，以适度成本实现了高级抽象，为比特币智能合约开发提供了更安全、更易用的解决方案。

Abstract: The rigorous security model of Bitcoin's UTXO architecture often comes at the cost of developer usability, forcing a reliance on manual stack manipulation that leads to critical financial vulnerabilities like signature malleability, unspendable states and unconstrained execution paths. Industry standards such as Miniscript provide necessary abstractions for policy verification but do not model the full imperative logic required for complex contracts, leaving gaps in state management and resource liveness. This paper introduces Bithoven, a high-level language designed to bridge the gap between expressiveness and formal safety. By integrating a strict type checker and a resource liveness analyzer with a semantic control-flow analyzer, Bithoven eliminates major categories of consensus and logic defects defined in our fault model prior to deployment. Our results indicate that this safety comes at modest cost: Bithoven compiles to Bitcoin Script with efficiency comparable to hand-optimized code, demonstrating that type-safe, developer-friendly abstractions are viable even within the strict byte-size constraints of the Bitcoin blockchain.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [7] [Universal Conditional Logic: A Formal Language for Prompt Engineering](https://arxiv.org/abs/2601.00880)
*Anthony Mikinka*

Main category: cs.AI

TL;DR: UCL是一个将提示工程从启发式实践转变为系统优化的数学框架，通过系统评估显著减少token使用（29.8%），其结构开销函数揭示了过度指定悖论，最优配置因模型架构而异。


<details>
  <summary>Details</summary>
Motivation: 当前提示工程主要依赖启发式实践，缺乏系统化的优化框架。研究者希望建立一个数学框架，将提示工程转变为可系统优化的过程，提高LLM交互的效率并降低成本。

Method: 提出了Universal Conditional Logic (UCL)框架，包含指示函数(I_i)、结构开销函数(O_s = gamma * sum(ln C_k))和早期绑定等核心机制。通过系统评估（N=305，11个模型，4次迭代）验证框架有效性，并分析过度指定悖论。

Result: UCL显著减少token使用29.8%（t(10)=6.36, p < 0.001, Cohen's d = 2.01），对应成本节约。发现过度指定阈值S* = 0.509，超过此阈值后额外指定会二次降低性能。不同模型架构需要特定优化配置。

Conclusion: UCL建立了一个可校准的LLM高效交互框架，揭示了模型家族特定优化的重要性。该框架将提示工程从启发式实践转变为系统化优化，为未来研究提供了重要方向。

Abstract: We present Universal Conditional Logic (UCL), a mathematical framework for prompt optimization that transforms prompt engineering from heuristic practice into systematic optimization. Through systematic evaluation (N=305, 11 models, 4 iterations), we demonstrate significant token reduction (29.8%, t(10)=6.36, p < 0.001, Cohen's d = 2.01) with corresponding cost savings. UCL's structural overhead function O_s(A) explains version-specific performance differences through the Over-Specification Paradox: beyond threshold S* = 0.509, additional specification degrades performance quadratically. Core mechanisms -- indicator functions (I_i in {0,1}), structural overhead (O_s = gamma * sum(ln C_k)), early binding -- are validated. Notably, optimal UCL configuration varies by model architecture -- certain models (e.g., Llama 4 Scout) require version-specific adaptations (V4.1). This work establishes UCL as a calibratable framework for efficient LLM interaction, with model-family-specific optimization as a key research direction.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [8] [Interaction Improvement](https://arxiv.org/abs/2601.01638)
*Adrienne Lancelot,Giulio Manzonetto,Guy McCusker,Gabriele Vanoni*

Main category: cs.LO

TL;DR: 使用检查器演算为线性逻辑关系语义提供定量解释，证明关系语义细化了上下文预序，约束了相关项与上下文之间的交互次数。


<details>
  <summary>Details</summary>
Motivation: 线性逻辑的关系语义虽然为λ演算提供了资源感知模型，但其定量方面未反映在模型诱导的预序和等式理论中。现有特征描述基于Böhm树在扩展性下的（不）等式，本质上是定性的。

Method: 采用最近引入的检查器演算（checkers calculus），为关系语义相关的预序提供定量和上下文解释。

Result: 证明关系语义细化了上下文预序，能够约束相关项与上下文之间的交互次数，从而为关系语义提供了定量维度。

Conclusion: 通过检查器演算，成功为线性逻辑关系语义的预序关系建立了定量解释框架，弥补了原有定性分析的不足，增强了关系语义在资源分析方面的表达能力。

Abstract: The relational semantics of linear logic is a powerful framework for defining resource-aware models of the $λ$-calculus. However, its quantitative aspects are not reflected in the preorders and equational theories induced by these models. Indeed, they can be characterized in terms of (in)equalities between Böhm trees up to extensionality, which are qualitative in nature. We employ the recently introduced checkers calculus to provide a quantitative and contextual interpretation of the preorder associated to the relational semantics. This way, we show that the relational semantics refines the contextual preorder constraining the number of interactions between the related terms and the context.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [9] [Deciding Serializability in Network Systems](https://arxiv.org/abs/2601.02251)
*Guy Amir,Mark Barbone,Nicolas Amat,Jules Jacobs*

Main category: cs.FL

TL;DR: SER语言用于自动验证并发程序的串行化，通过受限程序使问题可判定，并首次提供端到端决策程序，可证明串行化或生成反例


<details>
  <summary>Details</summary>
Motivation: 验证并发程序的串行化（即每个并发执行是否等价于某个串行执行）是重要但困难的问题，现有方法难以处理无限线程和无限步数的情况

Method: 1. 设计SER建模语言，限制程序使串行化问题可判定；2. 将SER程序编译为网络系统抽象；3. 将串行化验证简化为Petri网可达性查询；4. 通过Petri网切片、半线性集压缩和Presburger公式操作等优化缩减搜索空间

Result: 实现了首个自动化端到端决策程序，能够为串行化生成可检查证书或为反例生成追踪，成功处理了包括有状态防火墙、BGP路由器在内的真实程序模型

Conclusion: 尽管串行化验证在理论上很困难，但通过SER语言和优化的Petri网可达性方法，能够有效处理实际程序的串行化验证问题

Abstract: We present the SER modeling language for automatically verifying serializability of concurrent programs, i.e., whether every concurrent execution of the program is equivalent to some serial execution.
  SER programs are suitably restricted to make this problem decidable, while still allowing for an unbounded number of concurrent threads of execution, each potentially running for an unbounded number of steps.
  Building on prior theoretical results, we give the first automated end-to-end decision procedure that either proves serializability by producing a checkable certificate, or refutes it by producing a counterexample trace.
  We also present a network-system abstraction to which SER programs compile. Our decision procedure then reduces serializability in this setting to a Petri net reachability query.
  Furthermore, in order to scale, we curtail the search space via multiple optimizations, including Petri net slicing, semilinear-set compression, and Presburger-formula manipulation.
  We extensively evaluate our framework and show that, despite the theoretical hardness of the problem, it can successfully handle various models of real-world programs, including stateful firewalls, BGP routers, and more.

</details>
