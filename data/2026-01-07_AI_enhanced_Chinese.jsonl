{"id": "2601.02653", "pdf": "https://arxiv.org/pdf/2601.02653", "abs": "https://arxiv.org/abs/2601.02653", "authors": ["Ajay Brahmakshatriya", "Saman Amarasinghe", "Martin Rinard"], "title": "Backwards Data-Flow Analysis using Prophecy Variable in the BuildIt System", "categories": ["cs.PL"], "comment": null, "summary": "Many program transformations and optimizations require information about the future behavior of the program. A standard way to obtain this information is to build an intermediate program representation, then use a backwards program analysis to propagate relevant information against the flow of control back to the transformation/optimization site. We instead propose to use prophecy variables, which predict information about the future execution of the program, to enable such transformations and optimizations. We implement prophecy variables in BuildIt, a lightweight domain specific language implementation system. BuildIt uses staged compilation to implement high performance domain specific languages embedded within a standard general purpose programming language (C++). The BuildIt first phase uses standard C++ program execution to generate optimized C, C++, and CUDA second phase code. This approach enables BuildIt to eliminate programming language implementation components such as parsers and intermediate representations, delivering a dramatic decrease in the engineering effort required to implement domain specific languages. The combination of prophecy variables and repeated forward program execution enables BuildIt to extend this approach to include transformations and optimizations that require information about the future execution of the program without backwards analyses and without the engineering overhead associated with implementing these analyses. We formalize the use of prophecy variables for this purpose, discuss the implementation of prophecy variables and repeated execution in BuildIt, and present experimental results for BuildIt computations that benefit from optimizations enabled by the information that prophecy variables provide.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u9884\u8a00\u53d8\u91cf\uff08prophecy variables\uff09\u6765\u9884\u6d4b\u7a0b\u5e8f\u672a\u6765\u6267\u884c\u4fe1\u606f\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684\u53cd\u5411\u7a0b\u5e8f\u5206\u6790\uff0c\u4ece\u800c\u7b80\u5316\u9700\u8981\u672a\u6765\u884c\u4e3a\u4fe1\u606f\u7684\u7a0b\u5e8f\u8f6c\u6362\u548c\u4f18\u5316\u3002", "motivation": "\u8bb8\u591a\u7a0b\u5e8f\u8f6c\u6362\u548c\u4f18\u5316\u9700\u8981\u4e86\u89e3\u7a0b\u5e8f\u672a\u6765\u884c\u4e3a\u4fe1\u606f\u3002\u4f20\u7edf\u65b9\u6cd5\u901a\u8fc7\u6784\u5efa\u4e2d\u95f4\u7a0b\u5e8f\u8868\u793a\u5e76\u4f7f\u7528\u53cd\u5411\u7a0b\u5e8f\u5206\u6790\u6765\u4f20\u64ad\u76f8\u5173\u4fe1\u606f\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u5b9e\u73b0\u590d\u6742\u4e14\u5de5\u7a0b\u5f00\u9500\u5927\u3002", "method": "\u5728BuildIt\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u9884\u8a00\u53d8\u91cf\uff0c\u8fd9\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u9886\u57df\u7279\u5b9a\u8bed\u8a00\u5b9e\u73b0\u7cfb\u7edf\u3002BuildIt\u4f7f\u7528\u5206\u9636\u6bb5\u7f16\u8bd1\uff0c\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u6807\u51c6C++\u7a0b\u5e8f\u6267\u884c\u751f\u6210\u4f18\u5316\u7684C\u3001C++\u548cCUDA\u7b2c\u4e8c\u9636\u6bb5\u4ee3\u7801\u3002\u7ed3\u5408\u9884\u8a00\u53d8\u91cf\u548c\u91cd\u590d\u524d\u5411\u7a0b\u5e8f\u6267\u884c\uff0c\u65e0\u9700\u53cd\u5411\u5206\u6790\u5373\u53ef\u5b9e\u73b0\u9700\u8981\u672a\u6765\u6267\u884c\u4fe1\u606f\u7684\u8f6c\u6362\u548c\u4f18\u5316\u3002", "result": "BuildIt\u80fd\u591f\u6d88\u9664\u89e3\u6790\u5668\u548c\u4e2d\u95f4\u8868\u793a\u7b49\u7f16\u7a0b\u8bed\u8a00\u5b9e\u73b0\u7ec4\u4ef6\uff0c\u663e\u8457\u51cf\u5c11\u9886\u57df\u7279\u5b9a\u8bed\u8a00\u5b9e\u73b0\u7684\u5de5\u7a0b\u5de5\u4f5c\u91cf\u3002\u9884\u8a00\u53d8\u91cf\u4f7fBuildIt\u80fd\u591f\u6269\u5c55\u8fd9\u79cd\u65b9\u6cd5\uff0c\u652f\u6301\u9700\u8981\u672a\u6765\u6267\u884c\u4fe1\u606f\u7684\u4f18\u5316\uff0c\u800c\u65e0\u9700\u5b9e\u73b0\u53cd\u5411\u5206\u6790\u3002", "conclusion": "\u9884\u8a00\u53d8\u91cf\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u83b7\u53d6\u7a0b\u5e8f\u672a\u6765\u6267\u884c\u4fe1\u606f\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684\u53cd\u5411\u7a0b\u5e8f\u5206\u6790\uff0c\u7b80\u5316\u4e86\u9700\u8981\u8fd9\u7c7b\u4fe1\u606f\u7684\u7a0b\u5e8f\u8f6c\u6362\u548c\u4f18\u5316\u7684\u5b9e\u73b0\u8fc7\u7a0b\u3002"}}
{"id": "2601.02563", "pdf": "https://arxiv.org/pdf/2601.02563", "abs": "https://arxiv.org/abs/2601.02563", "authors": ["Viacheslav Siniaev", "Iaroslav Chelombitko", "Aleksey Komissarov"], "title": "Compressed code: the hidden effects of quantization and distillation on programming tokens", "categories": ["cs.SE", "cs.CL", "cs.LG", "cs.PL"], "comment": "18 pages, 1 figure and 6 tables", "summary": "Large Language Models (LLMs) have demonstrated exceptional code generation capabilities, yet their token-level mechanisms remain underexplored, particularly in compressed models. Through systematic analysis of programming language token representations, we characterize how programming languages are encoded in LLM tokenizers by analyzing their vocabulary distribution and keyword coverage patterns. We introduce a novel cold-start probability analysis method that provides insights into model behavior without requiring explicit prompts. Additionally, we present a comprehensive evaluation of how different model optimization techniques - including quantization, distillation, model scaling, and task-specific fine-tuning - affect token-level representations and code generation quality. Our experiments, supported by comprehensive probability distribution analysis and evaluation metrics, reveal critical insights into token-level behavior and provide empirically-validated guidelines for maintaining code generation quality under various optimization constraints. These findings advance both theoretical understanding of LLM code generation and practical implementation of optimized models in production environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u5206\u6790\u4e86LLM\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684token\u7ea7\u673a\u5236\uff0c\u7279\u522b\u662f\u538b\u7f29\u6a21\u578b\u4e2d\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u8bcd\u6c47\u5206\u5e03\u548c\u5173\u952e\u8bcd\u8986\u76d6\u6a21\u5f0f\u7814\u7a76\u7f16\u7a0b\u8bed\u8a00\u7f16\u7801\u65b9\u5f0f\uff0c\u5e76\u8bc4\u4f30\u4e0d\u540c\u4f18\u5316\u6280\u672f\u5bf9token\u8868\u793a\u548c\u4ee3\u7801\u751f\u6210\u8d28\u91cf\u7684\u5f71\u54cd\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176token\u7ea7\u673a\u5236\uff08\u5c24\u5176\u662f\u5728\u538b\u7f29\u6a21\u578b\u4e2d\uff09\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u9700\u8981\u7406\u89e3\u7f16\u7a0b\u8bed\u8a00\u5728LLM\u5206\u8bcd\u5668\u4e2d\u7684\u7f16\u7801\u65b9\u5f0f\uff0c\u4ee5\u53ca\u4e0d\u540c\u4f18\u5316\u6280\u672f\u5982\u4f55\u5f71\u54cdtoken\u7ea7\u8868\u793a\u548c\u4ee3\u7801\u751f\u6210\u8d28\u91cf\u3002", "method": "1. \u5206\u6790\u7f16\u7a0b\u8bed\u8a00token\u8868\u793a\u7684\u8bcd\u6c47\u5206\u5e03\u548c\u5173\u952e\u8bcd\u8986\u76d6\u6a21\u5f0f\uff1b2. \u5f15\u5165\u65b0\u9896\u7684\u51b7\u542f\u52a8\u6982\u7387\u5206\u6790\u65b9\u6cd5\uff0c\u65e0\u9700\u663e\u5f0f\u63d0\u793a\u5373\u53ef\u6d1e\u5bdf\u6a21\u578b\u884c\u4e3a\uff1b3. \u5168\u9762\u8bc4\u4f30\u91cf\u5316\u3001\u84b8\u998f\u3001\u6a21\u578b\u7f29\u653e\u548c\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u7b49\u4f18\u5316\u6280\u672f\u5bf9token\u7ea7\u8868\u793a\u548c\u4ee3\u7801\u751f\u6210\u8d28\u91cf\u7684\u5f71\u54cd\uff1b4. \u4f7f\u7528\u6982\u7387\u5206\u5e03\u5206\u6790\u548c\u8bc4\u4f30\u6307\u6807\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u63ed\u793a\u4e86token\u7ea7\u884c\u4e3a\u7684\u5173\u952e\u6d1e\u5bdf\uff0c\u63d0\u4f9b\u4e86\u7ecf\u9a8c\u9a8c\u8bc1\u7684\u6307\u5bfc\u539f\u5219\uff0c\u7528\u4e8e\u5728\u5404\u79cd\u4f18\u5316\u7ea6\u675f\u4e0b\u4fdd\u6301\u4ee3\u7801\u751f\u6210\u8d28\u91cf\u3002\u7814\u7a76\u53d1\u73b0\u4e0d\u540c\u4f18\u5316\u6280\u672f\u5bf9token\u8868\u793a\u548c\u4ee3\u7801\u751f\u6210\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e0d\u4ec5\u63a8\u8fdb\u4e86\u5bf9LLM\u4ee3\u7801\u751f\u6210\u7684\u7406\u8bba\u7406\u89e3\uff0c\u8fd8\u4e3a\u751f\u4ea7\u73af\u5883\u4e2d\u4f18\u5316\u6a21\u578b\u7684\u5b9e\u8df5\u5b9e\u65bd\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u6709\u52a9\u4e8e\u5728\u4fdd\u6301\u4ee3\u7801\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u5e94\u7528\u5404\u79cd\u6a21\u578b\u4f18\u5316\u6280\u672f\u3002"}}
{"id": "2601.03249", "pdf": "https://arxiv.org/pdf/2601.03249", "abs": "https://arxiv.org/abs/2601.03249", "authors": ["Leen Lambers", "Oszk\u00e1r Semer\u00e1th"], "title": "Proceedings 16th International Workshop on Graph Computation Models", "categories": ["cs.LO", "cs.FL", "cs.PL", "cs.SE"], "comment": null, "summary": "This volume contains the post-proceedings of the Sixteenth International Workshop on Graph Computation Models (GCM 2025). The workshops took place in Koblenz, Germany on June 10 as part of STAF (Software Technologies: Applications and Foundations).  \n  Graphs are common mathematical structures that are visual and intuitive. They constitute a natural and seamless way for system modeling in science, engineering, and beyond, including computer science, biology, and business process modeling. Graph computation models constitute a class of very high-level models where graphs are first-class citizens. The aim of the International GCM Workshop series is to bring together researchers interested in all aspects of computation models based on graphs and graph transformation. It promotes the cross-fertilizing exchange of ideas and experiences among senior and young researchers from the different communities interested in the foundations, applications, and implementations of graph computation models and related areas.", "AI": {"tldr": "GCM 2025 \u662f\u7b2c\u5341\u516d\u5c4a\u56fd\u9645\u56fe\u8ba1\u7b97\u6a21\u578b\u7814\u8ba8\u4f1a\u7684\u8bba\u6587\u96c6\uff0c\u8be5\u7814\u8ba8\u4f1a\u4f5c\u4e3aSTAF\u4f1a\u8bae\u7684\u4e00\u90e8\u5206\u5728\u5fb7\u56fd\u79d1\u5e03\u4f26\u8328\u4e3e\u884c\uff0c\u805a\u7126\u4e8e\u56fe\u4f5c\u4e3a\u7cfb\u7edf\u5efa\u6a21\u6838\u5fc3\u5de5\u5177\u7684\u7814\u7a76\u4e0e\u5e94\u7528\u3002", "motivation": "\u56fe\u4f5c\u4e3a\u76f4\u89c2\u7684\u6570\u5b66\u7ed3\u6784\uff0c\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u3001\u751f\u7269\u5b66\u3001\u4e1a\u52a1\u6d41\u7a0b\u5efa\u6a21\u7b49\u591a\u4e2a\u9886\u57df\u662f\u81ea\u7136\u4e14\u65e0\u7f1d\u7684\u7cfb\u7edf\u5efa\u6a21\u65b9\u5f0f\u3002\u56fe\u8ba1\u7b97\u6a21\u578b\u5c06\u56fe\u4f5c\u4e3a\u4e00\u7b49\u516c\u6c11\uff0c\u5c5e\u4e8e\u975e\u5e38\u9ad8\u7ea7\u7684\u6a21\u578b\u3002GCM\u7814\u8ba8\u4f1a\u65e8\u5728\u6c47\u96c6\u5bf9\u57fa\u4e8e\u56fe\u548c\u56fe\u53d8\u6362\u7684\u8ba1\u7b97\u6a21\u578b\u5404\u65b9\u9762\u611f\u5174\u8da3\u7684\u7814\u7a76\u4eba\u5458\u3002", "method": "\u901a\u8fc7\u56fd\u9645\u7814\u8ba8\u4f1a\u5f62\u5f0f\uff0c\u4fc3\u8fdb\u4e0d\u540c\u793e\u533a\uff08\u5305\u62ec\u8d44\u6df1\u548c\u5e74\u8f7b\u7814\u7a76\u4eba\u5458\uff09\u5728\u7406\u8bba\u57fa\u7840\u3001\u5e94\u7528\u548c\u5b9e\u73b0\u65b9\u9762\u7684\u601d\u60f3\u4e0e\u7ecf\u9a8c\u4ea4\u6d41\uff0c\u5b9e\u73b0\u8de8\u9886\u57df\u7684\u4ea4\u53c9\u878d\u5408\u3002", "result": "\u672c\u5377\u6536\u5f55\u4e86GCM 2025\u7814\u8ba8\u4f1a\u7684\u540e\u8bba\u6587\u96c6\uff0c\u8bb0\u5f55\u4e86\u5728\u56fe\u8ba1\u7b97\u6a21\u578b\u9886\u57df\u7684\u6700\u65b0\u7814\u7a76\u6210\u679c\u548c\u8ba8\u8bba\u3002", "conclusion": "GCM\u7814\u8ba8\u4f1a\u7cfb\u5217\u6301\u7eed\u63a8\u52a8\u56fe\u8ba1\u7b97\u6a21\u578b\u9886\u57df\u7684\u53d1\u5c55\uff0c\u901a\u8fc7\u8de8\u5b66\u79d1\u4ea4\u6d41\u4fc3\u8fdb\u8be5\u9886\u57df\u5728\u7406\u8bba\u57fa\u7840\u3001\u5e94\u7528\u5b9e\u8df5\u548c\u5b9e\u73b0\u6280\u672f\u65b9\u9762\u7684\u8fdb\u6b65\u3002"}}
