{"id": "2601.12385", "pdf": "https://arxiv.org/pdf/2601.12385", "abs": "https://arxiv.org/abs/2601.12385", "authors": ["Feifei Li", "Xiao Chen", "Xiaoyu Sun", "Xi Xiao", "Shaohua Wang", "Yong Ding", "Sheng Wen", "Qing Li"], "title": "Context-Free Grammar Inference for Complex Programming Languages in Black Box Settings", "categories": ["cs.PL"], "comment": null, "summary": "Grammar inference for complex programming languages remains a significant challenge, as existing approaches fail to scale to real world datasets within practical time constraints. In our experiments, none of the state-of-the-art tools, including Arvada, Treevada and Kedavra were able to infer grammars for complex languages such as C, C++, and Java within 48 hours. Arvada and Treevada perform grammar inference directly on full-length input examples, which proves inefficient for large files commonly found in such languages. While Kedavra introduces data decomposition to create shorter examples for grammar inference, its lexical analysis still relies on the original inputs. Additionally, its strict no-overgeneralization constraint limits the construction of complex grammars.\n  To overcome these limitations, we propose Crucio, which builds a decomposition forest to extract short examples for lexical and grammar inference via a distributional matrix. Experimental results show that Crucio is the only method capable of successfully inferring grammars for complex programming languages (where the number of nonterminals is up to 23x greater than in prior benchmarks) within reasonable time limits. On the prior simple benchmark, Crucio achieves an average recall improvement of 1.37x and 1.19x over Treevada and Kedavra, respectively, and improves F1 scores by 1.21x and 1.13x."}
{"id": "2601.12741", "pdf": "https://arxiv.org/pdf/2601.12741", "abs": "https://arxiv.org/abs/2601.12741", "authors": ["Gyeongwon Jeong", "Seonghun Park", "Hongseok Yang"], "title": "An Introduction to Razborov's Flag Algebra as a Proof System for Extremal Graph Theory", "categories": ["cs.PL", "cs.LO", "math.CO"], "comment": null, "summary": "Razborov's flag algebra forms a powerful framework for deriving asymptotic inequalities between induced subgraph densities, underpinning many advances in extremal graph theory. This survey introduces flag algebra to computer scientists working in logic, programming languages, automated verification, and formal methods. We take a logical perspective on flag algebra and present it in terms of syntax, semantics, and proof strategies, in a style closer to formal logic. One popular proof strategy derives valid inequalities by first proving inequalities in a labelled variant of flag algebra and then transferring them to the original unlabelled setting using the so-called downward operator. We explain this strategy in detail and highlight that its transfer mechanism relies on the notion of what we call an adjoint pair, reminiscent of Galois connections and categorical adjunctions, which appear frequently in work on automated verification and programming languages. Along the way, we work through representative examples, including Mantel's theorem and Goodman's bound on Ramsey multiplicity, to illustrate how mathematical arguments can be carried out symbolically in the flag algebra framework."}
{"id": "2601.12813", "pdf": "https://arxiv.org/pdf/2601.12813", "abs": "https://arxiv.org/abs/2601.12813", "authors": ["Keyin Wang", "Xiaomu Shi", "Jiaxiang Liu", "Zhilin Wu", "Taolve Chen", "Fu Song", "David N. Jansen"], "title": "A Formally Verified Procedure for Width Inference in FIRRTL", "categories": ["cs.PL", "cs.LO"], "comment": "Arxiv version for the European Symposium on Programming (ESOP 2026)(to appear) This work was supported by the Strategic Priority Research Program of the Chinese Academy of Sciences, Grant No.~XDA0320101, and partially supported by NSFC-RGC Collaborative Research Grant No.~62561160151. D.N. Jansen is supported by Beijing Natural Science Foundation Project No.~IS25071", "summary": "FIRRTL is an intermediate representation language for Register Transfer Level (RTL) hardware designs. In FIRRTL programs, the bit widths of many components are not specified explicitly and must be inferred during compilation. In mainstream FIRRTL compilers, such as the official compiler firtool, width inference is conducted by a compilation pass referred to as InferWidths, which may fail even for simple FIRRTL programs. In this paper, we thoroughly investigate the width inference problem for FIRRTL programs. We show that, if the constraints obtained from a FIRRTL program are satisfiable, there exists a unique least solution. Based on this result, we propose a complete procedure for solving the width inference problem. We implement it in the interactive theorem prover Rocq and prove its functional correctness. From the Rocq implementation, we extract an OCaml implementation, which is the first formally verified implementation of the InferWidths pass. Extensive experiments demonstrate that our approach can solve more instances than the official InferWidths pass in firtool, normally with high efficiency."}
{"id": "2601.12943", "pdf": "https://arxiv.org/pdf/2601.12943", "abs": "https://arxiv.org/abs/2601.12943", "authors": ["Han Xu", "Di Wang"], "title": "Dependently-Typed AARA: A Non-Affine Approach for Resource Analysis of Higher-Order Programs", "categories": ["cs.PL"], "comment": null, "summary": "Static resource analysis determines the resource consumption (e.g., time complexity) of a program without executing it. Among the numerous existing approaches for resource analysis, affine type systems have been one dominant approach. However, these affine type systems fall short of deriving precise resource behavior of higher-order programs, particularly in cases that involve partial applications.\n  This article presents λ_\\ms{amor}^\\ms{na}}, a non-affine AARA-style dependent type system for resource reasoning about higher-order functional programs. The key observation is that the main issue in previous approaches comes from (i) the close coupling of types and resources, and (ii) the conflict between affine and higher-order typing mechanisms. To derive precise resource behavior of higher-order functions, λ_\\ms{amor}^\\ms{na}} decouples resources from types and follows a non-affine typing mechanism. The non-affine type system of λ_\\ms{amor}^\\ms{na}} achieves this by using dependent types, which allows expressing type-level potential functions separate from ordinary types. This article formalizes λ_\\ms{amor}^\\ms{na}}'s syntax and semantics, and proves its soundness, which guarantees the correctness of resource bounds. Several challenging classic and higher-order examples are presented to demonstrate the expressiveness and compositionality of λ_\\ms{amor}^\\ms{na}}'s reasoning capability."}
{"id": "2601.13224", "pdf": "https://arxiv.org/pdf/2601.13224", "abs": "https://arxiv.org/abs/2601.13224", "authors": ["Michael Hanus", "Steven Libby"], "title": "Functional Logic Program Transformations", "categories": ["cs.PL"], "comment": "Presented at Conference on Declarative Programming (DECLARE 2025)", "summary": "Many tools used to process programs, like compilers, analyzers, or verifiers, perform transformations on their intermediate program representation, like abstract syntax trees. Implementing such program transformations is a non-trivial task, since it is necessary to iterate over the complete syntax tree and apply various transformations at nodes in a tree. In this paper we show how the features of functional logic programming are useful to implement program transformations in a compact and comprehensible manner. For this purpose, we propose to write program transformations as partially defined and non-deterministic operations. Since the implementation of non-determinism usually causes some overhead compared to deterministically defined operations, we compare our approach to a deterministic transformation method. We evaluate these alternatives for the functional logic language Curry and its intermediate representation FlatCurry which is used in various analysis and verification tools and compilers."}
{"id": "2601.13341", "pdf": "https://arxiv.org/pdf/2601.13341", "abs": "https://arxiv.org/abs/2601.13341", "authors": ["Namratha Gangamreddypalli", "Constantin Enea", "Shaz Qadeer"], "title": "Reduction for Structured Concurrent Programs", "categories": ["cs.PL"], "comment": null, "summary": "Commutativity reasoning based on Lipton's movers is a powerful technique for verification of concurrent programs. The idea is to define a program transformation that preserves a subset of the initial set of interleavings, which is sound modulo reorderings of commutative actions. Scaling commutativity reasoning to routinely-used features in software systems, such as procedures and parallel composition, remains a significant challenge.\n  In this work, we introduce a novel reduction technique for structured concurrent programs that unifies two key advances. First, we present a reduction strategy that soundly replaces parallel composition with sequential composition. Second, we generalize Lipton's reduction to support atomic sections containing (potentially recursive) procedure calls. Crucially, these two foundational strategies can be composed arbitrarily, greatly expanding the scope and flexibility of reduction-based reasoning. We implemented this technique in Civl and demonstrated its effectiveness on a number of challenging case studies, including a snapshot object, a fault-tolerant and linearizable register, the FLASH cache coherence protocol, and a non-trivial variant of Two-Phase Commit."}
{"id": "2601.13727", "pdf": "https://arxiv.org/pdf/2601.13727", "abs": "https://arxiv.org/abs/2601.13727", "authors": ["Bart Jacobs"], "title": "Foundational VeriFast: Pragmatic Certification of Verification Tool Results through Hinted Mirroring", "categories": ["cs.PL"], "comment": "8 pages, 2 figures", "summary": "VeriFast is a leading tool for the modular formal verification of correctness properties of single-threaded and multi-threaded C and Rust programs. It verifies a program by symbolically executing each function in isolation, exploiting user-annotated preconditions, postconditions, and loop invariants written in a form of separation logic, and using a separation logic-based symbolic representation of memory. However, the tool itself, written in roughly 30K lines of OCaml code, has not been formally verified. Therefore, bugs in the tool could cause it to falsely report the correctness of the input program. We here report on an early result extending VeriFast to emit, upon successful verification of a Rust program, a Rocq proof script that proves correctness of the program with respect to a Rocq-encoded axiomatic semantics of Rust. This significantly enhances VeriFast's applicability in safety-critical domains. We apply hinted mirroring: we record key information from VeriFast's symbolic execution run, and use it to direct a replay of the run in Rocq."}
{"id": "2601.13991", "pdf": "https://arxiv.org/pdf/2601.13991", "abs": "https://arxiv.org/abs/2601.13991", "authors": ["Darion Haase", "Kevin Batz", "Adrian Gallus", "Benjamin Lucien Kaminski", "Joost-Pieter Katoen", "Lutz Klinkenberg", "Tobias Winkler"], "title": "Generating Functions Meet Occupation Measures: Invariant Synthesis for Probabilistic Loops (Extended Version)", "categories": ["cs.PL"], "comment": "Full version of ESOP2026 paper 'Generating Functions Meet Occupation Measures: Invariant Synthesis for Probabilistic Loops'", "summary": "A fundamental computational task in probabilistic programming is to infer a program's output (posterior) distribution from a given initial (prior) distribution. This problem is challenging, especially for expressive languages that feature loops or unbounded recursion. While most of the existing literature focuses on statistical approximation, in this paper we address the problem of mathematically exact inference.\n  To achieve this for programs with loops, we rely on a relatively underexplored type of probabilistic loop invariant, which is linked to a loop's so-called occupation measure. The occupation measure associates program states with their expected number of visits, given the initial distribution. Based on this, we derive the notion of an occupation invariant. Such invariants are essentially dual to probabilistic martingales, the predominant technique for formal probabilistic loop analysis in the literature. A key feature of occupation invariants is that they can take the initial distribution into account and often yield a proof of positive almost sure termination as a by-product.\n  Finally, we present an automatic, template-based invariant synthesis approach for occupation invariants by encoding them as generating functions. The approach is implemented and evaluated on a set of benchmarks."}
{"id": "2601.14059", "pdf": "https://arxiv.org/pdf/2601.14059", "abs": "https://arxiv.org/abs/2601.14059", "authors": ["Andrea Gilot", "Axel Bergström", "Eva Darulova"], "title": "Verifying Floating-Point Programs in Stainless", "categories": ["cs.PL", "cs.LO"], "comment": null, "summary": "We extend the Stainless deductive verifier with floating-point support, providing the first automated verification support for floating-point numbers for a subset of Scala that includes polymorphism, recursion and higher-order functions. We follow the recent approach in the KeY verifier to axiomatise reasoning about mathematical functions, but go further by supporting all functions from Scala's math API, and by verifying the correctness of the axioms against the actual implementation in Stainless itself. We validate Stainless' floating-point support on a new set of benchmarks sampled from real-world code from GitHub, showing that it can verify specifications about, e.g., ranges of output or absence of special values for most supported functions, or produce counter-examples when the specifications do not hold."}
{"id": "2601.14114", "pdf": "https://arxiv.org/pdf/2601.14114", "abs": "https://arxiv.org/abs/2601.14114", "authors": ["Liam Chung", "Tobias Kappé"], "title": "Partial Reductions for Kleene Algebra with Linear Hypotheses", "categories": ["cs.PL"], "comment": null, "summary": "Kleene algebra (KA) is an important tool for reasoning about general program equivalences, with a decidable and complete equational theory. However, KA cannot always prove equivalences between specific programs. For this purpose, one adds hypotheses to KA that encode program-specific knowledge. Traditionally, a map on regular expressions called a reduction then lets us lift decidability and completeness to these more expressive systems. Explicitly constructing such a reduction requires significant labour. Moreover, due to regularity constraints, a reduction may not exist for all combinations of expression and hypothesis.\n  We describe an automaton-based construction to mechanically derive reductions for a wide class of hypotheses. These reductions can be partial, in which case they yield partial completeness: completeness for expressions in their domain. This allows us to automatically establish the provability of more equivalences than what is covered in existing work."}
{"id": "2601.12270", "pdf": "https://arxiv.org/pdf/2601.12270", "abs": "https://arxiv.org/abs/2601.12270", "authors": ["Reshabh K Sharma", "Dan Grossman", "David Kohlbrenner"], "title": "SplittingSecrets: A Compiler-Based Defense for Preventing Data Memory-Dependent Prefetcher Side-Channels", "categories": ["cs.CR", "cs.PL"], "comment": null, "summary": "Traditional side-channels take advantage of secrets being used as inputs to unsafe instructions, used for memory accesses, or used in control flow decisions. Constant-time programming, which restricts such code patterns, has been widely adopted as a defense against these vulnerabilities. However, new hardware optimizations in the form of Data Memory-dependent Prefetchers (DMP) present in Apple, Intel, and ARM CPUs have shown such defenses are not sufficient. These prefetchers, unlike classical prefetchers, use the content of memory as well as the trace of prior accesses to determine prefetch targets. An adversary abusing such a prefetcher has been shown to be able to mount attacks leaking data-at-rest; data that is never used by the program, even speculatively, in an unsafe manner.\n  In response, this paper introduces SplittingSecrets, a compiler-based tool that can harden software libraries against side-channels arising from DMPs. SplittingSecrets's approach avoids reasoning about the complex internals of different DMPs and instead relies on one key aspect of all DMPs: activation requires data to resemble addresses. To prevent secret data from leaking, SplittingSecrets transforms memory operations to ensure that secrets are never stored in memory in a manner resembling an address, thereby avoiding DMP activation on those secrets. Rather than disable a DMP entirely, SplittingSecrets can provide targeted hardening for only specific secrets entirely in software.\n  We have implemented SplittingSecrets using LLVM, supporting both source-level memory operations and those generated by the compiler backend for the AArch64 architecture, We have analyzed the performance overhead involved in safeguarding secrets from DMP-induced attacks using common primitives in libsodium, a popular cryptographic library when built for Apple M-series CPUs."}
{"id": "2601.13040", "pdf": "https://arxiv.org/pdf/2601.13040", "abs": "https://arxiv.org/abs/2601.13040", "authors": ["Harry Fitchett", "Charles Fox"], "title": "CPU-less parallel execution of lambda calculus in digital logic", "categories": ["cs.DC", "cs.AR", "cs.PL"], "comment": null, "summary": "While transistor density is still increasing, clock speeds are not, motivating the search for new parallel architectures. One approach is to completely abandon the concept of CPU -- and thus serial imperative programming -- and instead to specify and execute tasks in parallel, compiling from programming languages to data flow digital logic. It is well-known that pure functional languages are inherently parallel, due to the Church-Rosser theorem, and CPU-based parallel compilers exist for many functional languages. However, these still rely on conventional CPUs and their von Neumann bottlenecks. An alternative is to compile functional languages directly into digital logic to maximize available parallelism. It is difficult to work with complete modern functional languages due to their many features, so we demonstrate a proof-of-concept system using lambda calculus as the source language and compiling to digital logic. We show how functional hardware can be tailored to a simplistic functional language, forming the ground for a new model of CPU-less functional computation. At the algorithmic level, we use a tree-based representation, with data localized within nodes and communicated data passed between them. This is implemented by physical digital logic blocks corresponding to nodes, and buses enabling message passing. Node types and behaviors correspond to lambda grammar forms, and beta-reductions are performed in parallel allowing branches independent from one another to perform transformations simultaneously. As evidence for this approach, we present an implementation, along with simulation results, showcasing successful execution of lambda expressions. This suggests that the approach could be scaled to larger functional languages. Successful execution of a test suite of lambda expressions suggests that the approach could be scaled to larger functional languages."}
{"id": "2601.13325", "pdf": "https://arxiv.org/pdf/2601.13325", "abs": "https://arxiv.org/abs/2601.13325", "authors": ["Raz Lotan", "Neta Elad", "Oded Padon", "Sharon Shoham"], "title": "Verifying First-Order Temporal Properties of Infinite-State Systems via Timers and Rankings", "categories": ["cs.LO", "cs.PL"], "comment": null, "summary": "We present a unified deductive verification framework for first-order temporal properties based on well-founded rankings, where verification conditions are discharged using SMT solvers. To that end, we introduce a novel reduction from verification of arbitrary temporal properties to verification of termination. Our reduction augments the system with prophecy timer variables that predict the number of steps along a trace until the next time certain temporal formulas, including the negated property, hold. In contrast to standard tableaux-based reductions, which reduce the problem to fair termination, our reduction does not introduce fairness assumptions. To verify termination of the augmented system, we follow the traditional approach of assigning each state a rank from a well-founded set and showing that the rank decreases in every transition. We leverage the recently proposed formalism of implicit rankings to express and automatically verify the decrease of rank using SMT solvers, even when the rank is not expressible in first-order logic. We extend implicit rankings from finite to infinite domains, enabling verification of more general systems and making them applicable to the augmented systems generated by our reduction, which allows us to exploit the decrease of timers in termination proofs. We evaluate our technique on a range of temporal verification tasks from previous works, giving simple, intuitive proofs for them within our framework."}
{"id": "2601.13398", "pdf": "https://arxiv.org/pdf/2601.13398", "abs": "https://arxiv.org/abs/2601.13398", "authors": ["Nickil Maveli", "Antonio Vergari", "Shay B. Cohen"], "title": "Can LLMs Compress (and Decompress)? Evaluating Code Understanding and Execution via Invertibility", "categories": ["cs.LG", "cs.AI", "cs.PL"], "comment": "32 pages (preprint)", "summary": "LLMs demonstrate strong performance on code benchmarks, yet round-trip code execution reveals limitations in their ability to maintain consistent reasoning across forward and backward execution. We present RoundTripCodeEval (RTCE), a comprehensive benchmark consisting of four distinct code execution reasoning tasks designed to rigorously test round-trip consistency. RTCE provides an execution-free, exact-match evaluation of bijection fidelity, assessing whether models preserve a consistent one-to-one mapping between encoding and decoding operations across various algorithms and directions. We systematically evaluate state-of-the-art Code-LLMs using zero-shot prompting, supervised fine-tuning on execution traces, and self-reflection mechanisms. Each yields modest improvements, but none closes the gap, indicating that current LLMs struggle with true round-trip consistency, which demonstrates that they lack the internal coherence required for trustworthy code reasoning. RTCE surfaces several new and previously unmeasured insights that are not captured by existing I/O-prediction, execution-reasoning, or round-trip natural-language benchmarks. We will release the code and the dataset upon acceptance."}
{"id": "2601.13682", "pdf": "https://arxiv.org/pdf/2601.13682", "abs": "https://arxiv.org/abs/2601.13682", "authors": ["Jianfeng Cai", "Jinhua Zhu", "Ruopei Sun", "Kangwen Zhao", "Dongyun Xue", "Mingxiao Feng", "Wengang Zhou", "Houqiang Li"], "title": "CodeContests-O: Powering LLMs via Feedback-Driven Iterative Test Case Generation", "categories": ["cs.SE", "cs.PL"], "comment": null, "summary": "The rise of reasoning models necessitates large-scale verifiable data, for which programming tasks serve as an ideal source. However, while competitive programming platforms provide abundant problems and solutions, high-quality test cases for verification remain scarce. Existing approaches attempt to synthesize test cases using Large Language Models (LLMs), but rely solely on the model's intrinsic generation capabilities without external feedback, frequently resulting in insufficiently diverse cases. To address this limitation, we propose a $\\textbf{Feedback-Driven Iterative Framework}$ for comprehensive test case construction. Specifically, our method leverages the LLM to generate initial test cases, executes them against known correct and incorrect solutions, and utilizes the failed results as feedback to guide the LLM in refining the test cases toward high fidelity and discriminability. We then apply this method to the CodeContests dataset to construct an optimized high-quality derivative, $\\textbf{CodeContests-O}$. Evaluating against the entire pool of solutions ($1.1 \\times 10^7$ in total), our dataset achieves an average True Positive Rate (TPR) of $89.37\\%$ and True Negative Rate (TNR) of $90.89\\%$, significantly outperforming the CodeContests and CodeContests+ by margins of $4.32\\%$ and $9.37\\%$, respectively. Furthermore, fine-tuning the Qwen2.5-7B model on CodeContests-O results in a $9.52\\%$ improvement on LiveCodeBench (Pass@1). Experiments demonstrate the effectiveness of our framework and the quality of CodeContests-O. To support reproducibility and facilitate future research, we release the $\\href{https://github.com/cai-jianfeng/CodeContests-O}{code}$ and $\\href{https://huggingface.co/datasets/caijanfeng/CodeContests-O}{dataset}$."}
{"id": "2601.14252", "pdf": "https://arxiv.org/pdf/2601.14252", "abs": "https://arxiv.org/abs/2601.14252", "authors": ["Tristan Simas"], "title": "Identification capacity and rate-query tradeoffs in classification systems", "categories": ["cs.IT", "cs.PL"], "comment": "15 pages, 1 table. Lean 4 formalization (6,100+ lines, 0 sorry) included in source and archived at https://doi.org/10.5281/zenodo.18261188", "summary": "We study a one-shot identification analogue of rate-distortion for discrete classification under three resources: tag rate L (bits of side information stored per entity), identification cost W (attribute-membership queries per identification, excluding global preprocessing and amortized caching), and distortion D (misclassification probability). The question is to characterize achievable triples (L,W,D) when a decoder must recover an entity's class from limited observations. Zero-error barrier. If two distinct classes induce the same attribute profile, then the observation pi(V) is identical for both and no decoder can identify the class from attribute queries alone. Thus, if the profile map pi is not injective on classes, zero-error identification without tags is impossible (a zero-error feasibility threshold). Achievability and converse at D=0. With k classes, nominal tags of L = ceil(log2 k) bits enable O(1) identification cost with D=0. Conversely, any scheme with D=0 must satisfy L >= log2 k bits (tight). Without tags (L=0), identification requires Omega(n) queries in the worst case and may incur D>0. Combinatorial structure. Minimal sufficient query families form the bases of a matroid; the induced distinguishing dimension is well-defined and links to zero-error source coding via graph entropy. We illustrate implications for type systems, databases, and biological taxonomy. All results are mechanized in Lean4 (6000+ lines, 0 sorry)."}
