<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.SE](#cs.SE) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [DeGAS: Gradient-Based Optimization of Probabilistic Programs without Sampling](https://arxiv.org/abs/2601.15167)
*Francesca Randone,Romina Doz,Mirco Tribastone,Luca Bortolussi*

Main category: cs.PL

TL;DR: DeGAS：一种用于无循环概率程序的可微分高斯近似语义，支持免采样、基于梯度的优化，适用于连续和离散混合模型。


<details>
  <summary>Details</summary>
Motivation: 传统基于采样的概率程序优化方法（如MCMC、变分推断）在涉及连续变量的条件约束时可能收敛困难，且蒙特卡洛估计器存在方差问题。需要一种免采样、可微分的优化方法。

Method: 采用高斯混合语义评估程序，将测度零谓词和离散分支替换为渐近平滑，得到后验和路径概率的闭式表达式。证明这些量对程序参数的可微性，通过标准自动微分实现端到端优化。

Result: 在13个基准程序上，DeGAS在准确性和运行时间上与变分推断和MCMC相当。在涉及连续变量的条件优化问题上，DeGAS能可靠收敛，而基于采样的基线方法则失败。

Conclusion: DeGAS为混合连续-离散概率程序提供了一种有效的免采样、可微分优化框架，解决了传统采样方法在条件优化问题上的收敛困难。

Abstract: We present DeGAS, a differentiable Gaussian approximate semantics for loopless probabilistic programs that enables sample-free, gradient-based optimization in models with both continuous and discrete components. DeGAS evaluates programs under a Gaussian-mixture semantics and replaces measure-zero predicates and discrete branches with a vanishing smoothing, yielding closed-form expressions for posterior and path probabilities. We prove differentiability of these quantities with respect to program parameters, enabling end-to-end optimization via standard automatic differentiation, without Monte Carlo estimators. On thirteen benchmark programs, DeGAS achieves accuracy and runtime competitive with variational inference and MCMC. Importantly, it reliably tackles optimization problems where sampling-based baselines fail to converge due to conditioning involving continuous variables.

</details>


### [2] [Contextual Metaprogramming for Session Types](https://arxiv.org/abs/2601.15180)
*Pedro Ângelo,Atsushi Igarashi,Yuito Murase,Vasco T. Vasconcelos*

Main category: cs.PL

TL;DR: 将分阶段元编程集成到会话类型的消息传递函数式语言中，通过上下文模态类型理论和多级上下文支持代码传输和执行


<details>
  <summary>Details</summary>
Motivation: 解决现实世界中需要动态准备和传输代码的场景，如服务器按需通过会话类型消息准备和发送代码，实现灵活的程序生成和分发

Method: 基于多级上下文的上下文模态类型理论模型，支持上下文值的装箱和消息传输，区分线性资源（使用一次）和无限制资源（无限次使用），设计类型系统和类型检查器

Result: 实现了类型保持性、顺序计算的进展结果、并发运行时环境无运行时错误，以及类型检查器的正确性证明

Conclusion: 成功将分阶段元编程集成到会话类型消息传递语言中，为动态代码传输和执行提供了类型安全的框架，适用于服务器按需分发代码等实际应用场景

Abstract: We propose the integration of staged metaprogramming into a session-typed message passing functional language. We build on a model of contextual modal type theory with multi-level contexts, where contextual values, closing arbitrary terms over a series of variables, may be boxed and transmitted in messages. Once received, one such value may then be unboxed and locally applied before being run. To motivate this integration, we present examples of real-world use cases, for which our system would be suitable, such as servers preparing and shipping code on demand via session typed messages. We present a type system that distinguishes linear (used exactly once) from unrestricted (used an unbounded number of times) resources, and further define a type checker, suitable for a concrete implementation. We show type preservation, a progress result for sequential computations and absence of runtime errors for the concurrent runtime environment, as well as the correctness of the type checker.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [3] [Benchmarking Large Language Models for ABAP Code Generation: An Empirical Study on Iterative Improvement by Compiler Feedback](https://arxiv.org/abs/2601.15188)
*Stephan Wallraven,Tim Köhne,Hartmut Westenberger,Andreas Moser*

Main category: cs.SE

TL;DR: LLMs在生成ABAP代码方面表现出显著性能差异，强大模型通过迭代编译反馈可达75%成功率，而小模型表现较差。


<details>
  <summary>Details</summary>
Motivation: 尽管生成式AI在许多编程语言中应用成功，但ABAP代码生成缺乏系统分析。本研究旨在实证分析LLMs生成语法正确且功能正常的ABAP代码的能力，以及它们利用编译器反馈进行迭代改进的效果。

Method: 使用包含180个任务的基准测试，包括改编的HumanEval任务和实际SAP场景，评估各种LLMs生成ABAP代码的性能，并分析它们利用编译器反馈进行迭代改进的能力。

Result: 模型间存在显著性能差异：更强大的LLMs经过几次迭代后成功率可达75%左右，并能从编译器反馈中大幅受益；而较小的模型表现明显较弱。不同任务类型对模型构成特殊挑战。

Conclusion: 研究表明强大的LLMs在ABAP开发过程中具有巨大潜力，特别是在迭代错误修正方面，为ABAP开发流程的自动化提供了重要见解。

Abstract: This work investigates the performance of Large Language Models (LLMs) in generating ABAP code. Despite successful applications of generative AI in many programming languages, there are hardly any systematic analyses of ABAP code generation to date. The aim of the study is to empirically analyze to what extent various LLMs can generate syntactically correct and functional ABAP code, how effectively they use compiler feedback for iterative improvement, and which task types pose special challenges. For this purpose, a benchmark with 180 tasks is conducted, consisting of adapted HumanEval tasks and practical SAP scenarios. The results show significant performance differences between the models: more powerful LLMs achieve success rates of around 75% after several iterations and benefit greatly from compiler feedback, while smaller models perform significantly weaker. Overall, the study highlights the high potential of powerful LLMs for ABAP development processes, especially in iterative error correction.

</details>
