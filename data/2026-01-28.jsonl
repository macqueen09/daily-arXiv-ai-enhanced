{"id": "2601.19207", "pdf": "https://arxiv.org/pdf/2601.19207", "abs": "https://arxiv.org/abs/2601.19207", "authors": ["Matthew Britton", "Sasha Pak", "Alex Potanin"], "title": "Refactoring and Equivalence in Rust: Expanding the REM Toolchain with a Novel Approach to Automated Equivalence Proofs", "categories": ["cs.PL"], "comment": null, "summary": "Refactoring tools are central to modern development, with extract-function refactorings used heavily in day-to-day work. For Rust, however, ownership, borrowing, and advanced type features make automated extract-function refactoring challenging. Existing tools either rely on slow compiler-based analysis, support only restricted language fragments, or provide little assurance beyond \"it still compiles.\" This paper presents REM2.0, a new extract-function and verification toolchain for Rust. REM2.0 works atop rust-analyzer as a persistent daemon, providing low-latency refactorings with a VSCode front-end. It adds a repairer that automatically adjusts lifetimes and signatures when extraction exposes borrow-checker issues, and an optional verification pipeline connecting to CHARON and AENEAS to generate Coq equivalence proofs for a supported Rust subset. The architecture is evaluated on three benchmark suites. On the original REM artefact, REM2.0 achieves 100% compatibility while reducing latency from ~1000ms to single-digit milliseconds in the daemon. On 40 feature-focused extractions from 20 highly starred GitHub repositories, REM2.0 handles most examples involving async/await, const fn, non-local control flow, generics, and higher-ranked trait bounds. On twenty verification benchmarks, the CHARON/AENEAS pipeline constructs end-to-end equivalence proofs for cases within its current subset. Overall, results show that a rust-analyzer-based design can provide fast, feature-rich extract-function refactoring for real Rust programs, while opt-in verification delivers machine-checked behaviour preservation."}
{"id": "2601.19426", "pdf": "https://arxiv.org/pdf/2601.19426", "abs": "https://arxiv.org/abs/2601.19426", "authors": ["Samy Avrillon", "Ambrus Kaposi", "Ambroise Lafont", "Niyousha Najmaei", "Johann Rosain"], "title": "For Generalised Algebraic Theories, Two Sorts Are Enough", "categories": ["cs.PL", "cs.LO", "math.CT"], "comment": null, "summary": "Generalised algebraic theories (GATs) allow multiple sorts indexed over each other. For example, the theories of categories or Martin-L{รถ}f type theories form GATs. Categories have two sorts, objects and morphisms, and the latter are double-indexed over the former. Martin-L{รถ}f type theory has four sorts: contexts, substitutions, types and terms. For example, types are indexed over contexts, and terms are indexed over both contexts and types. In this paper we show that any GAT can be reduced to a GAT with only two sorts, and there is a section-retraction correspondence (formally, a strict coreflection) between models of the original and the reduced GAT. In particular, any model of the original GAT can be turned into a model of the reduced (two-sorted) GAT and back, and this roundtrip is the identity.\n  The reduced GAT is simpler than the original GAT in the following aspects: it does not have sort equalities; it does not have interleaved sorts and operations; if the original GAT did not have interleaved sorts and operations, then the reduced GAT won't have operations interleaved between different sorts. In a type-theoretic metatheory, the initial algebra of a GAT is called a quotient inductive-inductive type (QIIT). Our reduction provides a way to implement QIITs with sort equalities or interleaved constructors which are not allowed by Cubical Agda. An instance of our reduction is the well-known method of reducing mutual inductive types to a single indexed family. Our approach is semantic in that it does not rely on a syntactic description of GATs, but instead, on Uemura's bi-initial characterisation of the category of (finite) GATs in the 2-category of finitely complete categories with a chosen exponentiable morphism."}
{"id": "2601.18944", "pdf": "https://arxiv.org/pdf/2601.18944", "abs": "https://arxiv.org/abs/2601.18944", "authors": ["Qiyuan Xu", "Xiaokun Luan", "Renxi Wang", "Joshua Ong Jun Leang", "Peixin Wang", "Haonan Li", "Wenda Li", "Conrad Watt"], "title": "Neural Theorem Proving for Verification Conditions: A Real-World Benchmark", "categories": ["cs.AI", "cs.PL", "cs.SE"], "comment": "Accepted in ICLR'26", "summary": "Theorem proving is fundamental to program verification, where the automated proof of Verification Conditions (VCs) remains a primary bottleneck. Real-world program verification frequently encounters hard VCs that existing Automated Theorem Provers (ATPs) cannot prove, leading to a critical need for extensive manual proofs that burden practical application. While Neural Theorem Proving (NTP) has achieved significant success in mathematical competitions, demonstrating the potential of machine learning approaches to formal reasoning, its application to program verification--particularly VC proving--remains largely unexplored. Despite existing work on annotation synthesis and verification-related theorem proving, no benchmark has specifically targeted this fundamental bottleneck: automated VC proving. This work introduces Neural Theorem Proving for Verification Conditions (NTP4VC), presenting the first real-world multi-language benchmark for this task. From real-world projects such as Linux and Contiki-OS kernel, our benchmark leverages industrial pipelines (Why3 and Frama-C) to generate semantically equivalent test cases across formal languages of Isabelle, Lean, and Rocq. We evaluate large language models (LLMs), both general-purpose and those fine-tuned for theorem proving, on NTP4VC. Results indicate that although LLMs show promise in VC proving, significant challenges remain for program verification, highlighting a large gap and opportunity for future research."}
{"id": "2601.18987", "pdf": "https://arxiv.org/pdf/2601.18987", "abs": "https://arxiv.org/abs/2601.18987", "authors": ["Oren Sultan", "Jordi Armengol-Estape", "Pascal Kesseli", "Julien Vanegue", "Dafna Shahaf", "Yossi Adi", "Peter O'Hearn"], "title": "LLMs versus the Halting Problem: Revisiting Program Termination Prediction", "categories": ["cs.CL", "cs.AI", "cs.PL"], "comment": null, "summary": "Determining whether a program terminates is a central problem in computer science. Turing's foundational result established the Halting Problem as undecidable, showing that no algorithm can universally determine termination for all programs and inputs. Consequently, automatic verification tools approximate termination, sometimes failing to prove or disprove; these tools rely on problem-specific architectures and abstractions, and are usually tied to particular programming languages. Recent success and progress in large language models (LLMs) raises the following question: can LLMs reliably predict program termination? In this work, we evaluate LLMs on a diverse set of C programs from the Termination category of the International Competition on Software Verification (SV-Comp) 2025. Our results suggest that LLMs perform remarkably well at predicting program termination, where GPT-5 and Claude Sonnet-4.5 would rank just behind the top-ranked tool (using test-time-scaling), and Code World Model (CWM) would place just behind the second-ranked tool. While LLMs are effective at predicting program termination, they often fail to provide a valid witness as a proof. Moreover, LLMs performance drops as program length increases. We hope these insights motivate further research into program termination and the broader potential of LLMs for reasoning about undecidable problems."}
{"id": "2601.19065", "pdf": "https://arxiv.org/pdf/2601.19065", "abs": "https://arxiv.org/abs/2601.19065", "authors": ["Antonios Saravanos", "John Pazarzis", "Stavros Zervoudakis", "Dongnanzi Zheng"], "title": "The Opaque Pointer Design Pattern in Python: Towards a Pythonic PIMPL for Modularity, Encapsulation, and Stability", "categories": ["cs.SE", "cs.PL"], "comment": null, "summary": "Python libraries often need to maintain a stable public API even as internal implementations evolve, gain new backends, or depend on heavy optional libraries. In Python, where internal objects are easy to inspect and import, users can come to rely on \"reachable internals\" that were never intended to be public, making refactoring risky and slowing long-term maintenance. This paper revisits the pointer-to-implementation (PIMPL) idiom from C++ and reinterprets it as a Pythonic pattern of opaque delegation: a small public object (or module) that delegates its behavior to a separate implementation object treated as internal. We situate this pattern within a broader taxonomy of encapsulation techniques in Python, relate it to existing practices such as module-level indirection, facade objects, and backend dispatch, and identify PIMPL-like structures already used in the standard library and the scientific Python ecosystem. We then show how a Pythonic PIMPL can be used in existing codebases to isolate heavy dependencies, support lazy imports, and enable runtime selection of alternative backends without changing the public API. Finally, we discuss the benefits and trade-offs of the approach and offer practical guidance on when the pattern is appropriate and how to apply it in large, long-lived Python libraries."}
{"id": "2601.19092", "pdf": "https://arxiv.org/pdf/2601.19092", "abs": "https://arxiv.org/abs/2601.19092", "authors": ["Bohan Hou", "Hongyi Jin", "Guanjie Wang", "Jinqi Chen", "Yaxing Cai", "Lijie Yang", "Zihao Ye", "Yaoyao Ding", "Ruihang Lai", "Tianqi Chen"], "title": "Axe: A Simple Unified Layout Abstraction for Machine Learning Compilers", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.PL"], "comment": null, "summary": "Scaling modern deep learning workloads demands coordinated placement of data and compute across device meshes, memory hierarchies, and heterogeneous accelerators. We present Axe Layout, a hardware-aware abstraction that maps logical tensor coordinates to a multi-axis physical space via named axes. Axe unifies tiling, sharding, replication, and offsets across inter-device distribution and on-device layouts, enabling collective primitives to be expressed consistently from device meshes to threads. Building on Axe, we design a multi-granularity, distribution-aware DSL and compiler that composes thread-local control with collective operators in a single kernel. Experiments show that our unified approach can bring performance close to hand-tuned kernels on across latest GPU devices and multi-device environments and accelerator backends."}
