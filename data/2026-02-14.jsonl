{"id": "2602.11481", "pdf": "https://arxiv.org/pdf/2602.11481", "abs": "https://arxiv.org/abs/2602.11481", "authors": ["Minda Li", "Bhaskar Krishnamachari"], "title": "Compiler-Guided Inference-Time Adaptation: Improving GPT-5 Programming Performance in Idris", "categories": ["cs.PL", "cs.AI"], "comment": null, "summary": "GPT-5, a state of the art large language model from OpenAI, demonstrates strong performance in widely used programming languages such as Python, C++, and Java; however, its ability to operate in low resource or less commonly used languages remains underexplored. This work investigates whether GPT-5 can effectively acquire proficiency in an unfamiliar functional programming language, Idris, through iterative, feedback driven prompting. We first establish a baseline showing that with zero shot prompting the model solves only 22 out of 56 Idris exercises using the platform Exercism, substantially underperforming relative to higher resource languages (45 out of 50 in Python and 35 out of 47 in Erlang). We then evaluate several refinement strategies, including iterative prompting based on platform feedback, augmenting prompts with documentation and error classification guides, and iterative prompting using local compilation errors and failed test cases. Among these approaches, incorporating local compilation errors yields the most substantial improvements. Using this structured, error guided refinement loop, GPT-5 performance increased to an impressive 54 solved problems out of 56. These results suggest that while large language models may initially struggle in low resource settings, structured compiler level feedback can play a critical role in unlocking their capabilities."}
{"id": "2602.11232", "pdf": "https://arxiv.org/pdf/2602.11232", "abs": "https://arxiv.org/abs/2602.11232", "authors": ["Animesh Singh", "K Shiv Kumar", "S. VenkataKeerthy", "Pragna Mamidipaka", "R V B R N Aaseesh", "Sayandeep Sen", "Palanivel Kodeswaran", "Theophilus A. Benson", "Ramakrishna Upadrasta", "Praveen Tammana"], "title": "Yaksha-Prashna: Understanding eBPF Bytecode Network Function Behavior", "categories": ["cs.CR", "cs.PL", "cs.SE"], "comment": null, "summary": "Many cloud infrastructure organizations increasingly rely on third-party eBPF-based network functions for use cases like security, observability, and load balancing, so that not everyone requires a team of highly skilled eBPF experts. However, the network functions from third parties (e.g., F5, Palo Alto) are available in bytecode format to cloud operators, giving little or no understanding of their functional correctness and interaction with other network functions in a chain. Also, eBPF developers want to provide proof of functional correctness for their developed network functions without disclosing the source code to the operators. We design Yaksha-Prashna, a system that allows operators/developers to assert and query bytecode's conformance to its specification and dependencies on other bytecodes. Our work builds domain-specific models that enable us to employ scalable program analysis to extract and model eBPF programs. Using Yaksha-Prashna language, we express 24 properties on standard and non-standard eBPF-based network functions with 200-1000x speedup over the state-of-the-art work."}
