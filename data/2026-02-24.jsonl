{"id": "2602.18511", "pdf": "https://arxiv.org/pdf/2602.18511", "abs": "https://arxiv.org/abs/2602.18511", "authors": ["Lei Qiu", "Zi Yang", "Fang Lyu", "Ming Zhong", "Huimin Cui", "Xiaobing Feng"], "title": "Beyond Pass-by-Pass Optimization: Intent-Driven IR Optimization with Large Language Models", "categories": ["cs.PL", "cs.AI"], "comment": null, "summary": "Modern compilers optimize programs through a sequence of modular passes over intermediate representations (IR). While this pass-by-pass paradigm offers engineering benefits, it suffers from a pass coordination problem: locally beneficial transformations may block more profitable optimizations in later stages. This limitation stems from the lack of an explicit notion of optimization intent, defined as a holistic strategy for coordinating multiple transformations toward a global performance objective. Recent LLM-based approaches formulate IR optimization as an end-to-end generation task, thereby avoiding the traditional pass-by-pass structure. However, optimization intent remains implicit in these methods, forcing models to jointly infer optimization strategy and generate low-level transformations, which limits both correctness and performance. We propose IntOpt, the first intent-driven IR optimizer that explicitly separates high-level optimization intent from low-level analysis and transformation. IntOpt organizes IR optimization into three stages: intent formulation, intent refinement, and intent realization, enabling globally coordinated transformations. Experiments show that IntOpt achieves 90.5% verified correctness and 2.660x average speedup on 200-program test set, outperforming state-of-the-art LLM-based optimizers in both correctness and performance, and surpassing modern compiler with the -O3 option on 37 benchmarks with speedups of up to 272.60x."}
{"id": "2602.18602", "pdf": "https://arxiv.org/pdf/2602.18602", "abs": "https://arxiv.org/abs/2602.18602", "authors": ["Ryan Gibb", "Patrick Ferris", "David Allsopp", "Thomas Gazagnaire", "Anil Madhavapeddy"], "title": "Package Managers à la Carte: A Formal Model of Dependency Resolution", "categories": ["cs.PL", "cs.SE"], "comment": null, "summary": "Package managers are legion. Every programming language and operating system has its own solution, each with subtly different semantics for dependency resolution. This fragmentation prevents multilingual projects from expressing precise dependencies across language ecosystems; it leaves external system and hardware dependencies implicit and unversioned; it obscures security vulnerabilities that lie in the full dependency graph. We present the \\textit{Package Calculus}, a formalism for dependency resolution that unifies the core semantics of diverse package managers. Through a series of formal reductions, we show how this core is expressive enough to model the diversity that real-world package managers employ in their dependency expression languages. By using the Package Calculus as the intermediate representation of dependencies, we enable translation between distinct package managers and resolution across ecosystems."}
{"id": "2602.19686", "pdf": "https://arxiv.org/pdf/2602.19686", "abs": "https://arxiv.org/abs/2602.19686", "authors": ["Qiqi Jason Gu", "Lixue Liu", "Wei Ke"], "title": "A Flow Extension to Coroutine Types for Deadlock Detection in Go", "categories": ["cs.PL", "cs.SC"], "comment": "Accepted in ICSESS 2025, Macao", "summary": "Coroutines, as an abstract programming construct, are a generalization of functions that can suspend execution part- way for later resumption. Coroutine Types are behavioral types to model interactions of coroutines with a single receiving operation followed by a single yielding operation. Coroutine Types have been applied to model-driven engineering, smart contracts, and test case generation. We contribute a Flow extension to Coroutine Types, so that coroutines with more than one receiving and yielding operation can be modeled. We accordingly revise the reduction rules of Coroutine Types. To show the usefulness of the Flow extension, we contribute a type system that maps expressions of the Go programming language to Coroutine Types. If the reduction result is 0, the two channel operations are paired properly and the program has no deadlocks. We choose Go because it is a popular programming language for distributed systems, but a frequent kind of bugs in Go is deadlocks due to the wrong use of concurrency features. We concentrate on the most commonly used semantics in Go: unbuffered channels with the keywords go and defer. Our Flow extension and the type system recognize 17 patterns of channels and goroutine interactions, including mismatched receivers and senders, nested goroutines, etc. We also integrate the Z3 SMT solver to take account of conditional execution and type inheritance. Other static or dynamic deadlock detectors crashed or gave wrong predictions in some patterns. Therefore, our type-based deadlock analyzer not only fills the gap in the landscape of value-based detection, but also complements existing detectors."}
{"id": "2602.19762", "pdf": "https://arxiv.org/pdf/2602.19762", "abs": "https://arxiv.org/abs/2602.19762", "authors": ["Mohammed Javed Absar", "Muthu Baskaran", "Abhikrant Sharma", "Abhilash Bhandari", "Ankit Aggarwal", "Arun Rangasamy", "Dibyendu Das", "Fateme Hosseini", "Franck Slama", "Iulian Brumar", "Jyotsna Verma", "Krishnaprasad Bindumadhavan", "Mitesh Kothari", "Mohit Gupta", "Ravishankar Kolachana", "Richard Lethin", "Samarth Narang", "Sanjay Motilal Ladwa", "Shalini Jain", "Snigdha Suresh Dalvi", "Tasmia Rahman", "Venkat Rasagna Reddy Komatireddy", "Vivek Vasudevbhai Pandya", "Xiyue Shi", "Zachary Zipper"], "title": "Hexagon-MLIR: An AI Compilation Stack For Qualcomm's Neural Processing Units (NPUs)", "categories": ["cs.PL", "cs.AI"], "comment": null, "summary": "In this paper, we present Hexagon-MLIR,an open-source compilation stack that targets Qualcomm Hexagon Neural Processing Unit (NPU) and provides unified support for lowering Triton kernels and PyTorch models . Built using the MLIR framework, our compiler applies a structured sequence of passes to exploit NPU architectural features to accelerate AI workloads. It enables faster deployment of new Triton kernels (hand-written or subgraphs from PyTorch 2.0), for our target by providing automated compilation from kernel to binary. By ingesting Triton kernels, we generate mega-kernels that maximize data locality in the NPU's Tightly Coupled Memory (TCM), reducing the bandwidth bottlenecks inherent in library-based approaches. This initiative complements our commercial toolchains by providing developers with an open-source MLIR-based compilation stack that gives them a path to advance AI compilation capabilities through a more flexible approach. Hexagon-MLIR is a work-in-progress, and we are continuing to add many more optimizations and capabilities in this effort."}
{"id": "2602.19868", "pdf": "https://arxiv.org/pdf/2602.19868", "abs": "https://arxiv.org/abs/2602.19868", "authors": ["David Knothe", "Oliver Bringmann"], "title": "Combining Small-Step and Big-Step Semantics to Verify Loop Optimizations", "categories": ["cs.PL", "cs.LO"], "comment": "18 pages, 6 figures. Submitted to ITP 2026", "summary": "Verified compilers aim to guarantee that compilation preserves the observable behavior of source programs. While small-step semantics are widely used in such compilers, they are not always the most convenient framework for structural transformations such as loop optimizations. This paper proposes an approach that leverages both small-step and big-step semantics: small-step semantics are used for local transformations, while big-step semantics are employed for structural transformations. An abstract behavioral semantics is introduced as a common interface between the two styles. Coinductive big-step semantics is extended to correctly handle divergence with both finite and infinite traces, bringing it on par with the expressiveness of small-step semantics. This enables the insertion of big-step transformations into the middle of an existing small-step pipeline, thereby fully preserving all top-level semantic preservation theorems. This approach is practically demonstrated in CompCert by implementing and verifying a few new loop optimizations in big-step Cminor, including loop unswitching and, notably, full loop unrolling."}
{"id": "2602.19951", "pdf": "https://arxiv.org/pdf/2602.19951", "abs": "https://arxiv.org/abs/2602.19951", "authors": ["Tianyu Chen", "Darshal Shetty", "Jeremy G. Siek", "Chao-Hong Chen", "Weixi Ma", "Arnaud Venet", "Rocky Liu"], "title": "Taming Scope Extrusion in Gradual Imperative Metaprogramming", "categories": ["cs.PL"], "comment": "34 pages, 19 figures", "summary": "Metaprogramming enables the generation of performant code, while gradual typing facilitates the smooth migration from untyped scripts to robust statically typed programs. However, combining these features with imperative state - specifically mutable references - reintroduces the classic peril of scope extrusion, where code fragments containing free variables escape their defining lexical context. While static type systems utilizing environment classifiers have successfully tamed this interaction, enforcing these invariants in a gradual language remains an open challenge.\n  This paper presents $λ^{α,\\star}_{\\text{Ref}}$, the first gradual metaprogramming language that supports mutable references while guaranteeing scope safety. To put $λ^{α,\\star}_{\\text{Ref}}$ on a firm foundation, we also develop its statically typed sister language, $λ^α_{\\text{Ref}}$, that introduces unrestricted subtyping for environment classifiers. Our key innovation, however, is the dynamic enforcement of the environment classifier discipline in $λ^{α,\\star}_{\\text{Ref}}$, enabling the language to mediate between statically verified scopes and dynamically verified scopes. The dynamic enforcement is carried out in a novel cast calculus $\\mathrm{CC}^{α,\\star}_{\\text{Ref}}$ that uses an extension of Henglein's Coercion Calculus to handle code types, classifier polymorphism, and subtype constraints. We prove that $λ^{α,\\star}_{\\text{Ref}}$ satisfies type safety and scope safety. Finally, we provide a space-efficient implementation strategy for the dynamic scope checks, ensuring that the runtime overhead remains practical."}
{"id": "2602.19973", "pdf": "https://arxiv.org/pdf/2602.19973", "abs": "https://arxiv.org/abs/2602.19973", "authors": ["Cezar-Constantin Andrici", "Abigail Pribisova", "Danel Ahman", "Catalin Hritcu", "Exequiel Rivas", "Théo Winterhalter"], "title": "Misquoted No More: Securely Extracting F* Programs with IO", "categories": ["cs.PL", "cs.CR"], "comment": "Submitted to ICFP'26", "summary": "Shallow embeddings that use monads to represent effects are popular in proof-oriented languages because they are convenient for formal verification. Once shallowly embedded programs are verified, they are often extracted to mainstream languages like OCaml or C and linked into larger codebases. The extraction process is not fully verified because it often involves quotation -- turning the shallowly embedded program into a deeply embedded one -- and verifying quotation remains a major open challenge. Instead, some prior work obtains formal correctness guarantees using translation validation to certify individual extraction results. We build on this idea, but limit the use of translation validation to a first extraction step that we call relational quotation and that uses a metaprogram to construct a typing derivation for the given shallowly embedded program. This metaprogram is simple, since the typing derivation follows the structure of the original program. Once we validate, syntactically, that the typing derivation is valid for the original program, we pass it to a verified syntax-generation function that produces code guaranteed to be semantically related to the original program.\n  We apply this general idea to build SEIO*, a framework for extracting shallowly embedded F* programs with IO to a deeply embedded lambda-calculus while providing formal secure compilation guarantees. Using two cross-language logical relations, we devise a machine-checked proof in F* that SEIO* guarantees Robust Relational Hyperproperty Preservation (RrHP), a very strong secure compilation criterion that implies full abstraction as well as preservation of trace properties and hyperproperties against arbitrary adversarial contexts. This goes beyond the state of the art in verified and certifying extraction, which so far has focused on correctness rather than security."}
{"id": "2602.20064", "pdf": "https://arxiv.org/pdf/2602.20064", "abs": "https://arxiv.org/abs/2602.20064", "authors": ["Zac Garby", "Andrew D. Gordon", "David Sands"], "title": "The LLMbda Calculus: AI Agents, Conversations, and Information Flow", "categories": ["cs.PL", "cs.AI", "cs.CR"], "comment": null, "summary": "A conversation with a large language model (LLM) is a sequence of prompts and responses, with each response generated from the preceding conversation. AI agents build such conversations automatically: given an initial human prompt, a planner loop interleaves LLM calls with tool invocations and code execution. This tight coupling creates a new and poorly understood attack surface. A malicious prompt injected into a conversation can compromise later reasoning, trigger dangerous tool calls, or distort final outputs. Despite the centrality of such systems, we currently lack a principled semantic foundation for reasoning about their behaviour and safety. We address this gap by introducing an untyped call-by-value lambda calculus enriched with dynamic information-flow control and a small number of primitives for constructing prompt-response conversations. Our language includes a primitive that invokes an LLM: it serializes a value, sends it to the model as a prompt, and parses the response as a new term. This calculus faithfully represents planner loops and their vulnerabilities, including the mechanisms by which prompt injection alters subsequent computation. The semantics explicitly captures conversations, and so supports reasoning about defenses such as quarantined sub-conversations, isolation of generated code, and information-flow restrictions on what may influence an LLM call. A termination-insensitive noninterference theorem establishes integrity and confidentiality guarantees, demonstrating that a formal calculus can provide rigorous foundations for safe agentic programming."}
{"id": "2602.20082", "pdf": "https://arxiv.org/pdf/2602.20082", "abs": "https://arxiv.org/abs/2602.20082", "authors": ["Zoe Paraskevopoulou"], "title": "Machine-Generated, Machine-Checked Proofs for a Verified Compiler (Experience Report)", "categories": ["cs.PL"], "comment": null, "summary": "We report on using an agentic coding assistant (Claude Code, powered by Claude Opus 4.6) to mechanize a substantial Rocq correctness proof from scratch, with human guidance but without human proof writing. The proof establishes semantic preservation for the administrative normal form (ANF) transformation in the CertiCoq verified compiler for Rocq. The closely related continuation-passing style (CPS) transformation in CertiCoq was previously proved correct by human experts over several months. We use this proof as a template and instruct the LLM to adapt the proof technique to the ANF setting, which differs in important technical ways. The resulting ANF proof comprises approximately 7,800 lines of Rocq (larger than the 5,300-line CPS proof) and was developed in approximately 96 hours. We describe the proof technique and report on the experience of developing it with an LLM, discussing both the strengths and limitations of the approach and its implications for verified compiler construction."}
{"id": "2602.13400", "pdf": "https://arxiv.org/pdf/2602.13400", "abs": "https://arxiv.org/abs/2602.13400", "authors": ["Tanner Wright", "Adams Chen", "Gema Rodríguez-Pérez"], "title": "InEx-Bug: A Human Annotated Dataset of Intrinsic and Extrinsic Bugs in the NPM Ecosystem", "categories": ["cs.SE", "cs.PL"], "comment": null, "summary": "Understanding the causes of software defects is essential for reliable software maintenance and ecosystem stability. However, existing bug datasets do not distinguish between issues originating within a project from those caused by external dependencies or environmental factors. In this paper we present InEx-Bug, a manually annotated dataset of 377 GitHub issues from 103 NPM repositories, categorizing issues as Intrinsic (internal defect), Extrinsic (dependency/environment issue), Not-a-Bug, or Unknown. Beyond labels, the dataset includes rich temporal and behavioral metadata such as maintainer participation, code changes, and reopening patterns. Analyses show Intrinsic bugs resolve faster (median 8.9 vs 10.2 days), are close more often (92% vs 78%), and require code changes more frequently (57% vs 28%) compared to Extrinsic bugs. While Extrinsic bugs exhibit higher reopen rates (12% vs 4%) and delayed recurrence (median 157 vs 87 days). The dataset provides a foundation for further studying Intrinsic and Extrinsic defects in the NPM ecosystem."}
{"id": "2602.18534", "pdf": "https://arxiv.org/pdf/2602.18534", "abs": "https://arxiv.org/abs/2602.18534", "authors": ["Hanliang Zhang", "Arindam Sharma", "Cristina David", "Meng Wang", "Brandon Paulsen", "Daniel Kroening", "Wenjia Ye", "Taro Sekiyama"], "title": "Validated Code Translation for Projects with External Libraries", "categories": ["cs.SE", "cs.PL"], "comment": null, "summary": "Large Language Models (LLMs) have shown promise for program translation, particularly for migrating systems code to memory-safe languages such as Rust. However, existing approaches struggle when source programs depend on external libraries: LLMs frequently hallucinate non-existent target APIs and fail to generate call-enabling imports; moreover, validating semantic equivalence is challenging when the code manipulates opaque, library-defined types. We present a translation and validation framework for translating Go projects with external dependencies to Rust. Our approach combines (i) a retrieval mechanism that maps Go library APIs to Rust APIs, and (ii) a cross-language validation pipeline that establishes language interoperability in the presence of opaque library types by synthesising adapters exclusively from public library APIs, prior to validating I/O equivalence. We evaluate our system on six real-world Go repositories with non-trivial external dependencies. Our approach significantly increases both the compilation and equivalence success rate (up to 100% in the most dependency-heavy case; approx. 2x on average) by enabling validated translation that manipulate opaque, library-defined types."}
{"id": "2602.18545", "pdf": "https://arxiv.org/pdf/2602.18545", "abs": "https://arxiv.org/abs/2602.18545", "authors": ["Alperen Keles", "Justine Frank", "Ceren Mert", "Harrison Goldstein", "Leonidas Lampropoulos"], "title": "Programmable Property-Based Testing", "categories": ["cs.SE", "cs.PL"], "comment": null, "summary": "Property-based testing (PBT) is a popular technique for establishing confidence in software, where users write properties -- i.e., executable specifications -- that can be checked many times in a loop by a testing framework. In modern PBT frameworks, properties are usually written in shallowly embedded domain-specific languages, and their definition is tightly coupled to the way they are tested. Such frameworks often provide convenient configuration options to customize aspects of the testing process, but users are limited to precisely what library authors had the prescience to allow for when developing the framework; if they want more flexibility, they may need to write a new framework from scratch.\n  We propose a new, deeper language for properties based on a mixed embedding that we call deferred binding abstract syntax, which reifies properties as a data structure and decouples them from the property runners that execute them. We implement this language in Rocq and Racket, leveraging the power of dependent and dynamic types, respectively. Finally, we showcase the flexibility of this new approach by rapidly prototyping a variety of property runners, highlighting domain-specific testing improvements that can be unlocked by more programmable testing."}
