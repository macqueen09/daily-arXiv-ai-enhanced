<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 9]
- [cs.SE](#cs.SE) [Total: 3]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Beyond Pass-by-Pass Optimization: Intent-Driven IR Optimization with Large Language Models](https://arxiv.org/abs/2602.18511)
*Lei Qiu,Zi Yang,Fang Lyu,Ming Zhong,Huimin Cui,Xiaobing Feng*

Main category: cs.PL

TL;DR: IntOpt是一个意图驱动的中间表示优化器，通过将优化意图与底层分析转换分离，解决了传统编译器优化中pass协调问题，实现了全局协调的优化策略。


<details>
  <summary>Details</summary>
Motivation: 传统编译器采用模块化pass序列进行优化，存在pass协调问题：局部有益的转换可能阻碍后续更有利的优化。现有LLM方法将IR优化视为端到端生成任务，但优化意图仍然隐含，限制了正确性和性能。

Method: IntOpt将IR优化分为三个阶段：意图制定（定义高层次优化策略）、意图细化（完善优化计划）、意图实现（执行具体转换），实现了优化意图与底层分析的明确分离。

Result: 在200个程序的测试集上，IntOpt达到90.5%的验证正确性和2.660倍的平均加速比，在正确性和性能上都优于最先进的LLM优化器，并在37个基准测试中超过了-O3优化的现代编译器，最高加速比达272.60倍。

Conclusion: 通过明确分离优化意图与底层转换，IntOpt解决了传统编译器优化的协调问题，实现了全局优化的高性能和高正确性，为编译器优化提供了新的范式。

Abstract: Modern compilers optimize programs through a sequence of modular passes over intermediate representations (IR). While this pass-by-pass paradigm offers engineering benefits, it suffers from a pass coordination problem: locally beneficial transformations may block more profitable optimizations in later stages. This limitation stems from the lack of an explicit notion of optimization intent, defined as a holistic strategy for coordinating multiple transformations toward a global performance objective. Recent LLM-based approaches formulate IR optimization as an end-to-end generation task, thereby avoiding the traditional pass-by-pass structure. However, optimization intent remains implicit in these methods, forcing models to jointly infer optimization strategy and generate low-level transformations, which limits both correctness and performance. We propose IntOpt, the first intent-driven IR optimizer that explicitly separates high-level optimization intent from low-level analysis and transformation. IntOpt organizes IR optimization into three stages: intent formulation, intent refinement, and intent realization, enabling globally coordinated transformations. Experiments show that IntOpt achieves 90.5% verified correctness and 2.660x average speedup on 200-program test set, outperforming state-of-the-art LLM-based optimizers in both correctness and performance, and surpassing modern compiler with the -O3 option on 37 benchmarks with speedups of up to 272.60x.

</details>


### [2] [Package Managers à la Carte: A Formal Model of Dependency Resolution](https://arxiv.org/abs/2602.18602)
*Ryan Gibb,Patrick Ferris,David Allsopp,Thomas Gazagnaire,Anil Madhavapeddy*

Main category: cs.PL

TL;DR: 提出了Package Calculus形式化模型，统一不同包管理器的依赖解析语义，支持跨语言生态系统的依赖翻译和解析。


<details>
  <summary>Details</summary>
Motivation: 当前包管理器碎片化严重，每个编程语言和操作系统都有自己的解决方案，依赖解析语义存在细微差异。这导致多语言项目无法精确表达跨语言生态系统的依赖关系，外部系统和硬件依赖隐式且无版本控制，完整依赖图中的安全漏洞难以发现。

Method: 提出了Package Calculus形式化模型，通过一系列形式化规约证明该核心模型足以表达真实包管理器在依赖表达语言中使用的多样性。将该模型作为依赖关系的中间表示，实现不同包管理器之间的翻译和跨生态系统解析。

Result: Package Calculus能够统一不同包管理器的核心语义，为跨语言生态系统依赖管理提供了理论基础和实现框架。

Conclusion: Package Calculus为解决包管理器碎片化问题提供了形式化解决方案，通过统一的中间表示实现跨生态系统依赖管理，有助于提高多语言项目的依赖精确性、安全性和可维护性。

Abstract: Package managers are legion. Every programming language and operating system has its own solution, each with subtly different semantics for dependency resolution. This fragmentation prevents multilingual projects from expressing precise dependencies across language ecosystems; it leaves external system and hardware dependencies implicit and unversioned; it obscures security vulnerabilities that lie in the full dependency graph. We present the \textit{Package Calculus}, a formalism for dependency resolution that unifies the core semantics of diverse package managers. Through a series of formal reductions, we show how this core is expressive enough to model the diversity that real-world package managers employ in their dependency expression languages. By using the Package Calculus as the intermediate representation of dependencies, we enable translation between distinct package managers and resolution across ecosystems.

</details>


### [3] [A Flow Extension to Coroutine Types for Deadlock Detection in Go](https://arxiv.org/abs/2602.19686)
*Qiqi Jason Gu,Lixue Liu,Wei Ke*

Main category: cs.PL

TL;DR: 本文扩展了协程类型（Coroutine Types），增加了Flow扩展以支持多个接收和产出操作，并开发了Go语言的类型系统来检测通道死锁问题。


<details>
  <summary>Details</summary>
Motivation: Go语言在分布式系统中很流行，但由于并发特性使用不当经常出现死锁bug。现有的死锁检测器在某些模式下会崩溃或给出错误预测，需要更可靠的检测方法。

Method: 1. 扩展协程类型，增加Flow扩展以支持多个接收和产出操作；2. 修订协程类型的规约规则；3. 开发将Go表达式映射到协程类型的类型系统；4. 集成Z3 SMT求解器处理条件执行和类型继承。

Result: Flow扩展和类型系统能够识别17种通道和goroutine交互模式，包括不匹配的接收者和发送者、嵌套goroutine等。当规约结果为0时，表示通道操作正确配对且程序无死锁。

Conclusion: 该类型系统不仅填补了基于值检测的空白，还补充了现有的死锁检测器，为Go程序的死锁检测提供了更可靠的解决方案。

Abstract: Coroutines, as an abstract programming construct, are a generalization of functions that can suspend execution part- way for later resumption. Coroutine Types are behavioral types to model interactions of coroutines with a single receiving operation followed by a single yielding operation. Coroutine Types have been applied to model-driven engineering, smart contracts, and test case generation. We contribute a Flow extension to Coroutine Types, so that coroutines with more than one receiving and yielding operation can be modeled. We accordingly revise the reduction rules of Coroutine Types. To show the usefulness of the Flow extension, we contribute a type system that maps expressions of the Go programming language to Coroutine Types. If the reduction result is 0, the two channel operations are paired properly and the program has no deadlocks. We choose Go because it is a popular programming language for distributed systems, but a frequent kind of bugs in Go is deadlocks due to the wrong use of concurrency features. We concentrate on the most commonly used semantics in Go: unbuffered channels with the keywords go and defer. Our Flow extension and the type system recognize 17 patterns of channels and goroutine interactions, including mismatched receivers and senders, nested goroutines, etc. We also integrate the Z3 SMT solver to take account of conditional execution and type inheritance. Other static or dynamic deadlock detectors crashed or gave wrong predictions in some patterns. Therefore, our type-based deadlock analyzer not only fills the gap in the landscape of value-based detection, but also complements existing detectors.

</details>


### [4] [Hexagon-MLIR: An AI Compilation Stack For Qualcomm's Neural Processing Units (NPUs)](https://arxiv.org/abs/2602.19762)
*Mohammed Javed Absar,Muthu Baskaran,Abhikrant Sharma,Abhilash Bhandari,Ankit Aggarwal,Arun Rangasamy,Dibyendu Das,Fateme Hosseini,Franck Slama,Iulian Brumar,Jyotsna Verma,Krishnaprasad Bindumadhavan,Mitesh Kothari,Mohit Gupta,Ravishankar Kolachana,Richard Lethin,Samarth Narang,Sanjay Motilal Ladwa,Shalini Jain,Snigdha Suresh Dalvi,Tasmia Rahman,Venkat Rasagna Reddy Komatireddy,Vivek Vasudevbhai Pandya,Xiyue Shi,Zachary Zipper*

Main category: cs.PL

TL;DR: Hexagon-MLIR是一个开源编译栈，针对高通Hexagon NPU，支持将Triton内核和PyTorch模型统一编译优化，利用MLIR框架和NPU架构特性加速AI工作负载。


<details>
  <summary>Details</summary>
Motivation: 现有的库方法存在带宽瓶颈问题，需要更灵活的编译方案来加速AI工作负载在Hexagon NPU上的部署，特别是针对Triton内核和PyTorch 2.0子图。

Method: 基于MLIR框架构建，采用结构化序列的编译pass，利用NPU架构特性（如紧密耦合内存TCM），通过生成mega-kernels最大化数据局部性，减少带宽瓶颈。

Result: 实现了从内核到二进制文件的自动化编译，能够更快地部署新的Triton内核（手写或来自PyTorch 2.0子图），为开发者提供开源MLIR编译栈。

Conclusion: Hexagon-MLIR作为商业工具链的补充，为开发者提供了更灵活的AI编译能力提升路径，目前仍在开发中，将持续增加更多优化和功能。

Abstract: In this paper, we present Hexagon-MLIR,an open-source compilation stack that targets Qualcomm Hexagon Neural Processing Unit (NPU) and provides unified support for lowering Triton kernels and PyTorch models . Built using the MLIR framework, our compiler applies a structured sequence of passes to exploit NPU architectural features to accelerate AI workloads. It enables faster deployment of new Triton kernels (hand-written or subgraphs from PyTorch 2.0), for our target by providing automated compilation from kernel to binary. By ingesting Triton kernels, we generate mega-kernels that maximize data locality in the NPU's Tightly Coupled Memory (TCM), reducing the bandwidth bottlenecks inherent in library-based approaches. This initiative complements our commercial toolchains by providing developers with an open-source MLIR-based compilation stack that gives them a path to advance AI compilation capabilities through a more flexible approach. Hexagon-MLIR is a work-in-progress, and we are continuing to add many more optimizations and capabilities in this effort.

</details>


### [5] [Combining Small-Step and Big-Step Semantics to Verify Loop Optimizations](https://arxiv.org/abs/2602.19868)
*David Knothe,Oliver Bringmann*

Main category: cs.PL

TL;DR: 论文提出了一种结合小步和大步语义的编译器验证方法：小步语义用于局部变换，大步语义用于结构变换（如循环优化），通过抽象行为语义作为接口，使两种语义风格协同工作。


<details>
  <summary>Details</summary>
Motivation: 现有验证编译器通常使用小步语义，但小步语义在处理循环优化等结构变换时不够方便。需要一种既能利用小步语义优势，又能方便处理结构变换的方法。

Method: 1. 引入抽象行为语义作为小步和大步语义之间的共同接口；2. 扩展共归纳大步语义以正确处理有限和无限轨迹的散度；3. 在现有小步编译流水线中插入大步变换，保持顶层语义保持定理。

Result: 该方法在CompCert中实现并验证了几个新的循环优化，包括循环切换和完整的循环展开，证明了方法的实用性。

Conclusion: 通过结合小步和大步语义的优势，为验证编译器提供了一种更灵活的方法，既能处理局部变换，又能方便地进行结构优化，同时保持语义正确性。

Abstract: Verified compilers aim to guarantee that compilation preserves the observable behavior of source programs. While small-step semantics are widely used in such compilers, they are not always the most convenient framework for structural transformations such as loop optimizations. This paper proposes an approach that leverages both small-step and big-step semantics: small-step semantics are used for local transformations, while big-step semantics are employed for structural transformations. An abstract behavioral semantics is introduced as a common interface between the two styles. Coinductive big-step semantics is extended to correctly handle divergence with both finite and infinite traces, bringing it on par with the expressiveness of small-step semantics. This enables the insertion of big-step transformations into the middle of an existing small-step pipeline, thereby fully preserving all top-level semantic preservation theorems. This approach is practically demonstrated in CompCert by implementing and verifying a few new loop optimizations in big-step Cminor, including loop unswitching and, notably, full loop unrolling.

</details>


### [6] [Taming Scope Extrusion in Gradual Imperative Metaprogramming](https://arxiv.org/abs/2602.19951)
*Tianyu Chen,Darshal Shetty,Jeremy G. Siek,Chao-Hong Chen,Weixi Ma,Arnaud Venet,Rocky Liu*

Main category: cs.PL

TL;DR: 首个支持可变引用的渐进元编程语言，通过动态环境分类器强制执行保证作用域安全


<details>
  <summary>Details</summary>
Motivation: 结合元编程和渐进类型时，引入可变引用会重新引入作用域外泄问题。静态类型系统已通过环境分类器解决此问题，但在渐进语言中强制执行这些不变量仍是一个开放挑战

Method: 提出λ^{α,⋆}_{Ref}渐进元编程语言，引入其静态类型姊妹语言λ^α_{Ref}，并开发新颖的强制转换演算CC^{α,⋆}_{Ref}来动态执行环境分类器规则

Result: 证明了λ^{α,⋆}_{Ref}满足类型安全和作用域安全，并提供了空间高效的动态作用域检查实现策略

Conclusion: 成功解决了渐进元编程语言中支持可变引用时的作用域安全问题，为实际应用提供了可行的运行时机制

Abstract: Metaprogramming enables the generation of performant code, while gradual typing facilitates the smooth migration from untyped scripts to robust statically typed programs. However, combining these features with imperative state - specifically mutable references - reintroduces the classic peril of scope extrusion, where code fragments containing free variables escape their defining lexical context. While static type systems utilizing environment classifiers have successfully tamed this interaction, enforcing these invariants in a gradual language remains an open challenge.
  This paper presents $λ^{α,\star}_{\text{Ref}}$, the first gradual metaprogramming language that supports mutable references while guaranteeing scope safety. To put $λ^{α,\star}_{\text{Ref}}$ on a firm foundation, we also develop its statically typed sister language, $λ^α_{\text{Ref}}$, that introduces unrestricted subtyping for environment classifiers. Our key innovation, however, is the dynamic enforcement of the environment classifier discipline in $λ^{α,\star}_{\text{Ref}}$, enabling the language to mediate between statically verified scopes and dynamically verified scopes. The dynamic enforcement is carried out in a novel cast calculus $\mathrm{CC}^{α,\star}_{\text{Ref}}$ that uses an extension of Henglein's Coercion Calculus to handle code types, classifier polymorphism, and subtype constraints. We prove that $λ^{α,\star}_{\text{Ref}}$ satisfies type safety and scope safety. Finally, we provide a space-efficient implementation strategy for the dynamic scope checks, ensuring that the runtime overhead remains practical.

</details>


### [7] [Misquoted No More: Securely Extracting F* Programs with IO](https://arxiv.org/abs/2602.19973)
*Cezar-Constantin Andrici,Abigail Pribisova,Danel Ahman,Catalin Hritcu,Exequiel Rivas,Théo Winterhalter*

Main category: cs.PL

TL;DR: SEIO* 是一个从浅层嵌入的 F* 程序提取到深层嵌入 λ 演算的框架，通过关系引用和验证的语法生成提供形式化的安全编译保证，确保鲁棒关系超属性保持。


<details>
  <summary>Details</summary>
Motivation: 浅层嵌入程序验证后需要提取到主流语言，但提取过程涉及引用（quotation）难以验证。现有工作使用翻译验证来认证单个提取结果，但缺乏安全保证。本文旨在提供形式化的安全编译保证，超越传统的正确性验证。

Method: 提出关系引用（relational quotation）方法：使用元程序为浅层嵌入程序构造类型推导，验证类型推导的有效性后，通过验证的语法生成函数生成语义相关的代码。基于此构建 SEIO* 框架，使用跨语言逻辑关系证明其满足鲁棒关系超属性保持。

Result: SEIO* 在 F* 中实现了机器验证的证明，保证鲁棒关系超属性保持（RrHP），这是一个非常强的安全编译标准，蕴含完全抽象性、轨迹属性和超属性的保持，能够抵御任意对抗上下文。

Conclusion: 该方法通过关系引用和验证的语法生成，为浅层嵌入程序的提取提供了形式化的安全编译保证，超越了现有验证提取工作仅关注正确性的局限，实现了更强的安全属性保护。

Abstract: Shallow embeddings that use monads to represent effects are popular in proof-oriented languages because they are convenient for formal verification. Once shallowly embedded programs are verified, they are often extracted to mainstream languages like OCaml or C and linked into larger codebases. The extraction process is not fully verified because it often involves quotation -- turning the shallowly embedded program into a deeply embedded one -- and verifying quotation remains a major open challenge. Instead, some prior work obtains formal correctness guarantees using translation validation to certify individual extraction results. We build on this idea, but limit the use of translation validation to a first extraction step that we call relational quotation and that uses a metaprogram to construct a typing derivation for the given shallowly embedded program. This metaprogram is simple, since the typing derivation follows the structure of the original program. Once we validate, syntactically, that the typing derivation is valid for the original program, we pass it to a verified syntax-generation function that produces code guaranteed to be semantically related to the original program.
  We apply this general idea to build SEIO*, a framework for extracting shallowly embedded F* programs with IO to a deeply embedded lambda-calculus while providing formal secure compilation guarantees. Using two cross-language logical relations, we devise a machine-checked proof in F* that SEIO* guarantees Robust Relational Hyperproperty Preservation (RrHP), a very strong secure compilation criterion that implies full abstraction as well as preservation of trace properties and hyperproperties against arbitrary adversarial contexts. This goes beyond the state of the art in verified and certifying extraction, which so far has focused on correctness rather than security.

</details>


### [8] [The LLMbda Calculus: AI Agents, Conversations, and Information Flow](https://arxiv.org/abs/2602.20064)
*Zac Garby,Andrew D. Gordon,David Sands*

Main category: cs.PL

TL;DR: 本文提出了一种基于lambda演算的形式化框架，用于建模和分析LLM对话系统中的安全漏洞，特别是提示注入攻击，并证明了信息流控制能提供完整性/机密性保证。


<details>
  <summary>Details</summary>
Motivation: LLM对话系统（如AI代理）将提示、响应、工具调用和代码执行紧密耦合，形成了新的攻击面。恶意提示注入可能破坏后续推理、触发危险工具调用或扭曲最终输出。目前缺乏对这些系统行为和安全的原理性语义基础。

Method: 引入了一个无类型按值调用的lambda演算，扩展了动态信息流控制和少量用于构建提示-响应对话的原语。该语言包含调用LLM的原语：序列化值作为提示发送给模型，并将响应解析为新项。该演算能忠实表示规划器循环及其漏洞，包括提示注入改变后续计算的机制。

Result: 该形式化语义明确捕获了对话过程，支持对防御机制（如隔离子对话、生成代码隔离、LLM调用的信息流限制）进行推理。通过终止不敏感的非干扰定理，建立了完整性和机密性保证。

Conclusion: 形式化演算能为安全的代理编程提供严格基础，展示了如何通过信息流控制来防御提示注入等攻击，为LLM对话系统的安全分析提供了理论框架。

Abstract: A conversation with a large language model (LLM) is a sequence of prompts and responses, with each response generated from the preceding conversation. AI agents build such conversations automatically: given an initial human prompt, a planner loop interleaves LLM calls with tool invocations and code execution. This tight coupling creates a new and poorly understood attack surface. A malicious prompt injected into a conversation can compromise later reasoning, trigger dangerous tool calls, or distort final outputs. Despite the centrality of such systems, we currently lack a principled semantic foundation for reasoning about their behaviour and safety. We address this gap by introducing an untyped call-by-value lambda calculus enriched with dynamic information-flow control and a small number of primitives for constructing prompt-response conversations. Our language includes a primitive that invokes an LLM: it serializes a value, sends it to the model as a prompt, and parses the response as a new term. This calculus faithfully represents planner loops and their vulnerabilities, including the mechanisms by which prompt injection alters subsequent computation. The semantics explicitly captures conversations, and so supports reasoning about defenses such as quarantined sub-conversations, isolation of generated code, and information-flow restrictions on what may influence an LLM call. A termination-insensitive noninterference theorem establishes integrity and confidentiality guarantees, demonstrating that a formal calculus can provide rigorous foundations for safe agentic programming.

</details>


### [9] [Machine-Generated, Machine-Checked Proofs for a Verified Compiler (Experience Report)](https://arxiv.org/abs/2602.20082)
*Zoe Paraskevopoulou*

Main category: cs.PL

TL;DR: 使用Claude Code代理助手在96小时内机械化完成了CertiCoq验证编译器中ANF转换的语义保持性证明，生成了约7800行Rocq代码，比之前人工完成的CPS证明更大。


<details>
  <summary>Details</summary>
Motivation: 验证编译器中的变换正确性证明通常需要大量人工工作（如CPS变换证明耗时数月）。研究如何利用LLM助手机械化这类复杂证明，以加速验证编译器开发。

Method: 使用Claude Code（基于Claude Opus 4.6）作为代理编码助手，以人工完成的CPS变换证明为模板，指导LLM将证明技术适配到ANF变换场景。整个过程有人类指导但无需人工编写证明代码。

Result: 成功机械化完成了ANF变换的语义保持性证明，生成了约7800行Rocq代码（比CPS证明的5300行更大），开发时间约96小时，远少于人工证明的几个月时间。

Conclusion: LLM辅助证明开发在验证编译器构建中具有潜力，能显著加速证明过程，但方法仍有局限性，需要人类指导来适配不同的技术场景。

Abstract: We report on using an agentic coding assistant (Claude Code, powered by Claude Opus 4.6) to mechanize a substantial Rocq correctness proof from scratch, with human guidance but without human proof writing. The proof establishes semantic preservation for the administrative normal form (ANF) transformation in the CertiCoq verified compiler for Rocq. The closely related continuation-passing style (CPS) transformation in CertiCoq was previously proved correct by human experts over several months. We use this proof as a template and instruct the LLM to adapt the proof technique to the ANF setting, which differs in important technical ways. The resulting ANF proof comprises approximately 7,800 lines of Rocq (larger than the 5,300-line CPS proof) and was developed in approximately 96 hours. We describe the proof technique and report on the experience of developing it with an LLM, discussing both the strengths and limitations of the approach and its implications for verified compiler construction.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [10] [InEx-Bug: A Human Annotated Dataset of Intrinsic and Extrinsic Bugs in the NPM Ecosystem](https://arxiv.org/abs/2602.13400)
*Tanner Wright,Adams Chen,Gema Rodríguez-Pérez*

Main category: cs.SE

TL;DR: 论文提出了InEx-Bug数据集，对NPM仓库的GitHub问题进行分类，区分内部缺陷、外部依赖/环境问题、非缺陷和未知类别，并分析了不同类型问题的解决模式和特征差异。


<details>
  <summary>Details</summary>
Motivation: 现有缺陷数据集未能区分问题来源（项目内部缺陷 vs 外部依赖/环境因素），这限制了软件维护和生态系统稳定性研究的深度。需要专门的数据集来理解不同类型缺陷的特征和解决模式。

Method: 创建了InEx-Bug数据集，手动标注了来自103个NPM仓库的377个GitHub问题，将其分类为：内部缺陷、外部依赖/环境问题、非缺陷和未知类别。数据集包含丰富的时空和行为元数据，如维护者参与度、代码变更和重新打开模式。

Result: 分析显示：内部缺陷解决更快（中位数8.9天 vs 10.2天）、关闭率更高（92% vs 78%）、更频繁需要代码变更（57% vs 28%）。外部缺陷重新打开率更高（12% vs 4%）且复发延迟更长（中位数157天 vs 87天）。

Conclusion: InEx-Bug数据集为深入研究NPM生态系统中内部和外部缺陷提供了基础，揭示了不同类型缺陷的显著行为差异，有助于改进软件维护策略和生态系统稳定性管理。

Abstract: Understanding the causes of software defects is essential for reliable software maintenance and ecosystem stability. However, existing bug datasets do not distinguish between issues originating within a project from those caused by external dependencies or environmental factors. In this paper we present InEx-Bug, a manually annotated dataset of 377 GitHub issues from 103 NPM repositories, categorizing issues as Intrinsic (internal defect), Extrinsic (dependency/environment issue), Not-a-Bug, or Unknown. Beyond labels, the dataset includes rich temporal and behavioral metadata such as maintainer participation, code changes, and reopening patterns. Analyses show Intrinsic bugs resolve faster (median 8.9 vs 10.2 days), are close more often (92% vs 78%), and require code changes more frequently (57% vs 28%) compared to Extrinsic bugs. While Extrinsic bugs exhibit higher reopen rates (12% vs 4%) and delayed recurrence (median 157 vs 87 days). The dataset provides a foundation for further studying Intrinsic and Extrinsic defects in the NPM ecosystem.

</details>


### [11] [Validated Code Translation for Projects with External Libraries](https://arxiv.org/abs/2602.18534)
*Hanliang Zhang,Arindam Sharma,Cristina David,Meng Wang,Brandon Paulsen,Daniel Kroening,Wenjia Ye,Taro Sekiyama*

Main category: cs.SE

TL;DR: 提出Go到Rust的程式翻譯框架，解決外部函式庫依賴問題，透過API檢索和跨語言驗證管線確保語義等價


<details>
  <summary>Details</summary>
Motivation: 現有LLM在程式翻譯時，當原始程式依賴外部函式庫時會產生問題：經常幻覺不存在的目標API、無法生成正確的導入語句，且驗證語義等價在處理不透明函式庫類型時很困難

Method: 結合(i)將Go函式庫API映射到Rust API的檢索機制，和(ii)跨語言驗證管線，透過僅從公開函式庫API合成適配器來建立語言互通性，然後驗證I/O等價性

Result: 在六個具有非平凡外部依賴的真實Go專案上評估，方法顯著提高編譯和等價成功率（在最依賴密集的情況下達100%，平均約2倍提升）

Conclusion: 提出的翻譯和驗證框架能夠有效處理具有外部函式庫依賴的Go專案到Rust的翻譯，特別是在處理不透明函式庫定義類型時實現了經過驗證的翻譯

Abstract: Large Language Models (LLMs) have shown promise for program translation, particularly for migrating systems code to memory-safe languages such as Rust. However, existing approaches struggle when source programs depend on external libraries: LLMs frequently hallucinate non-existent target APIs and fail to generate call-enabling imports; moreover, validating semantic equivalence is challenging when the code manipulates opaque, library-defined types. We present a translation and validation framework for translating Go projects with external dependencies to Rust. Our approach combines (i) a retrieval mechanism that maps Go library APIs to Rust APIs, and (ii) a cross-language validation pipeline that establishes language interoperability in the presence of opaque library types by synthesising adapters exclusively from public library APIs, prior to validating I/O equivalence. We evaluate our system on six real-world Go repositories with non-trivial external dependencies. Our approach significantly increases both the compilation and equivalence success rate (up to 100% in the most dependency-heavy case; approx. 2x on average) by enabling validated translation that manipulate opaque, library-defined types.

</details>


### [12] [Programmable Property-Based Testing](https://arxiv.org/abs/2602.18545)
*Alperen Keles,Justine Frank,Ceren Mert,Harrison Goldstein,Leonidas Lampropoulos*

Main category: cs.SE

TL;DR: 提出基于延迟绑定抽象语法的深度嵌入式属性语言，解耦属性定义与测试执行，实现更灵活可编程的属性测试框架。


<details>
  <summary>Details</summary>
Motivation: 现有属性测试框架采用浅层嵌入式DSL，属性定义与测试执行紧密耦合，用户只能使用框架预设的配置选项，缺乏灵活性。当需要更多定制功能时，用户可能需要从头编写新框架。

Method: 提出基于延迟绑定抽象语法的深度嵌入式属性语言，将属性具体化为数据结构，使其与属性执行器解耦。在Rocq和Racket中实现该语言，分别利用依赖类型和动态类型的优势。

Result: 实现了新的属性语言，能够快速原型化多种属性执行器，展示了更可编程测试方法带来的领域特定测试改进潜力。

Conclusion: 通过深度嵌入式语言和延迟绑定抽象语法，实现了属性定义与测试执行的解耦，为更灵活、可编程的属性测试提供了新途径，能够支持更广泛的测试定制需求。

Abstract: Property-based testing (PBT) is a popular technique for establishing confidence in software, where users write properties -- i.e., executable specifications -- that can be checked many times in a loop by a testing framework. In modern PBT frameworks, properties are usually written in shallowly embedded domain-specific languages, and their definition is tightly coupled to the way they are tested. Such frameworks often provide convenient configuration options to customize aspects of the testing process, but users are limited to precisely what library authors had the prescience to allow for when developing the framework; if they want more flexibility, they may need to write a new framework from scratch.
  We propose a new, deeper language for properties based on a mixed embedding that we call deferred binding abstract syntax, which reifies properties as a data structure and decouples them from the property runners that execute them. We implement this language in Rocq and Racket, leveraging the power of dependent and dynamic types, respectively. Finally, we showcase the flexibility of this new approach by rapidly prototyping a variety of property runners, highlighting domain-specific testing improvements that can be unlocked by more programmable testing.

</details>
