<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.SE](#cs.SE) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Analyzing Latency Hiding and Parallelism in an MLIR-based AI Kernel Compiler](https://arxiv.org/abs/2602.20204)
*Javed Absar,Samarth Narang,Muthu Baskaran*

Main category: cs.PL

TL;DR: 该论文提出了针对边缘设备AI内核编译的基准测试方法，评估了向量化、多线程和双缓冲三种编译器控制机制的性能影响，使用GELU激活函数作为代表性内核进行量化分析。


<details>
  <summary>Details</summary>
Motivation: 边缘设备AI内核编译需要编译器有效利用并行性和隐藏内存延迟，特别是在存在分层内存和显式数据移动的情况下。需要量化评估不同编译器优化机制的实际效果。

Method: 采用MLIR编译流水线，通过Triton/Inductor生成的内核，设计消融阶梯实验分离向量化(Vec)、多线程(MT)和双缓冲(DB)三种机制的贡献，使用GELU作为代表性激活函数内核评估MT加速随问题规模的变化。

Result: 向量化为带宽敏感内核提供主要性能增益；多线程在调度开销被分摊后带来显著改进；双缓冲在传输和计算可以重叠时提供额外收益（既非纯内存受限也非纯计算受限的情况）。

Conclusion: 三种编译器控制机制在不同场景下各有优势：向量化是基础优化，多线程适合大规模问题，双缓冲在内存和计算平衡时效果最佳。这为边缘设备AI内核编译优化提供了量化指导。

Abstract: AI kernel compilation for edge devices depends on the compiler's ability to exploit parallelism and hide memory latency in the presence of hierarchical memory and explicit data movement. This paper reports a benchmark methodology and corresponding results for three compiler-controlled mechanisms in an MLIR-based compilation pipeline: vectorization (Vec), multi-threading (MT) across hardware contexts, and double buffering (DB) using ping--pong scratchpad buffers to overlap DMA transfers with compute. Using Triton/Inductor-generated kernels, we present an ablation ladder that separates the contribution of Vec, MT, and DB, and we quantify how MT speedup scales with problem size using GELU as a representative activation kernel. The results show that vectorization provides the primary gain for bandwidth-sensitive kernels, MT delivers substantial improvements once scheduling overhead is amortized, and DB provides additional benefit when transfers and compute can be overlapped (i.e., outside the extremes of purely memory-bound or purely compute-bound behavior).

</details>


### [2] [DeCo: A Core Calculus for Incremental Functional Programming with Generic Data Types](https://arxiv.org/abs/2602.20866)
*Timon Böhler,Tobias Reinhard,David Richter,Mira Mezini*

Main category: cs.PL

TL;DR: DeCo是一个支持用户自定义数据类型的增量函数式编程核心演算，能够静态增量化领域特定操作，相比其他通用技术更细粒度。


<details>
  <summary>Details</summary>
Motivation: 领域特定的增量化技术难以泛化，而通用方法对领域特定操作的支持不足。需要一种既能支持广泛用户自定义数据类型，又能增量化领域特定操作的通用方法。

Method: 提出DeCo核心演算，支持用户自定义数据类型，能够静态增量化领域特定操作。在Lean中形式化并证明其正确性，提供可执行实现和案例研究。

Result: 在Lean中形式化并证明DeCo的正确性（增量执行与完全重计算结果相同）。实现案例研究涵盖线性代数、关系代数、字典、树和CRDT，并进行了性能评估。

Conclusion: DeCo提供了一个通用的增量函数式编程框架，既能支持用户自定义数据类型，又能细粒度地增量化领域特定操作，填补了领域特定与通用方法之间的空白。

Abstract: Incrementalization speeds up computations by avoiding unnecessary recomputations and by efficiently reusing previous results. While domain-specific techniques achieve impressive speedups, e.g., in the context of database queries, they are difficult to generalize. Meanwhile, general approaches offer little support for incrementalizing domain-specific operations. In this work, we present DeCo, a novel core calculus for incremental functional programming with support for a wide range of user-defined data types. Despite its generic nature, our approach statically incrementalizes domain-specific operations on user-defined data types. It is, hence, more fine-grained than other generic techniques which resort to treating domain-specific operations as black boxes. We mechanized our work in Lean and proved it sound, meaning incrementalized execution computes the same result as full reevaluation. We also provide an executable implementation with case studies featuring examples from linear algebra, relational algebra, dictionaries, trees, and conflict-free replicated data types, plus a brief performance evaluation on linear and relational algebra and on trees.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [3] [Toward an Agentic Infused Software Ecosystem](https://arxiv.org/abs/2602.20979)
*Mark Marron*

Main category: cs.SE

TL;DR: 提出构建Agentic Infused Software Ecosystem (AISE)的愿景，包含AI代理、编程语言/API、运行时环境三大支柱，需要协同发展以充分发挥AI在软件开发中的潜力。


<details>
  <summary>Details</summary>
Motivation: 为了充分发挥AI代理在软件开发中的能力，需要重新思考软件生态系统本身。当前AI代理已从简单的代码补全发展到能够执行复杂独立开发任务，但需要更完善的生态系统支持。

Method: 提出构建AISE的三大支柱框架：1) AI代理本身；2) 编程语言和API（作为人机协作的沟通媒介）；3) 运行时环境和生态系统（提供与外部世界交互的能力）。

Result: 提出了一个全面的AISE概念框架，强调三大支柱需要以协同、整体的方式发展，既要支持当前AI代理，也要适应未来AI技术发展，同时兼顾人类开发者的需求。

Conclusion: 实现AISE愿景需要三大支柱的协同发展，为AI代理和人类开发者创造协同工作的环境，这是充分发挥AI在软件开发中潜力的关键。

Abstract: Fully leveraging the capabilities of AI agents in software development requires a rethinking of the software ecosystem itself. To this end, this paper outlines the creation of an Agentic Infused Software Ecosystem (AISE), that rests on three pillars. The first, of course, is the AI agents themselves, which in the past 5 years have moved from simple code completion and toward sophisticated independent development tasks, a trend which will only continue. The second pillar is the programming language and APIs (or tools) that these agents use to accomplish tasks, and increasingly, serve as the communication substrate that humans and AI agents interact and collaborate through. The final pillar is the runtime environment and ecosystem that agents operate within, and which provide the capabilities that programmatic agents use to interface with (and effect actions in) the external world. To realize the vision of AISE, all three pillars must be advanced in a holistic manner, and critically, in a manner that is synergistic for AI agents as they exist today, those that will exist in the future, and for the human developers that work alongside them.

</details>
