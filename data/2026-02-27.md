<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.MS](#cs.MS) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [From Prompts to Performance: Evaluating LLMs for Task-based Parallel Code Generation](https://arxiv.org/abs/2602.22240)
*Linus Bantel,Moritz Strack,Alexander Strack,Dirk Pflüger*

Main category: cs.PL

TL;DR: LLM在并行代码生成能力研究：评估从自然语言、顺序代码和并行伪代码生成任务并行程序的效果，测试OpenMP、C++并行库和HPX框架，分析正确性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在代码生成方面表现出色，但其在生成高效并行程序方面的能力尚未得到充分研究。本文旨在探索LLM如何从不同输入提示生成任务并行代码，评估其在高性能和科学计算领域的实际应用潜力。

Method: 研究使用三种输入提示类型：自然语言问题描述、顺序参考实现和并行伪代码。针对三个编程框架（OpenMP Tasking、C++标准并行库和HPX运行时）评估LLM生成的解决方案。主要评估指标包括代码正确性和可扩展性。

Result: 结果显示LLM在并行代码生成方面既有优势也有局限，性能表现与问题复杂度和所选框架密切相关。不同输入提示类型和并行框架对生成结果的质量有显著影响。

Conclusion: 研究揭示了LLM在并行编程方面的能力边界，为未来LLM辅助的高性能和科学计算开发提供了重要见解，指出了需要改进的方向和应用潜力。

Abstract: Large Language Models (LLM) show strong abilities in code generation, but their skill in creating efficient parallel programs is less studied. This paper explores how LLMs generate task-based parallel code from three kinds of input prompts: natural language problem descriptions, sequential reference implementations, and parallel pseudo code. We focus on three programming frameworks: OpenMP Tasking, C++ standard parallelism, and the asynchronous many-task runtime HPX. Each framework offers different levels of abstraction and control for task execution. We evaluate LLM-generated solutions for correctness and scalability. Our results reveal both strengths and weaknesses of LLMs with regard to problem complexity and framework. Finally, we discuss what these findings mean for future LLM-assisted development in high-performance and scientific computing.

</details>


### [2] [Array-Carrying Symbolic Execution for Function Contract Generation](https://arxiv.org/abs/2602.23216)
*Weijie Lu,Jingyu Ke,Hongfei Fu,Zhouyue Sun,Yi Zhou,Guoqiang Li,Haokun Li*

Main category: cs.PL

TL;DR: 提出基于符号执行的函数契约生成框架，专门处理数组操作中的连续数组段，解决现有方法难以处理数组信息传递的问题。


<details>
  <summary>Details</summary>
Motivation: 函数契约生成是程序分析中的经典问题，在多过程程序分析中至关重要。对于涉及数组操作的程序，处理数组段在推断不变式和赋值信息方面存在挑战，现有方法难以处理数组信息的传递。

Method: 提出新颖的符号执行框架，携带数组连续段的不变式和赋值信息。在LLVM中实现原型，并与ACSL断言格式和Frama-C软件验证平台集成。

Result: 实验评估表明，该框架能够处理涉及数组信息传递的数组操作函数，这些函数超出了现有方法的能力范围。

Conclusion: 提出的符号执行框架有效解决了数组操作函数契约生成中的数组段处理挑战，能够处理复杂的数组信息传递场景。

Abstract: Function contract generation is a classical problem in program analysis that targets the automated analysis of functions in a program with multiple procedures. The problem is fundamental in inter-procedural analysis where properties of functions are first obtained via the generation of function contracts and then the generated contracts are used as building blocks to analyze the whole program. Typical objectives in function contract generation include pre-/post-conditions and assigns information (that specifies the modification information over program variables and memory segments during function execution). In programs with array manipulations, a crucial point in function contract generation is the treatment of array segments that imposes challenges in inferring invariants and assigns information over such segments. To address this challenge, we propose a novel symbolic execution framework that carries invariants and assigns information over contiguous segments of arrays. We implement our framework as a prototype within LLVM, and further integrate our prototype with the ACSL assertion format and the Frama-C software verification platform. Experimental evaluation over a variety of benchmarks from the literature and functions from realistic libraries shows that our framework is capable of handling array manipulating functions that indeed involve the carry of array information and are beyond existing approaches.

</details>


<div id='cs.MS'></div>

# cs.MS [[Back]](#toc)

### [3] [TorchLean: Formalizing Neural Networks in Lean](https://arxiv.org/abs/2602.22631)
*Robert Joseph George,Jennifer Cruden,Xiangru Zhong,Huan Zhang,Anima Anandkumar*

Main category: cs.MS

TL;DR: TorchLean：一个在Lean 4定理证明器中构建的框架，将学习模型作为一等数学对象，为执行和验证提供统一的精确语义，实现端到端的形式化验证。


<details>
  <summary>Details</summary>
Motivation: 神经网络在安全和关键任务系统中部署增多，但现有验证工具与编程环境分离，导致语义鸿沟。验证结果可能依赖于隐式约定（如算子语义、张量布局、预处理、浮点细节），无法保证真正的安全性。

Method: 1. 在Lean 4中构建PyTorch风格的验证API，支持eager和compiled模式，统一到共享的op-tagged SSA/DAG计算图IR；2. 通过可执行的IEEE-754 binary32内核和证明相关的舍入模型提供明确的Float32语义；3. 通过IBP和CROWN/LiRPA风格的边界传播与证书检查进行验证。

Result: 在认证鲁棒性、PINNs的物理信息残差边界、神经控制器的Lyapunov风格验证等任务上进行了端到端验证，并实现了机械化理论结果（包括通用逼近定理）。

Conclusion: TorchLean提供了一个语义优先的基础设施，实现了学习使能系统的完全形式化、端到端验证，弥合了执行和验证之间的语义鸿沟。

Abstract: Neural networks are increasingly deployed in safety- and mission-critical pipelines, yet many verification and analysis results are produced outside the programming environment that defines and runs the model. This separation creates a semantic gap between the executed network and the analyzed artifact, so guarantees can hinge on implicit conventions such as operator semantics, tensor layouts, preprocessing, and floating-point corner cases. We introduce TorchLean, a framework in the Lean 4 theorem prover that treats learned models as first-class mathematical objects with a single, precise semantics shared by execution and verification. TorchLean unifies (1) a PyTorch-style verified API with eager and compiled modes that lower to a shared op-tagged SSA/DAG computation-graph IR, (2) explicit Float32 semantics via an executable IEEE-754 binary32 kernel and proof-relevant rounding models, and (3) verification via IBP and CROWN/LiRPA-style bound propagation with certificate checking. We validate TorchLean end-to-end on certified robustness, physics-informed residual bounds for PINNs, and Lyapunov-style neural controller verification, alongside mechanized theoretical results including a universal approximation theorem. These results demonstrate a semantics-first infrastructure for fully formal, end-to-end verification of learning-enabled systems.

</details>
