<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.MS](#cs.MS) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [From Prompts to Performance: Evaluating LLMs for Task-based Parallel Code Generation](https://arxiv.org/abs/2602.22240)
*Linus Bantel,Moritz Strack,Alexander Strack,Dirk Pflüger*

Main category: cs.PL

TL;DR: LLM在并行代码生成能力评估：探索从自然语言、顺序参考实现和并行伪代码三种输入生成任务并行代码，评估OpenMP Tasking、C++标准并行和HPX三种框架下的正确性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在代码生成方面表现出强大能力，但它们在生成高效并行程序方面的技能研究较少。本文旨在探索LLM在任务并行代码生成方面的能力，特别是针对高性能计算和科学计算领域的需求。

Method: 研究LLM从三种输入提示生成任务并行代码：1)自然语言问题描述，2)顺序参考实现，3)并行伪代码。聚焦三个编程框架：OpenMP Tasking、C++标准并行和HPX异步多任务运行时。评估生成代码的正确性和可扩展性。

Result: 结果揭示了LLM在并行代码生成方面的优势和弱点，这些表现与问题复杂性和所用框架相关。LLM在不同框架和输入类型下的表现存在差异。

Conclusion: 研究结果为未来LLM辅助的高性能计算和科学计算开发提供了重要见解，指出了LLM在并行编程方面的潜力和局限性，为改进方向提供了参考。

Abstract: Large Language Models (LLM) show strong abilities in code generation, but their skill in creating efficient parallel programs is less studied. This paper explores how LLMs generate task-based parallel code from three kinds of input prompts: natural language problem descriptions, sequential reference implementations, and parallel pseudo code. We focus on three programming frameworks: OpenMP Tasking, C++ standard parallelism, and the asynchronous many-task runtime HPX. Each framework offers different levels of abstraction and control for task execution. We evaluate LLM-generated solutions for correctness and scalability. Our results reveal both strengths and weaknesses of LLMs with regard to problem complexity and framework. Finally, we discuss what these findings mean for future LLM-assisted development in high-performance and scientific computing.

</details>


### [2] [Array-Carrying Symbolic Execution for Function Contract Generation](https://arxiv.org/abs/2602.23216)
*Weijie Lu,Jingyu Ke,Hongfei Fu,Zhouyue Sun,Yi Zhou,Guoqiang Li,Haokun Li*

Main category: cs.PL

TL;DR: 提出一种新的符号执行框架，用于处理数组操作函数中的连续数组段，生成函数契约（前置/后置条件和赋值信息），并集成到LLVM和Frama-C验证平台中。


<details>
  <summary>Details</summary>
Motivation: 在程序间分析中，函数契约生成是关键问题。对于涉及数组操作的程序，处理数组段在推断不变量和赋值信息方面存在挑战，现有方法难以处理涉及数组信息传递的函数。

Method: 提出一种新颖的符号执行框架，能够携带关于数组连续段的不变量和赋值信息。在LLVM中实现原型，并集成到ACSL断言格式和Frama-C软件验证平台中。

Result: 实验评估表明，该框架能够处理涉及数组信息传递的数组操作函数，这些函数超出了现有方法的处理能力。

Conclusion: 该符号执行框架有效解决了数组操作函数中连续数组段的契约生成挑战，为程序间分析提供了更强大的工具支持。

Abstract: Function contract generation is a classical problem in program analysis that targets the automated analysis of functions in a program with multiple procedures. The problem is fundamental in inter-procedural analysis where properties of functions are first obtained via the generation of function contracts and then the generated contracts are used as building blocks to analyze the whole program. Typical objectives in function contract generation include pre-/post-conditions and assigns information (that specifies the modification information over program variables and memory segments during function execution). In programs with array manipulations, a crucial point in function contract generation is the treatment of array segments that imposes challenges in inferring invariants and assigns information over such segments. To address this challenge, we propose a novel symbolic execution framework that carries invariants and assigns information over contiguous segments of arrays. We implement our framework as a prototype within LLVM, and further integrate our prototype with the ACSL assertion format and the Frama-C software verification platform. Experimental evaluation over a variety of benchmarks from the literature and functions from realistic libraries shows that our framework is capable of handling array manipulating functions that indeed involve the carry of array information and are beyond existing approaches.

</details>


<div id='cs.MS'></div>

# cs.MS [[Back]](#toc)

### [3] [TorchLean: Formalizing Neural Networks in Lean](https://arxiv.org/abs/2602.22631)
*Robert Joseph George,Jennifer Cruden,Xiangru Zhong,Huan Zhang,Anima Anandkumar*

Main category: cs.MS

TL;DR: TorchLean：一个在Lean 4定理证明器中构建的框架，为学习模型提供统一的精确语义，实现执行与验证的共享语义，支持端到端的形式化验证。


<details>
  <summary>Details</summary>
Motivation: 神经网络在安全和关键任务系统中部署增多，但现有验证工具与执行环境分离，导致语义鸿沟。验证结果可能依赖于隐式约定（如算子语义、张量布局、预处理、浮点细节），无法保证真正的安全性。

Method: 1) 提供PyTorch风格的验证API，包含eager和compiled模式，统一到共享的op-tagged SSA/DAG计算图IR；2) 通过可执行的IEEE-754 binary32内核和证明相关的舍入模型实现显式Float32语义；3) 支持IBP和CROWN/LiRPA风格的边界传播验证与证书检查。

Result: 在认证鲁棒性、PINNs的物理信息残差边界、神经控制器的Lyapunov风格验证等任务上进行了端到端验证，并实现了机械化理论结果（包括通用逼近定理）。

Conclusion: TorchLean提供了一个语义优先的基础设施，实现了学习使能系统的完全形式化、端到端验证，弥合了执行与验证之间的语义鸿沟。

Abstract: Neural networks are increasingly deployed in safety- and mission-critical pipelines, yet many verification and analysis results are produced outside the programming environment that defines and runs the model. This separation creates a semantic gap between the executed network and the analyzed artifact, so guarantees can hinge on implicit conventions such as operator semantics, tensor layouts, preprocessing, and floating-point corner cases. We introduce TorchLean, a framework in the Lean 4 theorem prover that treats learned models as first-class mathematical objects with a single, precise semantics shared by execution and verification. TorchLean unifies (1) a PyTorch-style verified API with eager and compiled modes that lower to a shared op-tagged SSA/DAG computation-graph IR, (2) explicit Float32 semantics via an executable IEEE-754 binary32 kernel and proof-relevant rounding models, and (3) verification via IBP and CROWN/LiRPA-style bound propagation with certificate checking. We validate TorchLean end-to-end on certified robustness, physics-informed residual bounds for PINNs, and Lyapunov-style neural controller verification, alongside mechanized theoretical results including a universal approximation theorem. These results demonstrate a semantics-first infrastructure for fully formal, end-to-end verification of learning-enabled systems.

</details>
