{"id": "2602.22240", "pdf": "https://arxiv.org/pdf/2602.22240", "abs": "https://arxiv.org/abs/2602.22240", "authors": ["Linus Bantel", "Moritz Strack", "Alexander Strack", "Dirk Pfl√ºger"], "title": "From Prompts to Performance: Evaluating LLMs for Task-based Parallel Code Generation", "categories": ["cs.PL", "cs.AI", "cs.DC"], "comment": "12 pages, 4 figures, 2 tables, Workshop on Asynchronous Many-Task Systems and Applications 2026", "summary": "Large Language Models (LLM) show strong abilities in code generation, but their skill in creating efficient parallel programs is less studied. This paper explores how LLMs generate task-based parallel code from three kinds of input prompts: natural language problem descriptions, sequential reference implementations, and parallel pseudo code. We focus on three programming frameworks: OpenMP Tasking, C++ standard parallelism, and the asynchronous many-task runtime HPX. Each framework offers different levels of abstraction and control for task execution. We evaluate LLM-generated solutions for correctness and scalability. Our results reveal both strengths and weaknesses of LLMs with regard to problem complexity and framework. Finally, we discuss what these findings mean for future LLM-assisted development in high-performance and scientific computing."}
{"id": "2602.23216", "pdf": "https://arxiv.org/pdf/2602.23216", "abs": "https://arxiv.org/abs/2602.23216", "authors": ["Weijie Lu", "Jingyu Ke", "Hongfei Fu", "Zhouyue Sun", "Yi Zhou", "Guoqiang Li", "Haokun Li"], "title": "Array-Carrying Symbolic Execution for Function Contract Generation", "categories": ["cs.PL", "cs.LO", "cs.SE"], "comment": "30 pages, 2 figures. To appear in the 27th International Symposium on Formal Methods (FM 2026)", "summary": "Function contract generation is a classical problem in program analysis that targets the automated analysis of functions in a program with multiple procedures. The problem is fundamental in inter-procedural analysis where properties of functions are first obtained via the generation of function contracts and then the generated contracts are used as building blocks to analyze the whole program. Typical objectives in function contract generation include pre-/post-conditions and assigns information (that specifies the modification information over program variables and memory segments during function execution). In programs with array manipulations, a crucial point in function contract generation is the treatment of array segments that imposes challenges in inferring invariants and assigns information over such segments. To address this challenge, we propose a novel symbolic execution framework that carries invariants and assigns information over contiguous segments of arrays. We implement our framework as a prototype within LLVM, and further integrate our prototype with the ACSL assertion format and the Frama-C software verification platform. Experimental evaluation over a variety of benchmarks from the literature and functions from realistic libraries shows that our framework is capable of handling array manipulating functions that indeed involve the carry of array information and are beyond existing approaches."}
{"id": "2602.22631", "pdf": "https://arxiv.org/pdf/2602.22631", "abs": "https://arxiv.org/abs/2602.22631", "authors": ["Robert Joseph George", "Jennifer Cruden", "Xiangru Zhong", "Huan Zhang", "Anima Anandkumar"], "title": "TorchLean: Formalizing Neural Networks in Lean", "categories": ["cs.MS", "cs.LG", "cs.LO", "cs.PL", "math.NA"], "comment": "35 pages, multiple figures and tables", "summary": "Neural networks are increasingly deployed in safety- and mission-critical pipelines, yet many verification and analysis results are produced outside the programming environment that defines and runs the model. This separation creates a semantic gap between the executed network and the analyzed artifact, so guarantees can hinge on implicit conventions such as operator semantics, tensor layouts, preprocessing, and floating-point corner cases. We introduce TorchLean, a framework in the Lean 4 theorem prover that treats learned models as first-class mathematical objects with a single, precise semantics shared by execution and verification. TorchLean unifies (1) a PyTorch-style verified API with eager and compiled modes that lower to a shared op-tagged SSA/DAG computation-graph IR, (2) explicit Float32 semantics via an executable IEEE-754 binary32 kernel and proof-relevant rounding models, and (3) verification via IBP and CROWN/LiRPA-style bound propagation with certificate checking. We validate TorchLean end-to-end on certified robustness, physics-informed residual bounds for PINNs, and Lyapunov-style neural controller verification, alongside mechanized theoretical results including a universal approximation theorem. These results demonstrate a semantics-first infrastructure for fully formal, end-to-end verification of learning-enabled systems."}
