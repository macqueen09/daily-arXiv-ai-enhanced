<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.MS](#cs.MS) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [From Prompts to Performance: Evaluating LLMs for Task-based Parallel Code Generation](https://arxiv.org/abs/2602.22240)
*Linus Bantel,Moritz Strack,Alexander Strack,Dirk Pflüger*

Main category: cs.PL

TL;DR: LLM在并行代码生成方面的能力评估：研究LLM如何从自然语言描述、顺序参考实现和并行伪代码三种输入生成任务并行代码，在OpenMP、C++标准并行和HPX三个框架下评估其正确性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在代码生成方面表现出强大能力，但它们在创建高效并行程序方面的技能研究较少。本文旨在探索LLM在生成任务并行代码方面的能力，特别是在高性能和科学计算领域。

Method: 研究LLM从三种输入提示生成并行代码：1)自然语言问题描述，2)顺序参考实现，3)并行伪代码。在三个编程框架下评估：OpenMP Tasking、C++标准并行和异步多任务运行时HPX。评估生成代码的正确性和可扩展性。

Result: 研究结果揭示了LLM在并行代码生成方面的优势和弱点，这些表现与问题复杂性和使用的框架相关。不同框架的抽象层次和控制粒度影响了LLM的生成质量。

Conclusion: 研究讨论了这些发现对未来LLM辅助开发在高性能和科学计算领域的影响，为LLM在并行编程方面的应用提供了重要见解。

Abstract: Large Language Models (LLM) show strong abilities in code generation, but their skill in creating efficient parallel programs is less studied. This paper explores how LLMs generate task-based parallel code from three kinds of input prompts: natural language problem descriptions, sequential reference implementations, and parallel pseudo code. We focus on three programming frameworks: OpenMP Tasking, C++ standard parallelism, and the asynchronous many-task runtime HPX. Each framework offers different levels of abstraction and control for task execution. We evaluate LLM-generated solutions for correctness and scalability. Our results reveal both strengths and weaknesses of LLMs with regard to problem complexity and framework. Finally, we discuss what these findings mean for future LLM-assisted development in high-performance and scientific computing.

</details>


### [2] [Array-Carrying Symbolic Execution for Function Contract Generation](https://arxiv.org/abs/2602.23216)
*Weijie Lu,Jingyu Ke,Hongfei Fu,Zhouyue Sun,Yi Zhou,Guoqiang Li,Haokun Li*

Main category: cs.PL

TL;DR: 提出基于符号执行的函数合约生成框架，专门处理数组操作中的连续段信息，在LLVM中实现原型并与Frama-C集成


<details>
  <summary>Details</summary>
Motivation: 函数合约生成是程序分析中的经典问题，在多过程程序中尤为重要。对于涉及数组操作的程序，处理数组段信息是生成函数合约的关键挑战，现有方法难以有效处理数组段的连续性和修改信息。

Method: 提出新颖的符号执行框架，能够携带数组连续段的变量信息和赋值信息。在LLVM中实现原型系统，并与ACSL断言格式和Frama-C软件验证平台集成。

Result: 在文献基准测试和实际库函数上的实验评估表明，该框架能够处理涉及数组信息传递的函数，这些函数超出了现有方法的处理能力。

Conclusion: 提出的符号执行框架有效解决了数组操作函数合约生成中的数组段处理难题，为涉及复杂数组操作的函数分析提供了实用解决方案。

Abstract: Function contract generation is a classical problem in program analysis that targets the automated analysis of functions in a program with multiple procedures. The problem is fundamental in inter-procedural analysis where properties of functions are first obtained via the generation of function contracts and then the generated contracts are used as building blocks to analyze the whole program. Typical objectives in function contract generation include pre-/post-conditions and assigns information (that specifies the modification information over program variables and memory segments during function execution). In programs with array manipulations, a crucial point in function contract generation is the treatment of array segments that imposes challenges in inferring invariants and assigns information over such segments. To address this challenge, we propose a novel symbolic execution framework that carries invariants and assigns information over contiguous segments of arrays. We implement our framework as a prototype within LLVM, and further integrate our prototype with the ACSL assertion format and the Frama-C software verification platform. Experimental evaluation over a variety of benchmarks from the literature and functions from realistic libraries shows that our framework is capable of handling array manipulating functions that indeed involve the carry of array information and are beyond existing approaches.

</details>


<div id='cs.MS'></div>

# cs.MS [[Back]](#toc)

### [3] [TorchLean: Formalizing Neural Networks in Lean](https://arxiv.org/abs/2602.22631)
*Robert Joseph George,Jennifer Cruden,Xiangru Zhong,Huan Zhang,Anima Anandkumar*

Main category: cs.MS

TL;DR: TorchLean：一个在Lean 4定理证明器中实现的框架，为神经网络提供统一的数学语义，将执行和验证结合在同一个编程环境中，消除语义鸿沟。


<details>
  <summary>Details</summary>
Motivation: 当前神经网络验证和分析通常在定义和运行模型的编程环境之外进行，这造成了执行网络与分析工件之间的语义鸿沟。保证可能依赖于隐式约定，如算子语义、张量布局、预处理和浮点边界情况。

Method: 1) 提供类似PyTorch的验证API，具有eager和compiled模式，可降级到共享的op-tagged SSA/DAG计算图IR；2) 通过可执行的IEEE-754 binary32内核和证明相关的舍入模型实现显式Float32语义；3) 通过IBP和CROWN/LiRPA风格的边界传播与证书检查进行验证。

Result: 在认证鲁棒性、PINNs的物理信息残差边界、Lyapunov风格神经控制器验证等方面进行了端到端验证，并实现了机械化理论结果（包括通用逼近定理）。

Conclusion: TorchLean展示了基于语义优先的基础设施，能够实现学习启用的系统的完全形式化、端到端验证。

Abstract: Neural networks are increasingly deployed in safety- and mission-critical pipelines, yet many verification and analysis results are produced outside the programming environment that defines and runs the model. This separation creates a semantic gap between the executed network and the analyzed artifact, so guarantees can hinge on implicit conventions such as operator semantics, tensor layouts, preprocessing, and floating-point corner cases. We introduce TorchLean, a framework in the Lean 4 theorem prover that treats learned models as first-class mathematical objects with a single, precise semantics shared by execution and verification. TorchLean unifies (1) a PyTorch-style verified API with eager and compiled modes that lower to a shared op-tagged SSA/DAG computation-graph IR, (2) explicit Float32 semantics via an executable IEEE-754 binary32 kernel and proof-relevant rounding models, and (3) verification via IBP and CROWN/LiRPA-style bound propagation with certificate checking. We validate TorchLean end-to-end on certified robustness, physics-informed residual bounds for PINNs, and Lyapunov-style neural controller verification, alongside mechanized theoretical results including a universal approximation theorem. These results demonstrate a semantics-first infrastructure for fully formal, end-to-end verification of learning-enabled systems.

</details>
